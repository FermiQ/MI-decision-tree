{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10. Neural networks\n",
    "\n",
    "#### Table of contents\n",
    "\n",
    "1. Overview\n",
    "2. The QM7 dataset\n",
    "3. Prepare the data\n",
    "4. Implementation\n",
    "5. Learning rate\n",
    "6. Regularization\n",
    "\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "In this lab session we will improve the code we developed in Lab 9 to optimize a neural network. We will explore the learning rate and regularization hyperparameters.\n",
    "\n",
    "## 2. The QM7 dataset\n",
    "\n",
    "This dataset is an extension of the QM7 dataset for multitask learning where 13 additional properties (e.g. polarizability, HOMO and LUMO eigenvalues, excitation energies) have to be predicted at different levels of theory (ZINDO, SCS, PBE0, GW). Additional molecules comprising chlorine atoms are also included, totalling 7211 molecules.\n",
    "The dataset is composed of two multidimensional arrays $X$ ($7211\\times 23\\times 23$) and $T$ ($7211\\times 14$) representing the inputs (Coulomb matrices) and the labels (molecular properties) and one array names of size 14 listing the names of the different properties.\n",
    "More details are provided in this [paper](https://iopscience.iop.org/article/10.1088/1367-2630/15/9/095003/meta).\n",
    "\n",
    "Basically, the datatset contains features to describe some small molecules (these features are called Coulomb matrices) and various molecular properties (14) as follow:\n",
    "\n",
    "1. Atomization energies (PBE0, unit: kcal/mol)\n",
    "2. Excitation of maximal optimal absorption (ZINDO, unit: eV)\n",
    "3. Absorption Intensity at maximal absorption (ZINDO)\n",
    "4. Highest occupied molecular orbital HOMO (ZINDO, unit: eV)\n",
    "5. Lowest unoccupied molecular orbital LUMO (ZINDO, unit: eV)\n",
    "6. First excitation energy (ZINDO, unit: eV)\n",
    "7. Ionization potential IP (ZINDO, unit: eV)\n",
    "8. Electron affinity EA (ZINDO, unit: eV)\n",
    "9. Highest occupied molecular orbital HOMO (PBE0, unit: eV)\n",
    "10. Lowest unoccupied molecular orbital LUMO (PBE0, unit: eV)\n",
    "11. Highest occupied molecular orbital HOMO (GW, unit: eV)\n",
    "12. Lowest unoccupied molecular orbital LUMO (GW, unit: eV)\n",
    "13. Polarizabilities (PBE0, unit: $A^3$)\n",
    "14. Polarizabilities (SCS, unit: $A^3$)\n",
    "\n",
    "Because these properties are complicated to compute, methods based on machine learning can be trained to predict them based on some meaningfull features. Coulomb matrices are such good representations.\n",
    "\n",
    "A Coulmb matrix is defined based on the atomic positions $R_i$ and atomic charges $Z_i$ of atoms in a molecule as:\n",
    "\n",
    "$M_{IJ}=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "0.5Z_I^{2.4}\\text{ for }I=J\\\\\n",
    "\\frac{Z_IZ_J}{|R_I-R_J|}\\text{ for }I\\neq J\\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$\n",
    "\n",
    "Here, the Coulomb matrices are already computed and provided in the training set.\n",
    "\n",
    "## 3. Prepare the data\n",
    "\n",
    "Let's first load the data and reshape it into 2D arrays (this was explained in Lab 8). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "qm7 = loadmat('qm7b.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we set the input and output variables. To speedup the neural network optimization, we will select only the first 1500 examples in the dataset. Note that changing the size of the dataset can have dramatic effects on the results and parameters discussed below. You should keep this number to answer all questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 529)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "xsize = 1500\n",
    "X0 = qm7['X']\n",
    "X = X0.reshape(7211,529)\n",
    "X = np.c_[X[:xsize]]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 1)\n"
     ]
    }
   ],
   "source": [
    "y = qm7['T'][:,0]*0.043\n",
    "y = np.c_[y[:xsize]]\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into 80% training, 10% validation and 10% testing. Then, we standardize the data. Note that standardization is defined on the training data and then applied to transform the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 529) (150, 529) (150, 529)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state=31)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test,y_test,test_size = 0.5,random_state=32)\n",
    "print(X_train.shape,X_test.shape,X_val.shape)\n",
    "\n",
    "X_scaler = StandardScaler(with_mean=False,with_std=False).fit(X_train)\n",
    "X_train = X_scaler.transform(X_train)\n",
    "X_val = X_scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation\n",
    "\n",
    "__Q.1.__ Implement in the code block below l1 regularization. You should look at the regular implementation (without regularization) in the code of Lab 9 and modified it to include regularization. You must only assign values for the variable `self.weights` (3 marks).\n",
    "\n",
    "__Q.2.__ Similarly, implement in the code block below l2 regularization (3 marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, reg='l1'):\n",
    "        prng = RandomState(33) # seed for random numbers\n",
    "        self.num_layers = len(sizes)        \n",
    "        self.sizes = sizes\n",
    "        self.reg = reg # variable for regularization technique\n",
    "        self.biases = [prng.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [prng.randn(y, x)/np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "                \n",
    "    def feedforward(self, a):  \n",
    "        a_list = [a]\n",
    "        z_list = []\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            z = np.dot(w, a)+b\n",
    "            z_list.append(z)\n",
    "            a = tanh(z)\n",
    "            a_list.append(a)\n",
    "        z = np.dot(self.weights[-1], a)+self.biases[-1]\n",
    "        z_list.append(z)\n",
    "        a_list.append(z)\n",
    "        return a_list,z_list\n",
    "\n",
    "    def SGD(self, X, y, X_test, y_test, hyper_params):\n",
    "        # We get the hyper-parameters\n",
    "        epochs, mini_batch_size, alpha, lmbda = hyper_params\n",
    "        rmse, y_pred = self.evaluate(X,y)\n",
    "        rmse_test, y_pred_test = self.evaluate(X_test,y_test)\n",
    "        print(\"Epoch {:3d} complete Train {:.4f} eV Test {:.4f} eV\".format(0,rmse,rmse_test))\n",
    "        m,n = X.shape\n",
    "        rmse_list, rmse_test_list = [],[]\n",
    "        # Loop over epochs\n",
    "        for j in range(epochs):\n",
    "            t0 = time.time()\n",
    "            total_batch = int(m/mini_batch_size)\n",
    "            # Loop over batches\n",
    "            for k in range(total_batch):\n",
    "                offset = k*mini_batch_size\n",
    "                Xi = X[offset:offset+mini_batch_size]\n",
    "                Yi = y[offset:offset+mini_batch_size]\n",
    "                # Update weights and biases\n",
    "                self.update_mini_batch(Xi,Yi,alpha,lmbda,m)\n",
    "            if (j+1) % 1 == 0:\n",
    "                rmse, y_pred = self.evaluate(X,y)\n",
    "                rmse_list.append(rmse)\n",
    "                t = time.time()\n",
    "                rmse_test, y_pred_test = self.evaluate(X_test,y_test)\n",
    "                rmse_test_list.append(rmse_test)\n",
    "                print(\"Epoch {:3d} complete Train {:.4f} eV Test {:.4f} eV @{:.3f}s\".format(j+1,rmse,rmse_test,t-t0))\n",
    "            else: \n",
    "                t = time.time()\n",
    "                print(\"Epoch {:3d} complete @{:.3f}s\".format(j+1,t-t0))\n",
    "        return rmse_list, rmse_test_list, y_pred, y_pred_test\n",
    "\n",
    "    def update_mini_batch(self, Xi, Yi, alpha, lmbda, m):\n",
    "        # Create arrays filled with zeros\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        mi,ni = Xi.shape\n",
    "        # Loop over examples in the mini batch\n",
    "        for i in range(mi):\n",
    "            # Backprop\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(np.c_[Xi[i]], Yi[i])\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        # Update weights and biases via GD based on all examples in the mini batch\n",
    "        if self.reg == 'l1':\n",
    "            ### Q.1. BEGIN SOLUTION\n",
    "            ### Q.1. END SOLUTION\n",
    "        if self.reg == 'l2':\n",
    "            ### Q.2. BEGIN SOLUTION\n",
    "            ### Q.2. END SOLUTION\n",
    "        self.biases = [b-(alpha/mi)*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, xi, yi):\n",
    "        # Initialize arrays\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # Forward path\n",
    "        a_list,z_list = self.feedforward(xi)\n",
    "        delta = self.J_prime(a_list[-1], yi) # BP1\n",
    "        nabla_b[-1] = delta # BP3\n",
    "        nabla_w[-1] = np.dot(delta, a_list[-2].transpose()) # BP4\n",
    "        # Backpropagate\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = z_list[-l]\n",
    "            sp = tanh_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            # Compute dJ/db of each layer (BP3)\n",
    "            nabla_b[-l] = delta\n",
    "            # Compute dJ/dw of each layer (BP4)\n",
    "            nabla_w[-l] = np.dot(delta, a_list[-l-1].transpose())\n",
    "        return nabla_b, nabla_w\n",
    "    \n",
    "    def evaluate(self, X_val, y_val):\n",
    "        # Feedforward and compute RMSE\n",
    "        m_val,n_val = X_val.shape\n",
    "        a_list,z_list = self.feedforward(np.c_[X_val].T)\n",
    "        rmse = np.sqrt(np.sum((a_list[-1]-np.c_[y_val].T)**2)/m_val)\n",
    "        # Returns RMSE value and list of output values over arguments data X_val/y_val\n",
    "        return rmse,a_list[-1]\n",
    "    \n",
    "    def J_prime(self, hi, y):\n",
    "        return hi-y\n",
    "\n",
    "# Some activation functions and their respective derivatives\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return 2*sigmoid(2*z)-1\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def tanh_prime(z):\n",
    "    return 1-tanh(z)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning rate\n",
    "\n",
    "We would like first to find a good learning rate corresponding to the data we have. We will use a feedforward neural network with 2 hidden layers each of 100 neurons. In a first time, we wont regularize.\n",
    "\n",
    "__Q.3.__ Define the list `alphas` of learning rate corresponding to values of 0.01, 0.001, 0.0001 and 0.00001 and perform gradient descent for 40 epochs with a minibatch size of 10 and without regularization (define the valiables `epochs`, `mini_batch_size` and `lmbda`). Once you have defined the variables, you can execute the code block to perform GD over the various learning rates (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "### END SOLUTION\n",
    "\n",
    "err, err_val  = [],[]\n",
    "for alpha in alphas:\n",
    "    print('>>> Alpha =',alpha)\n",
    "    net = Network([X_train.shape[1],100,100,1])\n",
    "    hyper_params = epochs, mini_batch_size, alpha,lmbda\n",
    "    rmse_list,rmse_list_test,y_predict, y_predict_test = net.SGD(X_train,y_train,X_val,y_val,hyper_params) \n",
    "    err.append(rmse_list)\n",
    "    err_val.append(rmse_list_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the optimal learning rate, we can plot the RMSE of the training and validation datasets for the various learning rates explored. For better selection, we print the averaged values of the RMSE over the last 10 epochs (10-RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "for i in range(len(err)):\n",
    "    ax1.plot(err[i],marker = '.',ms=0,ls='-',lw=4,label='alpha '+str(alphas[i]))\n",
    "    ax2.plot(err_val[i],marker = '.',ms=0,ls='-',lw=4,label='alpha '+str(alphas[i]))\n",
    "    print('>>> Alpha=',alphas[i],' 10-RMSE=',np.mean(err[i][-10:]),np.mean(err_val[i][-10:]),'eV')\n",
    "\n",
    "ax1.set_ylim(1,10)\n",
    "ax2.set_ylim(4,10)\n",
    "ax1.set_xlabel('epoch',fontsize=22)\n",
    "ax2.set_xlabel('epoch',fontsize=22)\n",
    "ax1.set_ylabel('RMSE (eV)',fontsize=22)\n",
    "ax1.set_title('Training set',fontsize=22)\n",
    "ax2.set_title('Validation set',fontsize=22)\n",
    "plt.legend(ncol=2)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figure.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous plots, we can select the optimal learning rate corresponding to that leading to the lowest (10-RMSE) validation error.\n",
    "\n",
    "## 6. Regularization\n",
    "\n",
    "We will now select the optimal regularization parameter while keeping the learning rate to the value previously selected.\n",
    "\n",
    "__Q.4.__ Define the list `lmbdas` of regularization parameters corresponding to values of 0, 10, 1, 0.1, 0.01 and perform gradient descent for 40 epochs with a minibatch size of 10 and learning rate previously optimized (define the valiables `epochs`, `mini_batch_size` and `alpha`). We will only consider l1 regularization (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "### END SOLUTION\n",
    "\n",
    "err, err_val  = [],[]\n",
    "for lmbda in lmbdas:\n",
    "    print('>>> Lambda =',lmbda)\n",
    "    net = Network([X_train.shape[1],100,100,1])\n",
    "    hyper_params = epochs, mini_batch_size, alpha,lmbda\n",
    "    rmse_list,rmse_list_test,y_predict, y_predict_test = net.SGD(X_train,y_train,X_val,y_val,hyper_params) \n",
    "    err.append(rmse_list)\n",
    "    err_val.append(rmse_list_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the optimal ragularization parameter, we can plot the RMSE on the training and validation data. For better selection, we print the averaged values of the RMSE over the last 10 epochs (10-RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "for i in range(len(err)):\n",
    "    ax1.plot(err[i],marker = '.',ms=0,ls='-',lw=4,label='lambda '+str(lmbdas[i]))\n",
    "    ax2.plot(err_val[i],marker = '.',ms=0,ls='-',lw=4,label='lambda '+str(lmbdas[i]))\n",
    "    print('>>> Lambda=',lmbdas[i],' 10-RMSE=',np.mean(err[i][-10:]),np.mean(err_val[i][-10:]),'eV')\n",
    "\n",
    "ax1.set_ylim(1.5,4)\n",
    "ax1.set_xlim(20,41)\n",
    "ax2.set_ylim(4,6)\n",
    "ax2.set_xlim(20,41)\n",
    "ax1.set_xlabel('epoch',fontsize=22)\n",
    "ax2.set_xlabel('epoch',fontsize=22)\n",
    "ax1.set_ylabel('RMSE (eV)',fontsize=22)\n",
    "ax1.set_title('Training set',fontsize=22)\n",
    "ax2.set_title('Validation set',fontsize=22)\n",
    "plt.legend(ncol=2)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figure.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous plots, we can select the optimal regularization parameter corresponding to that leading to the lowest (10-RMSE) validation error.\n",
    "\n",
    "__Q.5.__ Assign the variables `lmbda` and `alpha` leading to smallest (10-epoch averaged) validation error. Just report the values you have selected previously (2 marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform gradient descent over 100 epochs with the optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 complete Train 64.6352 eV Test 63.0658 eV\n",
      "Epoch   1 complete Train 24.4468 eV Test 23.2667 eV @0.367s\n",
      "Epoch   2 complete Train 13.5417 eV Test 13.3540 eV @0.346s\n",
      "Epoch   3 complete Train 12.1148 eV Test 12.5226 eV @0.347s\n",
      "Epoch   4 complete Train 11.8128 eV Test 12.4237 eV @0.344s\n",
      "Epoch   5 complete Train 10.6796 eV Test 11.2918 eV @0.386s\n",
      "Epoch   6 complete Train 9.1471 eV Test 9.5945 eV @0.326s\n",
      "Epoch   7 complete Train 8.0417 eV Test 8.6487 eV @0.322s\n",
      "Epoch   8 complete Train 7.3207 eV Test 8.2213 eV @0.321s\n",
      "Epoch   9 complete Train 6.7683 eV Test 7.6870 eV @0.322s\n",
      "Epoch  10 complete Train 6.3633 eV Test 7.3581 eV @0.323s\n",
      "Epoch  11 complete Train 5.8644 eV Test 6.9188 eV @0.325s\n",
      "Epoch  12 complete Train 5.4530 eV Test 6.6285 eV @0.402s\n",
      "Epoch  13 complete Train 5.1463 eV Test 6.4615 eV @0.320s\n",
      "Epoch  14 complete Train 4.8910 eV Test 6.3074 eV @0.326s\n",
      "Epoch  15 complete Train 4.5741 eV Test 6.0498 eV @0.324s\n",
      "Epoch  16 complete Train 4.5079 eV Test 6.0484 eV @0.319s\n",
      "Epoch  17 complete Train 4.2329 eV Test 5.9247 eV @0.320s\n",
      "Epoch  18 complete Train 3.9833 eV Test 5.7005 eV @0.358s\n",
      "Epoch  19 complete Train 3.8127 eV Test 5.5957 eV @0.387s\n",
      "Epoch  20 complete Train 3.6293 eV Test 5.3893 eV @0.321s\n",
      "Epoch  21 complete Train 3.4273 eV Test 5.2983 eV @0.335s\n",
      "Epoch  22 complete Train 3.3088 eV Test 5.1938 eV @0.323s\n",
      "Epoch  23 complete Train 3.1829 eV Test 5.0969 eV @0.398s\n",
      "Epoch  24 complete Train 3.0758 eV Test 5.0746 eV @0.326s\n",
      "Epoch  25 complete Train 2.9765 eV Test 4.9929 eV @0.321s\n",
      "Epoch  26 complete Train 2.8701 eV Test 4.9504 eV @0.322s\n",
      "Epoch  27 complete Train 2.7818 eV Test 4.9121 eV @0.355s\n",
      "Epoch  28 complete Train 2.6995 eV Test 4.8740 eV @0.321s\n",
      "Epoch  29 complete Train 2.6202 eV Test 4.8480 eV @0.431s\n",
      "Epoch  30 complete Train 2.5382 eV Test 4.7896 eV @0.332s\n",
      "Epoch  31 complete Train 2.4683 eV Test 4.7575 eV @0.321s\n",
      "Epoch  32 complete Train 2.4024 eV Test 4.7014 eV @0.320s\n",
      "Epoch  33 complete Train 2.3446 eV Test 4.6808 eV @0.329s\n",
      "Epoch  34 complete Train 2.2868 eV Test 4.6487 eV @0.400s\n",
      "Epoch  35 complete Train 2.2465 eV Test 4.6190 eV @0.323s\n",
      "Epoch  36 complete Train 2.1856 eV Test 4.5913 eV @0.331s\n",
      "Epoch  37 complete Train 2.1450 eV Test 4.5702 eV @0.353s\n",
      "Epoch  38 complete Train 2.0851 eV Test 4.5353 eV @0.379s\n",
      "Epoch  39 complete Train 2.0487 eV Test 4.5311 eV @0.409s\n",
      "Epoch  40 complete Train 1.9913 eV Test 4.5005 eV @0.339s\n",
      "Epoch  41 complete Train 1.9428 eV Test 4.4736 eV @0.352s\n",
      "Epoch  42 complete Train 1.8878 eV Test 4.4415 eV @0.339s\n",
      "Epoch  43 complete Train 1.8559 eV Test 4.4256 eV @0.418s\n",
      "Epoch  44 complete Train 1.8170 eV Test 4.4205 eV @0.397s\n",
      "Epoch  45 complete Train 1.7823 eV Test 4.3997 eV @0.423s\n",
      "Epoch  46 complete Train 1.7514 eV Test 4.3975 eV @0.346s\n",
      "Epoch  47 complete Train 1.7250 eV Test 4.3907 eV @0.360s\n",
      "Epoch  48 complete Train 1.6934 eV Test 4.3803 eV @0.403s\n",
      "Epoch  49 complete Train 1.6689 eV Test 4.3755 eV @0.342s\n",
      "Epoch  50 complete Train 1.6528 eV Test 4.3766 eV @0.327s\n",
      "Epoch  51 complete Train 1.6214 eV Test 4.3664 eV @0.323s\n",
      "Epoch  52 complete Train 1.6063 eV Test 4.3659 eV @0.352s\n",
      "Epoch  53 complete Train 1.5798 eV Test 4.3613 eV @0.363s\n",
      "Epoch  54 complete Train 1.5600 eV Test 4.3555 eV @0.322s\n",
      "Epoch  55 complete Train 1.5358 eV Test 4.3510 eV @0.379s\n",
      "Epoch  56 complete Train 1.5145 eV Test 4.3443 eV @0.360s\n",
      "Epoch  57 complete Train 1.4882 eV Test 4.3359 eV @0.324s\n",
      "Epoch  58 complete Train 1.4683 eV Test 4.3324 eV @0.327s\n",
      "Epoch  59 complete Train 1.4379 eV Test 4.3180 eV @0.325s\n",
      "Epoch  60 complete Train 1.4221 eV Test 4.3221 eV @0.324s\n",
      "Epoch  61 complete Train 1.3892 eV Test 4.3041 eV @0.328s\n",
      "Epoch  62 complete Train 1.3730 eV Test 4.3107 eV @0.334s\n",
      "Epoch  63 complete Train 1.3423 eV Test 4.2973 eV @0.396s\n",
      "Epoch  64 complete Train 1.3177 eV Test 4.2926 eV @0.325s\n",
      "Epoch  65 complete Train 1.2965 eV Test 4.2911 eV @0.322s\n",
      "Epoch  66 complete Train 1.2604 eV Test 4.2717 eV @0.323s\n",
      "Epoch  67 complete Train 1.2543 eV Test 4.2829 eV @0.332s\n",
      "Epoch  68 complete Train 1.2091 eV Test 4.2531 eV @0.327s\n",
      "Epoch  69 complete Train 1.2191 eV Test 4.2762 eV @0.355s\n",
      "Epoch  70 complete Train 1.1672 eV Test 4.2384 eV @0.388s\n",
      "Epoch  71 complete Train 1.1826 eV Test 4.2644 eV @0.332s\n",
      "Epoch  72 complete Train 1.1293 eV Test 4.2246 eV @0.325s\n",
      "Epoch  73 complete Train 1.1396 eV Test 4.2466 eV @0.326s\n",
      "Epoch  74 complete Train 1.0926 eV Test 4.2130 eV @0.391s\n",
      "Epoch  75 complete Train 1.0891 eV Test 4.2213 eV @0.324s\n",
      "Epoch  76 complete Train 1.0624 eV Test 4.2061 eV @0.326s\n",
      "Epoch  77 complete Train 1.0518 eV Test 4.2026 eV @0.321s\n",
      "Epoch  78 complete Train 1.0338 eV Test 4.1961 eV @0.322s\n",
      "Epoch  79 complete Train 1.0196 eV Test 4.1839 eV @0.326s\n",
      "Epoch  80 complete Train 1.0180 eV Test 4.1928 eV @0.326s\n",
      "Epoch  81 complete Train 0.9900 eV Test 4.1704 eV @0.324s\n",
      "Epoch  82 complete Train 1.0060 eV Test 4.1899 eV @0.400s\n",
      "Epoch  83 complete Train 0.9663 eV Test 4.1686 eV @0.359s\n",
      "Epoch  84 complete Train 0.9660 eV Test 4.1614 eV @0.329s\n",
      "Epoch  85 complete Train 0.9723 eV Test 4.1788 eV @0.327s\n",
      "Epoch  86 complete Train 0.9366 eV Test 4.1477 eV @0.322s\n",
      "Epoch  87 complete Train 0.9709 eV Test 4.1796 eV @0.391s\n",
      "Epoch  88 complete Train 0.9082 eV Test 4.1569 eV @0.331s\n",
      "Epoch  89 complete Train 0.9293 eV Test 4.1577 eV @0.321s\n",
      "Epoch  90 complete Train 0.8868 eV Test 4.1476 eV @0.327s\n",
      "Epoch  91 complete Train 0.9001 eV Test 4.1470 eV @0.323s\n",
      "Epoch  92 complete Train 0.8986 eV Test 4.1514 eV @0.323s\n",
      "Epoch  93 complete Train 0.8745 eV Test 4.1346 eV @0.328s\n",
      "Epoch  94 complete Train 0.9077 eV Test 4.1532 eV @0.325s\n",
      "Epoch  95 complete Train 0.8396 eV Test 4.1425 eV @0.322s\n",
      "Epoch  96 complete Train 0.8482 eV Test 4.1384 eV @0.327s\n",
      "Epoch  97 complete Train 0.8321 eV Test 4.1252 eV @0.331s\n",
      "Epoch  98 complete Train 0.8310 eV Test 4.1334 eV @0.346s\n",
      "Epoch  99 complete Train 0.8643 eV Test 4.1378 eV @0.391s\n",
      "Epoch 100 complete Train 0.8156 eV Test 4.1392 eV @0.321s\n"
     ]
    }
   ],
   "source": [
    "epochs, mini_batch_size = 100, 10\n",
    "net = Network([X_train.shape[1],100,100,1])\n",
    "hyper_params = epochs, mini_batch_size, alpha,lmbda\n",
    "rmse_list,rmse_list_test,y_predict,y_predict_test = net.SGD(X_train,y_train,X_val,y_val,hyper_params) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can appreaciate the accuracy of the model developed by ploting the actual values of the energy as a function of the predicted values for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "\n",
    "ax1.plot(y_train[:xsize],y_predict.T,'.',lw=0)\n",
    "ax2.plot(y_val[:xsize],y_predict_test.T,'.',lw=0,c='g')\n",
    "ax1.plot(y_train[:xsize],y_train[:xsize],lw=1,label='y=x')\n",
    "ax2.plot(y_val[:xsize],y_val[:xsize],lw=1,label='y=x')\n",
    "\n",
    "ax1.set_xlabel('energies (eV)',fontsize=22)\n",
    "ax2.set_xlabel('energies (eV)',fontsize=22)\n",
    "ax1.set_ylabel('predicted (eV)',fontsize=22)\n",
    "ax1.set_title('Training set',fontsize=22)\n",
    "ax2.set_title('Validation set',fontsize=22)\n",
    "plt.legend(ncol=2)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figure.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
