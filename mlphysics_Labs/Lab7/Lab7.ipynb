{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e94ff797bbd96cbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 7. Learning curves\n",
    "\n",
    "#### Table of contents\n",
    "\n",
    "1. Overview\n",
    "2. Equation of state\n",
    "3. Linear regression with sklearn\n",
    "4. Prepare the data & linear regression\n",
    "5. Learning curves\n",
    "6. Model selection\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "In this lab session we will learn how to use Python's optimized libraries to perform linear regression and we will explore learning curves based on simulated equation of states data.\n",
    "\n",
    "## 2. Equation of state\n",
    "\n",
    "In physics and thermodynamics, an equation of state is a thermodynamic equation relating state variables which describe the state of matter under a given set of physical conditions, such as pressure, volume, temperature (PVT), or internal energy [Wikipedia].\n",
    "\n",
    "## 3. Linear regression with sklearn\n",
    "\n",
    "In the previous labs, we wrote from scratch various functions to perform pre-processing of the data, regressions, regularization, post-processing, etc. The goal of these labs was to understand the principle of the principal machine learning algorithms and techniques. We will now use Python's libraries to perform these tasks because the functions in these libraries have been highly optimized to work with large datasets and large number of features. Sklearn is an efficient Python library for machine learning.\n",
    "\n",
    "### 3.1. Linear regression\n",
    "\n",
    "Linear regression can be achieved with sklearn on the dataset `{X,y}` simply with the commands:\n",
    "\n",
    "`model = sklearn.linea_model.LinearRegression()`<br>\n",
    "`model.fit(X,y)`\n",
    "\n",
    "Note that the data `X` (and `y`) must be an ndarray of shape (m, n) with `m` the number of samples and `n` the number of features. For example, if we consider the data `X0` to be a Series, you can transform the data to an ndarray as:\n",
    "\n",
    "`X = np.c_[X0]`\n",
    "\n",
    "or alternatively,\n",
    "\n",
    "`X = X0.to_numpy().reshape(-1,1)`\n",
    "\n",
    "To predict the output values of the model based on an input array `Z` (of the same shape as `X`) you can use the following command:\n",
    "\n",
    "`model.predict(Z)`\n",
    "\n",
    "This will return an array of the same shape as `Z.shape[0]`.\n",
    "\n",
    "### 3.2. Linear regression with polynomial features\n",
    "\n",
    "With sklearn you can also easily transform features. Starting from the feature `X`, you can create polynomial features as:\n",
    "\n",
    "`poly_features = sklearn.preprocessing.PolynomialFeatures(degree = degree)`<br>\n",
    "`X_poly = poly_features.fit_transform(X)`\n",
    "\n",
    "with `degree` the degree of the polynomial. For example, if your feature is `X = [[1],[2],[3]]` a polynomial feature of degree 2 will be `X = [[1,1],[2,4],[3,9]]`.\n",
    "\n",
    "Because sklearn's `LinearRegression` model takes in a ndarray, you can directly provide a design matrix with polynomial features to perform linear regression.\n",
    "\n",
    "### 3.3. Metrics\n",
    "\n",
    "Sklearn also provides a wide variety of pre-built performance measures. For example, you can compute the mean square error between predicted values `y_predict` and actual data `y` as:\n",
    "\n",
    "`mse = sklearn.metrics.mean_squared_error(y,y_predict)`\n",
    "\n",
    "You can learn more on sklean on the official [website](https://scikit-learn.org/stable/).\n",
    "\n",
    "## 4. Prepare the data & linear regression\n",
    "\n",
    "Let first load some noisy data of an equation of state representing the energy of a crystal as a function of its volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "e0 = pd.read_csv('elastic.csv')\n",
    "e0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q.1.__ Define the series `X0` and `y0` containing the data for the `Volume` and the `Energy`, respectively. The data is clean and there is no need to check for NaN values (1 marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X0,y0,marker='.',lw=0,c='b',ms=12)\n",
    "plt.xlabel('$X_0$',fontsize=22)\n",
    "plt.ylabel('$y_0$',fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prepare data for sklearn. Let's convert the Series to a numpy array and reshape the array to be 2D. This must be done because sklearn takes in a general ndarray as the design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[X0]\n",
    "y = np.c_[y0]\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perfrom linear regression with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q.2.__ Define the array `y_predict` that contains the estimate of the linear model based on the input `X`. `y_predict` must be of shape (m,1) with `m` the number of example in the dataset (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "### END SOLUTION\n",
    "print(y_predict.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the linear model together with the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X0,y0,marker='.',lw=0,c='b',ms=12,label='data')\n",
    "plt.plot(X,y_predict,marker='o',ms=0,lw=2,color='r',label='linear regression')\n",
    "plt.xlabel('$X_0$',fontsize=22)\n",
    "plt.ylabel('$y_0$',fontsize=22)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the fit is very poor because the data is not linear at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q.3.__ Assign to the variable `rmse` the root mean squared error between the output data and the prediction of the linear model (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "### END SOLUTION\n",
    "\n",
    "print(\"The RMSE between the data and the linear fit is:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning curves\n",
    "\n",
    "We will now see how the number of data in the training set affects the mean square error of the training and validation sets. We first need to divide the data into training and validation data. One could simply split the dataset array sequencially however it is better to select randomly the data of the training and validation set. This can be achieved with sklearn as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.2,random_state=235)\n",
    "print(X_train.shape,X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `test_size` defines here the fraction of the dataset to be included in the test set (here the validation set). We use a random state for reproductibility. You should not change the value of the random state.\n",
    "Let's visualize the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train,y_train,marker='.',lw=0,c='g',label='Training data',ms=12)\n",
    "plt.plot(X_val,y_val,marker='.',lw=0,c='b',label='Validation data',ms=12)\n",
    "plt.xlabel('$X$',fontsize=22)\n",
    "plt.ylabel('y',fontsize=22)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like now to plot the learning curve of the model. This corresponds to training the model on a variable number of examples (1$\\rightarrow m_{train}$) and to plot the corresponding training and validation MSE.\n",
    "\n",
    "__Q.4.__ Complete the function below that takes in a model (for example LinearRegression as in the last line of the code block below) and the full training and validation datasets. The function does the following (you must complete what is highlighted in red):\n",
    "\n",
    "- loops over the number of training examples (`m` from 1 training example to $m_{train}$)\n",
    "- fits the model based on the selected training examples\n",
    "<font color=red>\n",
    "- predicts the training output values of the selected training set (size `m`)\n",
    "- predicts the validation output values based on the full validation set\n",
    "- compute the MSE between the actual training data output and the predicted selected training data\n",
    "- compute the MSE between the actual validation data output and the predicted full validation data\n",
    "</font>\n",
    "- store the MSE values into lists\n",
    "- plot the corresponding RMSE of the training and validation data\n",
    "\n",
    "(3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model,X_train,y_train,X_val,y_val):\n",
    "    mse_train_list,mse_val_list = [],[]\n",
    "    mtrain = X_train.shape[0]\n",
    "    for m in range(1,mtrain):\n",
    "        model.fit(X_train[:m],y_train[:m])\n",
    "        ### BEGIN SOLUTION\n",
    "        ### END SOLUTION\n",
    "        mse_train_list.append(mse_train)\n",
    "        mse_val_list.append(mse_val)\n",
    "    plt.plot(np.sqrt(mse_train_list),'r-+',lw=2,label='Training error')\n",
    "    plt.plot(np.sqrt(mse_val_list),'b-',lw=3,label='Validation error')\n",
    "    plt.xlabel('Training set size',fontsize=22)\n",
    "    plt.ylabel('RMSE',fontsize=22)\n",
    "    plt.legend()\n",
    "    plt.ylim(0,8)\n",
    "    plt.show()\n",
    "    \n",
    "model = LinearRegression()\n",
    "plot_learning_curves(model,X_train,y_train,X_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss the curves. When there is one or two examples in the training set, the model can fit them perfectly this is why the training error starts at zero. But as more examples are added, it becomes impossible for the linear model to fit the training data because the data is noisy and not linear at all. So the error on the training data goes up and reaches a plateau at which adding new training data doesnt change much the training error. Concerning the validation error, when there is only few examples in the training data, the model cannot generalize which is why the validation error starts high. As we add examples to the training data, the model learns and the validation error goes down. However, once again, a straight line cannot do a good job to fit the non-linear data hence the validation error also reaches a plateau. These learning curves are typical of an underfitting model; both curves reached a plateau, they are close to each other around a high error value. If a model underfits the training data, adding more examples won't help, you need to define a more complex model to improve the description. For example, we can add polynomial features to the input data. Let's define polynomial features of degree 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree = 20)\n",
    "X_poly_train = poly_features.fit_transform(X_train)\n",
    "X_poly_val = poly_features.fit_transform(X_val)\n",
    "print(X_poly_train.shape,X_poly_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of features is 21 because of the first column of ones in the linear model. We can now represent the learning curves corresponding to this new model. Here the only difference is to provide the newly developed polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "plot_learning_curves(model,X_poly_train,y_train,X_poly_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These learning curves look similar to the previous with the following important differences:\n",
    "\n",
    "- The error on the training data is much lower than with the simple linear model\n",
    "- There is a gap between the training and validation error curves, especially for small training set size, which is a mark of an overfitting model\n",
    "\n",
    "We note that as the number of training examples are added, the curves get closer to each other.\n",
    "\n",
    "## 6. Model selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q.5.__ Complete the function below that returns the best value of the degree of a polynomial model to fit the dataset based on the minimium of the validation error. Here is the explanation of the function (you must complete what is highlighted in red):\n",
    "\n",
    "- it loops over degrees of the polynomial (`degree` from 1 to 20)\n",
    "- it transforms training and validation features into polynomial features of degree `degree`\n",
    "- fits the model based on the polynomial training features (full training examples)\n",
    "<font color=red>\n",
    "- predicts the training output values based of the (full) training set\n",
    "- predicts the validation output values based of the (full) validation set\n",
    "- computes the MSE between the actual training data output and the predicted training data\n",
    "- computes the MSE between the actual validation data output and the predicted validation data\n",
    "</font>\n",
    "- stores the MSE values into lists\n",
    "- returns lists of the training and validation MSEs and the argument of the minimum validation error (adds 1 because the list indices start at 0)\n",
    "\n",
    "(3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_degree(degrees,model,X_train,y_train,X_val,y_val):\n",
    "    mse_train_list,mse_val_list = [],[]\n",
    "    for degree in degrees:\n",
    "        poly_features = PolynomialFeatures(degree = degree)\n",
    "        X_poly_train = poly_features.fit_transform(X_train)\n",
    "        X_poly_val = poly_features.fit_transform(X_val)\n",
    "        model.fit(X_poly_train, y_train)\n",
    "        ### BEGIN SOLUTION\n",
    "        ### END SOLUTION\n",
    "        mse_train_list.append(mse_train)\n",
    "        mse_val_list.append(mse_val)\n",
    "    return mse_train_list, mse_val_list, np.argmin(mse_val_list)+1\n",
    "\n",
    "model = LinearRegression()\n",
    "degrees = list(range(1,20))\n",
    "mse_train_list, mse_val_list, best_degree = find_best_degree(degrees,model,X_train,y_train,X_val,y_val)\n",
    "print(\"The degree of polynomial with lowest validation error is: \", best_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the training and validation MSEs as a function of the degree of the polynomial to better appreciate the model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Polynomial degree\",fontsize=22)\n",
    "plt.ylabel(\"Error\",fontsize=22)\n",
    "plt.plot(degrees,mse_train_list,'r-+',lw=2,label='Training error')\n",
    "plt.plot(degrees,mse_val_list,'b-',lw=3,label='Validation error')\n",
    "plt.ylim(0,2)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that as we increase the polynomial degree, the training error decreases because the model always fits better the training points. However, as we increase the polynomial degree, the validation error reaches a minima and then further increases, because of overfitting. Let's plot the data and the best polynomial that fits it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q.6.__ Complete the code below to assign `y_predict` the array of output values predicted by the best polynomial model over the training data set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features = PolynomialFeatures(degree = best_degree)\n",
    "X_poly_train = poly_features.fit_transform(X_train)\n",
    "model.fit(X_poly_train, y_train)\n",
    "\n",
    "### BEING SOLUTION\n",
    "### END SOLUTION\n",
    "\n",
    "plt.plot(X_train,y_train,marker='.',lw=0,c='g',label='Training data',ms=12)\n",
    "plt.plot(X_val,y_val,marker='.',lw=0,c='b',label='Validation data',ms=12)\n",
    "X_fit, y_predict = zip(*sorted(zip(X_train, y_predict)))\n",
    "plt.plot(X_fit,y_predict,lw=2,c='r',marker=None,label='Polynomial of degree '+str(best_degree))\n",
    "plt.xlabel('$X$',fontsize=22)\n",
    "plt.ylabel('y',fontsize=22)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
