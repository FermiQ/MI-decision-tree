{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9. Neural networks\n",
    "\n",
    "#### Table of contents\n",
    "\n",
    "1. Overview\n",
    "2. The QM7 dataset\n",
    "3. Object oriented programming\n",
    "4. Prepare the data\n",
    "5. Feedforward\n",
    "6. Backpropagation\n",
    "7. Debuging\n",
    "8. Training\n",
    "\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "In this lab session we will write a code to optimize a neural network, run the model to predict the atomization energy of some small molecules and select the best hyperparameters.\n",
    "\n",
    "## 2. The QM7 dataset\n",
    "\n",
    "This dataset is an extension of the QM7 dataset for multitask learning where 13 additional properties (e.g. polarizability, HOMO and LUMO eigenvalues, excitation energies) have to be predicted at different levels of theory (ZINDO, SCS, PBE0, GW). Additional molecules comprising chlorine atoms are also included, totalling 7211 molecules.\n",
    "The dataset is composed of two multidimensional arrays $X$ ($7211\\times 23\\times 23$) and $T$ ($7211\\times 14$) representing the inputs (Coulomb matrices) and the labels (molecular properties) and one array names of size 14 listing the names of the different properties.\n",
    "More details are provided in this [paper](https://iopscience.iop.org/article/10.1088/1367-2630/15/9/095003/meta).\n",
    "\n",
    "Basically, the datatset contains features to describe some small molecules (these features are called Coulomb matrices) and various molecular properties (14) as follow:\n",
    "\n",
    "1. Atomization energies (PBE0, unit: kcal/mol)\n",
    "2. Excitation of maximal optimal absorption (ZINDO, unit: eV)\n",
    "3. Absorption Intensity at maximal absorption (ZINDO)\n",
    "4. Highest occupied molecular orbital HOMO (ZINDO, unit: eV)\n",
    "5. Lowest unoccupied molecular orbital LUMO (ZINDO, unit: eV)\n",
    "6. First excitation energy (ZINDO, unit: eV)\n",
    "7. Ionization potential IP (ZINDO, unit: eV)\n",
    "8. Electron affinity EA (ZINDO, unit: eV)\n",
    "9. Highest occupied molecular orbital HOMO (PBE0, unit: eV)\n",
    "10. Lowest unoccupied molecular orbital LUMO (PBE0, unit: eV)\n",
    "11. Highest occupied molecular orbital HOMO (GW, unit: eV)\n",
    "12. Lowest unoccupied molecular orbital LUMO (GW, unit: eV)\n",
    "13. Polarizabilities (PBE0, unit: $A^3$)\n",
    "14. Polarizabilities (SCS, unit: $A^3$)\n",
    "\n",
    "Because these properties are complicated to compute, methods based on machine learning can be trained to predict them based on some meaningfull features. Coulomb matrices are such good representations.\n",
    "\n",
    "A Coulmb matrix is defined based on the atomic positions $R_i$ and atomic charges $Z_i$ of atoms in a molecule as:\n",
    "\n",
    "$M_{IJ}=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "0.5Z_I^{2.4}\\text{ for }I=J\\\\\n",
    "\\frac{Z_IZ_J}{|R_I-R_J|}\\text{ for }I\\neq J\\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$\n",
    "\n",
    "Here, the Coulomb matrices are already computed and provided in the training set.\n",
    "\n",
    "## 3. Object oriented programming\n",
    "\n",
    "In Python like in many other object oriented programmation languages, you often work with classes. Classes are somewhat similar to functions but need to be instantiated as objects before you can use them. Most of the libraries we have used previously are actually classes (e.g. Numpy, StandardScaler, LinearRegression, etc.). Classes contain functions (called methods) and variables that can be accesses by the object instance.\n",
    "\n",
    "One of the main advantage with classes is that some key variables are accessible anywhere from all methods without having to pass it as arguments. In the neural network example below, these key variables are the architecture of the network, the number of layers, and weights and biases. \n",
    "\n",
    "To complete the lab succesfully, you just need to know some basics about classes. The only differences between classes/methods and the functions we usually define are the following:\n",
    "\n",
    "- A class must have a default method `__init__` to define the key variables\n",
    "- Some other methods are defined within a class\n",
    "  - All these methods take as 1st argument the keyword `self`\n",
    "  - To call any method from another method in the class the name must be provided with the prefix `self.` and you can ignore the `self` argument\n",
    "\n",
    "That's all you need to know to complete the lab.\n",
    "You can learn more about object oriented programming and the use of classes in [this Wikipedia article](https://en.wikipedia.org/wiki/Object-oriented_programming).\n",
    "\n",
    "## 4. Prepare the data\n",
    "\n",
    "Let's first load the data and reshape it into 2D arrays (this was explained in Lab8). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "qm7 = loadmat('qm7b.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7211, 529)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X0 = qm7['X']\n",
    "X = X0.reshape(7211,529)\n",
    "X = np.c_[X]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7211, 1)\n"
     ]
    }
   ],
   "source": [
    "y = qm7['T'][:,0]*0.043\n",
    "y = np.c_[y]\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now answer the following questions in the code block below that defines the entire class `Network` as well as some additional (regular) functions. Because this might be complicated for you to debug, once you have completed the feedforward method (Q.1), you can jump to section 7 and test you feedforward method to be sure it works. You can proceed similarly with the backpropagation method (once you answered Q.2, Q.3 and Q.4 you can jump to section 7 to test your backprop method). Although the following questions only address two functions of the class `Network` you must read the whole code (and all comments) carefully to be sure understand it perfectly!\n",
    "\n",
    "## 5. Feedforward\n",
    "\n",
    "__Q.1.__ Complete the `feedforward` method (Q1). The loop provides for each layer (from $l=2$ to $l=L-1$) the corresponding weight matrix $\\omega$ and the bias vector $b$. The loop starts at $l=2$ because there is no weights and biases in the input layer, and it ends at $l=L-1$ because we will use a linear activation in the output layer $l=L$ (see part of the code after the loop). Inside the loop, you need to compute the function $z = \\omega\\cdot a+b$ and append the result (assigned to the variable `z`) to the list `z_list`. You can use numpy's library for dot product ($a\\cdot b=$`np.dot(a,b)`). Then you have to pass the result throught the activation function, here we will use the hyperbolic tangeant, and append the result (assigned to the variable `a`) to the list `a_list` (2 marks).\n",
    "\n",
    "## 6. Backpropagation\n",
    "\n",
    "__Q.2.__ Complete the `backprop` method (Q2) to evaluate the error in the output layer. You will assign this error to the variable `delta`. This corresponds to equation BP1: $\\delta^L = \\frac{\\partial J}{\\partial a^L}\\sigma'(z^L)$. However, since the activation in the last layer is linear, BP1 becomes: $\\delta^L = \\frac{\\partial J}{\\partial a^L}$. The derivative of $J$ with respect to activation is provided as the method `J_prime`. You just need to evaluate `J_prime` with arguments the activation of the output layer and the actual output value `yi`. Note that because we are inside a class, you must call functions with the `self.` prefix and you can ignore the `self` argument. You can see as an example how the `backprop` function is called within the `update_mini_batch` method. In Python, you can access values in a list (or an array) starting from the end. For example, `a[-1]` corresponds to the last value in the list `a`, `a[-2]` correspond to the second last value, etc. You can use this trick to access the output activation which corresponds to the last value of the `a_list` (2 marks).\n",
    "\n",
    "__Q.3.__ Complete the `backprop` method (Q3) to evaluate the rate change of the cost with respect to biases in the output layer. The array `nabla_b` contains the partial derivatives $\\frac{\\partial J}{\\partial b^l}$ for each layer $l$. Here you must assign only the last value of the list `nabla_b` corresponding to the equation BP3: $\\frac{\\partial J}{\\partial b^L} = \\delta^L$ for $l=L$. Note that the value $\\delta^L$ was computed in the previous question. You can use negative index to assign the last value of the `nabla_b` list (2 marks).\n",
    "\n",
    "__Q.4.__ Complete the `backprop` method (Q4) to evaluate the rate change of the cost with respect to weights in the last layer. The array `nabla_w` contains the partial derivatives $\\frac{\\partial J}{\\partial \\omega^l}$ for each layer $l$. Here you must assign the last value of the list `nabla_w` corresponding to the equation BP4: $\\frac{\\partial J}{\\partial \\omega^L} = \\delta^L\\cdot(a^{L-1})^T$ for $l=L$. The values $\\delta^L$ and $a$ were computed previously. You must take the transpose of the array $a^{L-1}$. This can be achieved with the `.transpose()` or `.T` suffix. You can use negative indices to assign the last value of the `nabla_w` list (2 marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import time\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        prng = RandomState(33) # seed for random numbers\n",
    "        self.num_layers = len(sizes)        \n",
    "        self.sizes = sizes\n",
    "        self.biases = [prng.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [prng.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "                \n",
    "    def feedforward(self, a):  \n",
    "        # The list of activations\n",
    "        a_list = [a]\n",
    "        # The list of z's\n",
    "        z_list = []\n",
    "        # We loop over biased and weights in each layer (from the 2nd layer to L-1)\n",
    "        # Variables b correspond to the bias vector in a given layer\n",
    "        # Variables w correspond to the weight matrix in a given layer\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            ### Q1. BEGIN SOLUTION\n",
    "            ### Q1. END SOLUTION\n",
    "        # Now we compute activation and z of the output layer\n",
    "        z = np.dot(self.weights[-1], a)+self.biases[-1]\n",
    "        z_list.append(z)\n",
    "        # The output layer is linear (no activation) therefore a = z\n",
    "        a_list.append(z)\n",
    "        return a_list,z_list\n",
    "\n",
    "    def SGD(self, X, y, hyper_params):\n",
    "        # We get the hyper-parameters\n",
    "        epochs, mini_batch_size, alpha = hyper_params\n",
    "        # This computes the RMSE\n",
    "        # Also returns an array of the output values of the network\n",
    "        rmse, y_pred = self.evaluate(X,y)\n",
    "        print(\"Epoch {:3d} complete {:.4f} eV\".format(0,rmse))\n",
    "        m,n = X.shape\n",
    "        rmse_list = []\n",
    "        # Loop over epochs\n",
    "        for j in range(epochs):\n",
    "            t0 = time.time()\n",
    "            # Compute number of batches\n",
    "            total_batch = int(m/mini_batch_size)\n",
    "            # Loop over batches\n",
    "            for k in range(total_batch):\n",
    "                offset = k*mini_batch_size\n",
    "                Xi = X[offset:offset+mini_batch_size]\n",
    "                Yi = y[offset:offset+mini_batch_size]\n",
    "                # Update weights and biases\n",
    "                self.update_mini_batch(Xi,Yi,alpha)\n",
    "            if (j+1) % 1 == 0:\n",
    "                rmse, y_pred = self.evaluate(X,y)\n",
    "                rmse_list.append(rmse)\n",
    "                t = time.time()\n",
    "                print(\"Epoch {:3d} complete {:.4f} eV @{:.3f}s\".format(j+1,rmse,t-t0))\n",
    "            else: \n",
    "                t = time.time()\n",
    "                print(\"Epoch {:3d} complete @{:.3f}s\".format(j+1,t-t0))\n",
    "        return rmse_list, y_pred\n",
    "\n",
    "    def update_mini_batch(self, Xi, Yi, alpha):\n",
    "        # Create arrays filled with zeros\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        mi,ni = Xi.shape\n",
    "        # Loop over examples in the mini batch\n",
    "        for i in range(mi):\n",
    "            # Backprop\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(np.c_[Xi[i]], Yi[i])\n",
    "            # Compute partial derivatives over single example\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        # Update weights and biases via GD based on all examples in the mini batch\n",
    "        self.weights = [w-(alpha/mi)*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(alpha/mi)*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, xi, yi):\n",
    "        # Initialize arrays\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # Forward path\n",
    "        # We get the list of activations and z values in each layer\n",
    "        a_list,z_list = self.feedforward(xi)\n",
    "        # Compute delta of last layer (BP1)\n",
    "        ### Q2. BEGIN SOLUTION\n",
    "        ### Q2. END SOLUTION\n",
    "        # Compute dJ/db of the last layer (BP3)\n",
    "        ### Q3. BEGIN SOLUTION\n",
    "        ### Q3. END SOLUTION\n",
    "        # Compute dJ/dw of the last layer (BP4)\n",
    "        ### Q4. BEGIN SOLUTION\n",
    "        ### Q4. END SOLUTION\n",
    "        # Backpropagate\n",
    "        # Loop over layers starting at 2\n",
    "        # Here we will use negative indices\n",
    "        # For example -2, -3, etc. -(L+1) correspond to layers L-1, L-2, etc. 2\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = z_list[-l]\n",
    "            sp = tanh_prime(z)\n",
    "            # Backpropagate delta of each layers (BP2)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            # Compute dJ/db of each layer (BP3)\n",
    "            nabla_b[-l] = delta\n",
    "            # Compute dJ/dw of each layer (BP4)\n",
    "            nabla_w[-l] = np.dot(delta, a_list[-l-1].transpose())\n",
    "        # Returns lists of arrays containing partial derivatives dJ/db and dJ/dw\n",
    "        # Each list has the dimension of the number of layers-1\n",
    "        # Each array has dimension of weights and biases array per layer\n",
    "        return nabla_b, nabla_w\n",
    "    \n",
    "    def evaluate(self, X_val, y_val):\n",
    "        # Feedforward and compute RMSE\n",
    "        m_val,n_val = X_val.shape\n",
    "        a_list,z_list = self.feedforward(np.c_[X_val].T)\n",
    "        rmse = np.sqrt(np.sum((a_list[-1]-np.c_[y_val].T)**2)/m_val)\n",
    "        # Returns RMSE value and list of output values over arguments data X_val/y_val\n",
    "        return rmse,a_list[-1]\n",
    "    \n",
    "    def J_prime(self, hi, y):\n",
    "        # Derivative of the cost function with respect to output activation\n",
    "        # Here the cost function J is the MSE/2\n",
    "        # Just for one example\n",
    "        return hi-y\n",
    "\n",
    "# Some activation functions and their respective derivatives\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return 2*sigmoid(2*z)-1\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def tanh_prime(z):\n",
    "    return 1-tanh(z)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Debuging\n",
    "\n",
    "Below we define a minimal network of 1 input, 1 hidden and 1 output units that we feed with the value 1.0 to test the `feedforward` method. If you have properly coded that method, you should find the same results listed as `Your a_list/z_list` compared to that provided in the `Correct a_list/z_list` lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your a_list: [1.0, array([[-0.95212792]]), array([[-1.05988594]])]\n",
      "Correct a_list: [1.0, array([[-0.95212792]]), array([[-1.05988594]])]\n",
      "Your z_list: [array([[-1.85407138]]), array([[-1.05988594]])]\n",
      "Correct z_list: [array([[-1.85407138]]), array([[-1.05988594]])]\n",
      "If you do not find similar numbers, you must review your feedforward method.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for the network\n",
    "# Number of input neurons, number of neurons in the hidden layers and number of output neurons\n",
    "n_input, n_hidden1, n_output = 1, 1, 1\n",
    "arch = [n_input, n_hidden1, n_output]\n",
    "# Here we define the network\n",
    "net = Network(arch)\n",
    "a_list, z_list = net.feedforward(1.0)\n",
    "\n",
    "print('Your a_list:',a_list)\n",
    "print('Correct a_list:',[1.0, np.array([[-0.95212792]]), np.array([[-1.05988594]])])\n",
    "\n",
    "print('Your z_list:',z_list)\n",
    "print('Correct z_list:',[np.array([[-1.85407138]]), np.array([[-1.05988594]])])\n",
    "print('If you do not find similar numbers, you must review your feedforward method.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, based on the same network defined above, we feed the backpropagation method with values (1.0,1.0). If you have properly coded the `backprop` method, you should find the same results listed as `Your nabla_b/nabla_w` compared to that provided in the `Correct nabla_b/nabla_w` lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your nabla_b: [array([[0.10980294]]), array([[-2.05988594]])]\n",
      "Correct nabla_b: [array([[0.10980294]]), array([[-2.05988594]])]\n",
      "Your nabla_w: [array([[0.10980294]]), array([[1.96127491]])]\n",
      "Correct nabla_w: [array([[0.10980294]]), array([[1.96127491]])]\n",
      "If you do not find similar numbers, you must review your backprop method.\n"
     ]
    }
   ],
   "source": [
    "nabla_b, nabla_w = net.backprop(np.c_[1],np.c_[1])\n",
    "\n",
    "print('Your nabla_b:',nabla_b)\n",
    "print('Correct nabla_b:',[np.array([[0.10980294]]), np.array([[-2.05988594]])])\n",
    "\n",
    "print('Your nabla_w:',nabla_w)\n",
    "print('Correct nabla_w:',[np.array([[0.10980294]]), np.array([[1.96127491]])])\n",
    "print('If you do not find similar numbers, you must review your backprop method.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training\n",
    "\n",
    "You should have now a working version of the class network. \n",
    "We can now train a neural network to predict the atomization energy of the qm7 dataset.\n",
    "We will try various hyperparameters for the network and select the best set by computing error on the training set.\n",
    "Note that we should be splitting the dataset into training, validation and test set to propery evaluate the performance of the model however we won't do this today.\n",
    "This will be explore further in the next lectures.\n",
    "However, we need to scale the data because the optimization is based on SGD. Let's standardize with sklearn's standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler().fit(np.c_[X])\n",
    "X_train = X_scaler.transform(np.c_[X])\n",
    "print(X_train.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on a network with 2 hidden layers. The network architecture is therefore defined by 4 variables `n_input`, `n_hidden1`, `n_hidden2`, `n_output`. We will vary these 4 variables as follow (note that the input and output layers are fixed and defined by the dataset shape). \n",
    "\n",
    "- X_train.shape[1], 10, 10, 1 (10 neurons in the 1st and 2nd hidden layer)\n",
    "- X_train.shape[1], 50, 50, 1\n",
    "- X_train.shape[1], 100, 100, 1\n",
    "- X_train.shape[1], 200, 200, 1\n",
    "\n",
    "For each network architecture, you will vary the learning rate of the SGD method as $\\alpha$ = 0.01, 0.001, 0.0001.\n",
    "\n",
    "Define the architecture of the network, the hyperparameters for SGD, and optimize each netwroks (12 total).\n",
    "For each network, note in a spreadsheet the final value of the cost in eV. The default values are initialized to the first architecture and first learning rate. Note that to answer the following questions, you should not vary the other hyperparameters (i.e. number of epoch, minibatch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for the network\n",
    "# Number of input neurons, number of neurons in the hidden layers and number of output neurons\n",
    "n_input, n_hidden1, n_hidden2, n_output = X_train.shape[1], 10, 10, 1\n",
    "arch = [n_input, n_hidden1, n_hidden2, n_output]\n",
    "# Here we define the network\n",
    "net = Network(arch)\n",
    "# Hyperparameters for GD\n",
    "# Number of iterations, size of minibatches and learning rate\n",
    "epochs, mini_batch_size, alpha = 20, 10, 1e-1\n",
    "hyper_params = epochs, mini_batch_size, alpha\n",
    "# We optimize the network with SGD\n",
    "rmse_list,y_predict = net.SGD(X_train,y,hyper_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q.5.__ Report the optimal values of the network hyperparameters and learning rate $\\alpha$ that gives the lowest final error (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the optimized hyperparameters rerun the model optimization this time with 100 iteration of SGD (set `epochs` to 100).\n",
    "\n",
    "We can finally plot the following:\n",
    "\n",
    "- the RMSE (the 1st list returned by SGD) as a function of epoch number\n",
    "- the prediction of the network (the 2nd list returned by SGD) against the actual energies over the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(y,y_predict.T,marker = '.',lw = 0,label = 'training data')\n",
    "plt.plot(y,y,color='r',ls='-',label='y=x')\n",
    "plt.legend()\n",
    "plt.ylabel('energies (eV)',fontsize=22)\n",
    "plt.xlabel('predicted energies (eV)',fontsize=22)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rmse_list,marker = '.',color='b',ms=20,ls='-')\n",
    "plt.ylabel('predicted energies (eV)',fontsize=22)\n",
    "plt.xlabel('iterations',fontsize=22)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q.6.__ Based on the last optimization, do you think the model overfit the data? answer by writing `yes` or `no` in the markdown below (1 mark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
