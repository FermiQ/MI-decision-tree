{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e94ff797bbd96cbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 4. Multivariate linear regression\n",
    "\n",
    "#### Table of contents\n",
    "\n",
    "1. Overview\n",
    "2. About pm$_{2.5}$\n",
    "3. Prepare the data\n",
    "4. Univariate linear regression\n",
    "5. Multivariate linear regression\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "In this Lab session we will use multivariate linear regression to predict the PM$_{2.5}$ concentration in Hong Kong based on atmospheric data. Following Lab3, we will extend the codes and functions to compute the cost function and gradient descent to deal with multiple variables.\n",
    "\n",
    "## 2. About pm2.5\n",
    "\n",
    "PM$_{2.5}$ are fine particules with aerodynamic diameters equal to or smaller than 2.5 microns, which is recognized as a major component for air pollution, and has been shown to lead to multiple adverse health outcomes. Usually, the concentration of PM$_{2.5}$ in the air is measured by ground stations and the coverage is extended via spacial interpolations. However, the results may contain uncertainties due to the limited number of monitoring stations and sampling points for the interpolation. To compensate this information gap, satellite, meteorological and additional air quality index data have been used to monitor air quality. In the following lab, we will investigate the relationship between the concentration of PM$_{2.5}$ and air quality index indicators such as the concentration of NO$_2$, O$_3$, etc.\n",
    "\n",
    "## 3. Prepare the data\n",
    "\n",
    "We will use data available from Hong Kong's environmental protection department. Original data can be downloaded from their [website](https://cd.epic.epd.gov.hk/EPICDI/air/station/?lang=en) however, we have already compiled and partially cleaned data between 1 January 2019 to 31 December 2019, recoreded by the central/western station. You can download the csv from the blackboard.\n",
    "\n",
    "__Q.1.__ Load the data, drop the column corresponding to CO chemical (labelled `CO`) which is empty, and drop all rows that have no data. The final dataframe will be stored in the variable `pm25` (2 marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "### BEGIN SOLUTION\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pm25.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pm25.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a time series with date and hours, and various chemical concentration. FSP stands for fine suspended particle and corresponds to the concentration of PM$_{2.5}$. All polluants are given in $\\mu$g/m$^3$. Our goal is to predict FSP based on the concentration of various chemicals NO$_2$, NO$_x$, O$_3$ and SO$_2$. Respirable suspended particulates (RSP) are another type of suspended particles (larger in size) and their concentration strongly correlates with FSP (see below) therefore, we will ignore them for the prediction, focusing only on FSP.\n",
    "\n",
    "Let's have a look at the scatter matrix first to have an idea of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(pm25[['FSP','RSP','NO2','NOX','O3','SO2']],figsize=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pm25.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Univariate linear regression\n",
    "\n",
    "This first attempt in predicting the concentration of PM$_{2.5}$ is based on what we learnt during our previous Lab session. Let's redefine below the model we developed. All key ingredients to perform feature scaling and linear regression have been compiled in functions. You must review the functions carefully and be sure you understand all the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def standard(x):\n",
    "    # Standardize a list x\n",
    "    mu = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    x_std = [(xi-mu)/std for xi in x]\n",
    "    return x_std, mu, std\n",
    "\n",
    "def recover_beta(t0,t1,x,y,stdx,stdy):\n",
    "    # Recover beta linear regression coefficients from scaled ones\n",
    "    m = len(x)\n",
    "    b1 = t1*stdy/stdx\n",
    "    b0 = np.mean([y[i]-b1*x[i] for i in range(m)])\n",
    "    return b0,b1\n",
    "\n",
    "def hypothesis_uni(b0,b1,x):\n",
    "    # The univariate hypothesis\n",
    "    h_uni = [b0+b1*xi for xi in x]\n",
    "    return h_uni\n",
    "\n",
    "def cost_function(h,y):\n",
    "    # Computes square average error\n",
    "    m = len(y)\n",
    "    err = (1.0/(2.0*m))*sum([(h[i]-y[i])**2 for i in range(m)])\n",
    "    return err\n",
    "\n",
    "def update_t0(t0,alpha,h,y):\n",
    "    # Update t0 coefficient during GD\n",
    "    m = len(y)\n",
    "    grad_t0 = (1.0/m)*sum([h[i]-y[i] for i in range(m)])\n",
    "    new_t0 = t0-alpha*grad_t0\n",
    "    return new_t0\n",
    "\n",
    "def update_t1(t1,alpha,h,x,y):\n",
    "    # Update t1 coefficient during GD\n",
    "    m = len(y)\n",
    "    grad_t1 = (1.0/m)*sum([x[i]*(h[i]-y[i]) for i in range(m)])\n",
    "    new_t1 = t1-alpha*grad_t1\n",
    "    return new_t1\n",
    "\n",
    "def gd(x,y,Niter):\n",
    "    # Gradient descent\n",
    "    t0, t1 = 0,0\n",
    "    alpha = 0.1\n",
    "    for step in range(1,Niter):\n",
    "        h = hypothesis_uni(t0,t1,x)\n",
    "        t0 = update_t0(t0,alpha,h,y)\n",
    "        t1 = update_t1(t1,alpha,h,x,y)\n",
    "        h = hypothesis_uni(t0,t1,x)\n",
    "        err = cost_function(h,y)\n",
    "        print(step,t0,t1,err)\n",
    "    print(\"Final values of the coefficients t0 and t1:\", t0, t1)\n",
    "    return t0,t1\n",
    "    \n",
    "def plot_data(x,y,t0,t1):\n",
    "    # Plot y = x data and a the line t0+t1*x\n",
    "    plt.plot(x,y,marker='.',lw=0,label=\"data\")\n",
    "    x_fit = np.linspace(min(x),max(x),100)\n",
    "    y_fit = [t0+t1*xi for xi in x_fit]\n",
    "    plt.plot(x_fit,y_fit,lw=1,label=\"linear fit\",color='r')\n",
    "    plt.xlabel('NO2')\n",
    "    plt.ylabel('FSP')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's select the concentration of NO$_2$ as the input feature and try to linearly fit the concentration of PM$_{2.5}$ with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = list(pm25['NO2'])\n",
    "y = list(pm25['FSP'])\n",
    "\n",
    "x_std, mux, stdx = standard(x)\n",
    "y_std, muy, stdy = standard(y)\n",
    "t0,t1 = gd(x_std,y_std,100)\n",
    "b0,b1 = recover_beta(t0,t1,x,y,stdx,stdy)\n",
    "print(\"Coefficients b0 and b1 corresponding to the best linear fit:\",b0,b1)\n",
    "plot_data(x,y,b0,b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the Pearson correlation between NO$_2$ and FPS was approximately 0.5 hence a poor linear fit.\n",
    "To evaluate how well observed outcomes are replicated by the model, we can use various quantitative and qualitative analysis:\n",
    "\n",
    "- the final cost\n",
    "- r2 score\n",
    "- plots of model vs prediction\n",
    "\n",
    "This is illustrated in the following.\n",
    "The final cost i.e. the mean square average or its square root i.e. the root mean square average (RMS) provide information about the error between the model and data however, it is relative to the actual values in the dataset.\n",
    "Therefore, in linear regression, we often compute the coefficient of determination (R2) defined as:\n",
    "\n",
    "$R^2 = 1-\\frac{SS_{res}}{SS_{tot}}$\n",
    "\n",
    "with $SS_{res}$, the sum of squares of residuals, also called the residual sum of squares (proportional to the cost function):\n",
    "\n",
    "$SS_{res} = \\sum_i\\left(y^{(i)}-h^{(i)}\\right)^2$\n",
    "\n",
    "and, $SS_{tot}$ the total sum of squares (proportional to the variance of the data):\n",
    "\n",
    "$SS_{tot} = \\sum_i\\left(y^{(i)}-\\mu_y\\right)^2$\n",
    "\n",
    "This leads to a residual $R2$ being a value between 0 and 1. The closest value to 1 indicates a better fit.\n",
    "\n",
    "__Q.2.__ Complete the function r2 below that computes the coefficient of determination based on values of the output data `y` and the hypothesis `h`, both lists of length `m` (the number of examples in the dataset) (2 marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def r2(y,h):\n",
    "    m = len(y)\n",
    "    mu = np.mean(y)\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return 1-ss_res/ss_tot\n",
    "\n",
    "h_uni = hypothesis_uni(b0,b1,x)\n",
    "print(\"R2 score :{:.4f}\".format(r2(y,h_uni)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a great value for R2 and we will try to improve our model predicton later by introducing additional features.\n",
    "\n",
    "Another qualitative way to appreciate the model accuracy is to plot the actual data and the predicted data. Here we have data measured over time so we can represent the concentration of PM$_{2.5}$ and the value predicted by the linear model as a function of time. Each row in the dataframe represent one hour, we will then use the elapsed time since the first data point (1 Jan 2019, midnight) as the x-axis. Moreover, it can be interesting to zoom in a time period so we define an initial and final time in hour `Ni` and `Nf`, respectively. According to the information we know on the dataframe we have approximately 8300 rows hence, a little less than a full year in hour (365x24) because of missing value we deleted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ni = 0\n",
    "Nf = 8300\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(range(Ni,Nf),y[Ni:Nf],marker='.',ms=0,color='r',lw=1.0,label='data')\n",
    "plt.plot(range(Ni,Nf),h_uni[Ni:Nf],marker='o',ms=0,color='b',lw=1.0,label='univariate model prediction')\n",
    "plt.legend()\n",
    "plt.ylabel(\"PM$_{25}$ ($\\mu$g/m$^3$)\",fontsize=22)\n",
    "plt.xlabel(\"time since Jan 1st (h)\",fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the output data (here PM$_{2.5}$) as a function of the predicted output based on the linear model. Moreover, it is common to add the line `y = x` corresponding to a perfect model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y,h_uni,marker='.',color='r',lw=0.0,ms=2.0)\n",
    "plt.plot(range(150),range(150),color='k',lw=0.5,label='y = x')\n",
    "plt.xlabel('PM predicted',fontsize=22)\n",
    "plt.ylabel('PM data',fontsize=22)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most of the red dots appear below the `y=x` line, this means we often overestimate the actual value of PM$_{2.5}$ concentration. This can also be appreciated in the time series plot.\n",
    "Overall, the univariate linear model somehow reflects the variation of the PM$_{2.5}$ concentration but with limited accuracy.\n",
    "\n",
    "Note that here we actually train and test the model on the same dataset. The proper way of evaluating the performance of a model is to have separated training and testing datasets. The model is trained on the training set and the model performance is evaluated on the test set. This is a very important point that we will explore further in the class.\n",
    "\n",
    "## 5. Multivariate linear regression\n",
    "\n",
    "Let's now modify the functions to perform multivariate linear regression.\n",
    "The goal is to modify the previous functions to take as input not just a feature vector but a feature matrix (often called design matrix). To illustrate this, we will only consider 4 features however, the model must be general an valid for `n` features. The mulitvariate linear regression hypothesis can be written as follow:\n",
    "\n",
    "$h(x_0,x_1,x_2,x_3,x_4) = \\theta_0x_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_3+\\theta_4x_4$\n",
    "\n",
    "Or in matrix notation:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "h(X^{(1)})\\\\\n",
    "h(X^{(2)})\\\\\n",
    "\\vdots\\\\\n",
    "h(X^{(m)})\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\theta_0\\\\\n",
    "\\theta_1\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\\\\\n",
    "\\theta_4\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & x_1^{(1)} & x_2^{(1)} & x_3^{(1)} & x_4^{(1)}\\\\\n",
    "1 & x_1^{(2)} & x_2^{(2)} & x_3^{(2)} & x_4^{(2)}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_1^{(m)} & x_2^{(m)} & x_3^{(m)} & x_4^{(m)}\n",
    "\\end{bmatrix}= \\Theta X\n",
    "\\end{align}\n",
    "\n",
    "First we prepare the design matrix `X` as a 2D numpy array.  As dicussed in the class, we need to add a row of ones to the design matrix X to account for the intercept in the hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pm25[['NO2','NOX','O3','SO2']].to_numpy()\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "y = list(pm25['FSP'])\n",
    "\n",
    "print(\"Shape as row, columns\",X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple features, it is fundamental to scale data before we use gradient descent. We will use standardization.\n",
    "\n",
    "__Q.3.__ Complete the function `standard_multi` below that apply standardization to each column (but the first) of a given feature matrix `X`. The function must return the standardized design matrix and a list of mean and standard deviation for each column of the matrix. We will assume that the mean and standard deviation of the first row are equal to 1 and 1, respectively (2 marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standard_multi(X):\n",
    "    m,n = X.shape\n",
    "    X_std = np.ones(X.shape) # The standardize feature matrix initialized with ones\n",
    "    # The following 2 lists will contain the mean and standard deviation of each column\n",
    "    # We initialize the lists with the mean and standardization of the first column as ones\n",
    "    mu,std = [1],[1] \n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return X_std, mu, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q.4.__ Define the hypothesis function `hypothesis_multi` that returns a list of the hypothesis evaluated for each row of the feature matrix. The list of hypothesis must therefore be of length `m`; the number of examples in the dataset or rows of the feature matrix (2 marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hypothesis_multi(ts,X):\n",
    "    m,n = X.shape\n",
    "    h = []\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q.5.__ Complete the function `update_ts` that updates the coefficients theta during gradient descent. This function takes in a list of the theta values, the learning rate (alpha), the hypothesis list evaluated previously based on previous theta values, the feature matrix `X`, and the list of output values `y`. The function should return a list of updated values of theta. You should look at the `gd` function below to understand better the role of the `update_ts` function (2 marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_ts(ts,alpha,h,X,y):\n",
    "    m,n = X.shape\n",
    "    grads = []\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    for i in range(n):\n",
    "        ts[i] = ts[i]-alpha*grads[i]\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we provide the gradient descent function. You should minimize the cost and obtain the parameters $\\Theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gd(X,y,Niter):\n",
    "    ts = [0]*X.shape[1]\n",
    "    alpha = 0.1\n",
    "    for step in range(1,Niter):\n",
    "        h = hypothesis_multi(ts,X)\n",
    "        ts = update_ts(ts,alpha,h,X,y)\n",
    "        h = hypothesis_multi(ts,X)\n",
    "        err = cost_function(h,y)\n",
    "        print(step,ts,err)\n",
    "    print(\"Final values of the coefficients ts and MSE:\", ts,err*2.0)\n",
    "    return ts\n",
    "\n",
    "X_std,mux,stdx = standard_multi(X)\n",
    "y_std,muy,stdy = standard(y)\n",
    "ts = gd(X_std,y_std,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now retrive the unscaled coeffients $\\beta$ and plot the time series together with the univariate linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrive the actual coefficients b0, b1\n",
    "m,n = X.shape\n",
    "bs = [ts[i]*stdy/stdx[i] for i in range(n)]\n",
    "bs[0] = np.mean([y[j]-sum([bs[i]*X[j][i] for i in range(1,n)]) for j in range(m)])\n",
    "h_multi = hypothesis_multi(bs,X)\n",
    "\n",
    "print(\"R2 score uni: {:.4f} and final cost: {:.4f}\".format(r2(y,h_uni),cost_function(h_uni,y)))\n",
    "print(\"R2 score multi: {:.4f} and final cost: {:.4f}\".format(r2(y,h_multi),cost_function(h_multi,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ni = 0\n",
    "Nf = 8300\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(range(Ni,Nf),y[Ni:Nf],marker='.',ms=0,color='r',lw=1.0,label='data')\n",
    "plt.plot(range(Ni,Nf),h_uni[Ni:Nf],marker='o',ms=0,color='b',lw=1.0,label='uni model')\n",
    "plt.plot(range(Ni,Nf),h_multi[Ni:Nf],marker='o',ms=0,color='g',lw=1.0,label='multi model')\n",
    "plt.legend()\n",
    "plt.ylabel(\"PM$_{25}$ ($\\mu$g/m$^3$)\",fontsize=22)\n",
    "plt.xlabel(\"time since Jan 1st (h)\",fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y,h_uni,marker='.',color='r',lw=0.0,ms=2.0,label='uni')\n",
    "plt.plot(y,h_multi,marker='.',color='g',lw=0.0,ms=2.0,label='multi')\n",
    "plt.plot(range(150),range(150),color='k',lw=0.5,label='y = x')\n",
    "plt.xlabel('PM predicted',fontsize=22)\n",
    "plt.ylabel('PM data',fontsize=22)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From both plots, we can appreciate the improvement in the predicted PM$_{2.5}$ based on the multivariate linear regression compared to univariate case."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
