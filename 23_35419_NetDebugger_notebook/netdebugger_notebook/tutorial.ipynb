{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is a tutorial on NetDebugger\n",
    "Author: Rishi Gurnani, Georgia Institute of Technology<br />\n",
    "Creation Date: July 21, 2021 4:54 PM EST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4.0\n",
      "Python 3.8.10\n",
      "2.2.0\n",
      "2.4.3\n",
      "1.2.4\n"
     ]
    }
   ],
   "source": [
    "!jupyter notebook --version\n",
    "!python --version\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "#import sklearn\n",
    "#print(sklearn.__version__)\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "import pandas as pd \n",
    "print(pd.__version__)\n",
    "# 6.4.0\n",
    "# Python 3.8.10\n",
    "# 2.2.0\n",
    "# 2.4.3\n",
    "# 1.2.4\n",
    "###################################################################\n",
    "#https://nanohub.org/tools/jupyter70\n",
    "#Jupyter Notebook with Anaconda 2020.11\n",
    "#6.4.0, Python 3.8.10, tf 2.2.0, sklearn 1.0, keras 2.4.3 pd 1.2.4\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\n",
      "---------------------------------- -------------------\n",
      "absl-py                            0.12.0\n",
      "aiohttp                            3.7.4\n",
      "alabaster                          0.7.12\n",
      "anaconda-client                    1.7.2\n",
      "anaconda-navigator                 2.0.3\n",
      "anaconda-project                   0.10.0\n",
      "ansiwrap                           0.8.4\n",
      "anyio                              3.1.0\n",
      "appdirs                            1.4.4\n",
      "appmode                            0.8.0\n",
      "argh                               0.26.2\n",
      "argon2-cffi                        20.1.0\n",
      "arrow                              1.1.0\n",
      "ase                                3.21.1\n",
      "asn1crypto                         1.4.0\n",
      "astor                              0.8.1\n",
      "astroid                            2.5.7\n",
      "astropy                            4.2.1\n",
      "astunparse                         1.6.3\n",
      "async-generator                    1.10\n",
      "async-timeout                      3.0.1\n",
      "atomicwrites                       1.4.0\n",
      "attrs                              21.2.0\n",
      "autopep8                           1.5.5\n",
      "Babel                              2.9.1\n",
      "backcall                           0.2.0\n",
      "backports.functools-lru-cache      1.6.4\n",
      "backports.shutil-get-terminal-size 1.0.0\n",
      "beautifulsoup4                     4.9.3\n",
      "binaryornot                        0.4.4\n",
      "bitarray                           2.0.1\n",
      "bkcharts                           0.2\n",
      "black                              21.5b2\n",
      "bleach                             3.3.0\n",
      "blinker                            1.4\n",
      "bokeh                              2.3.2\n",
      "boto                               2.49.0\n",
      "Bottleneck                         1.3.2\n",
      "brotlipy                           0.7.0\n",
      "bumps                              0.8.0\n",
      "cached-property                    1.5.2\n",
      "cachetools                         4.2.2\n",
      "certifi                            2021.5.30\n",
      "cffi                               1.14.5\n",
      "chardet                            4.0.0\n",
      "click                              8.0.1\n",
      "cloudpickle                        1.6.0\n",
      "clyent                             1.2.2\n",
      "colorama                           0.4.4\n",
      "conda                              4.10.3\n",
      "conda-build                        3.21.4\n",
      "conda-content-trust                0.1.3\n",
      "conda-pack                         0.6.0\n",
      "conda-package-handling             1.7.3\n",
      "conda-repo-cli                     1.0.4\n",
      "conda-token                        0.3.0\n",
      "conda-verify                       3.1.1\n",
      "contextlib2                        0.6.0.post1\n",
      "cookiecutter                       1.6.0\n",
      "coverage                           5.5\n",
      "cryptography                       3.4.7\n",
      "cycler                             0.10.0\n",
      "Cython                             0.29.23\n",
      "cytoolz                            0.11.0\n",
      "dask                               2021.5.1\n",
      "decorator                          4.4.2\n",
      "deepchem                           2.5.0\n",
      "defusedxml                         0.7.1\n",
      "diff-match-patch                   20200713\n",
      "distributed                        2021.5.1\n",
      "docutils                           0.17.1\n",
      "entrypoints                        0.3\n",
      "et-xmlfile                         1.0.1\n",
      "fastcache                          1.1.0\n",
      "filelock                           3.0.12\n",
      "flake8                             3.8.4\n",
      "Flask                              2.0.1\n",
      "fsspec                             2021.5.0\n",
      "future                             0.18.2\n",
      "gast                               0.3.3\n",
      "gevent                             21.1.2\n",
      "glob2                              0.7\n",
      "gmpy2                              2.1.0b1\n",
      "google-auth                        1.30.0\n",
      "google-auth-oauthlib               0.4.1\n",
      "google-pasta                       0.2.0\n",
      "googledrivedownloader              0.4\n",
      "greenlet                           1.1.0\n",
      "grpcio                             1.38.0\n",
      "h5py                               2.10.0\n",
      "HeapDict                           1.0.1\n",
      "html5lib                           1.1\n",
      "hublib                             0.9.96\n",
      "idna                               2.10\n",
      "imagecodecs                        2021.3.31\n",
      "imageio                            2.9.0\n",
      "imagesize                          1.2.0\n",
      "importlib-metadata                 4.4.0\n",
      "inflection                         0.5.1\n",
      "iniconfig                          1.1.1\n",
      "intervaltree                       3.0.2\n",
      "ipykernel                          5.5.5\n",
      "ipython                            7.24.1\n",
      "ipython-genutils                   0.2.0\n",
      "ipywidgets                         7.6.3\n",
      "isodate                            0.6.0\n",
      "isort                              5.8.0\n",
      "itsdangerous                       2.0.1\n",
      "jdcal                              1.4.1\n",
      "jedi                               0.17.2\n",
      "jeepney                            0.6.0\n",
      "Jinja2                             3.0.1\n",
      "jinja2-time                        0.2.0\n",
      "joblib                             1.0.1\n",
      "json5                              0.9.5\n",
      "jsonpickle                         2.0.0\n",
      "jsonschema                         3.2.0\n",
      "jupyter                            1.0.0\n",
      "jupyter-client                     6.1.12\n",
      "jupyter-console                    6.4.0\n",
      "jupyter-core                       4.7.1\n",
      "jupyter-server                     1.8.0\n",
      "jupyterlab                         3.0.16\n",
      "jupyterlab-pygments                0.1.2\n",
      "jupyterlab-server                  2.6.0\n",
      "jupyterlab-widgets                 1.0.0\n",
      "Keras                              2.4.3\n",
      "Keras-Preprocessing                1.1.2\n",
      "keyring                            23.0.1\n",
      "kiwisolver                         1.3.1\n",
      "lazy-object-proxy                  1.6.0\n",
      "libarchive-c                       3.1\n",
      "llvmlite                           0.36.0\n",
      "locket                             0.2.0\n",
      "lxml                               4.6.3\n",
      "Markdown                           3.3.4\n",
      "MarkupSafe                         2.0.1\n",
      "matlab-kernel                      0.16.11\n",
      "matlabengineforpython              R2021a\n",
      "matplotlib                         3.4.2\n",
      "matplotlib-inline                  0.1.2\n",
      "mccabe                             0.6.1\n",
      "megnet                             1.2.9\n",
      "mendeleev                          0.7.0\n",
      "metakernel                         0.27.5\n",
      "mistune                            0.8.4\n",
      "mkl-fft                            1.3.0\n",
      "mkl-random                         1.2.2\n",
      "mkl-service                        2.4.0\n",
      "mock                               4.0.3\n",
      "monty                              2021.8.17\n",
      "more-itertools                     8.8.0\n",
      "mpmath                             1.2.1\n",
      "msgpack                            1.0.2\n",
      "multidict                          5.1.0\n",
      "multipledispatch                   0.6.0\n",
      "mypy-extensions                    0.4.3\n",
      "navigator-updater                  0.2.1\n",
      "nbclassic                          0.3.1\n",
      "nbconvert                          5.6.1\n",
      "nbformat                           4.4.0\n",
      "nest-asyncio                       1.5.1\n",
      "networkx                           2.5.1\n",
      "nltk                               3.6.2\n",
      "nose                               1.3.7\n",
      "notebook                           6.4.0\n",
      "nteract-scrapbook                  0.4.2\n",
      "numba                              0.53.1\n",
      "numexpr                            2.7.3\n",
      "numpy                              1.20.2\n",
      "numpydoc                           1.1.0\n",
      "oauthlib                           3.1.1\n",
      "olefile                            0.46\n",
      "opencv-python                      4.5.2.54\n",
      "openpyxl                           3.0.7\n",
      "opt-einsum                         3.3.0\n",
      "packaging                          20.9\n",
      "palettable                         3.3.0\n",
      "pandas                             1.2.4\n",
      "pandocfilters                      1.4.2\n",
      "papermill                          1.1.0\n",
      "parso                              0.7.0\n",
      "partd                              1.2.0\n",
      "path                               15.1.2\n",
      "pathlib2                           2.3.5\n",
      "pathspec                           0.8.1\n",
      "pathtools                          0.1.2\n",
      "patsy                              0.5.1\n",
      "pbr                                5.6.0\n",
      "pep8                               1.7.1\n",
      "pexpect                            4.8.0\n",
      "pickleshare                        0.7.5\n",
      "Pillow                             8.2.0\n",
      "Pint                               0.17\n",
      "pip                                21.1.2\n",
      "pkginfo                            1.7.0\n",
      "plotly                             4.14.3\n",
      "plotly-express                     0.4.1\n",
      "pluggy                             0.13.1\n",
      "ply                                3.11\n",
      "polyga                             1.0.12\n",
      "pooch                              1.3.0\n",
      "poyo                               0.5.0\n",
      "prometheus-client                  0.11.0\n",
      "prompt-toolkit                     3.0.18\n",
      "protobuf                           3.17.2\n",
      "psutil                             5.8.0\n",
      "ptyprocess                         0.7.0\n",
      "py                                 1.10.0\n",
      "pyarrow                            4.0.1\n",
      "pyasn1                             0.4.8\n",
      "pyasn1-modules                     0.2.7\n",
      "pycairo                            1.20.0\n",
      "pycalphad                          0.9.2\n",
      "pycodestyle                        2.6.0\n",
      "pycosat                            0.6.3\n",
      "pycparser                          2.20\n",
      "pycurl                             7.43.0.6\n",
      "pydocstyle                         6.1.1\n",
      "pyerfa                             1.7.3\n",
      "pyfiglet                           0.8.post1\n",
      "pyflakes                           2.2.0\n",
      "Pygments                           2.9.0\n",
      "PyJWT                              2.1.0\n",
      "pylint                             2.7.2\n",
      "pyls-black                         0.4.6\n",
      "pyls-spyder                        0.3.2\n",
      "pymatgen                           2022.0.16\n",
      "pyodbc                             4.0.30\n",
      "pyOpenSSL                          20.0.1\n",
      "pyparsing                          2.4.7\n",
      "pyrsistent                         0.17.3\n",
      "pysmiles                           1.0.1\n",
      "PySocks                            1.7.1\n",
      "pytest                             6.2.4\n",
      "pytest-cov                         2.12.1\n",
      "python-dateutil                    2.8.1\n",
      "python-jsonrpc-server              0.4.0\n",
      "python-language-server             0.36.2\n",
      "python-louvain                     0.15\n",
      "pytz                               2021.1\n",
      "PyWavelets                         1.1.1\n",
      "pyxdg                              0.27\n",
      "PyYAML                             5.4.1\n",
      "pyzmq                              22.1.0\n",
      "QDarkStyle                         3.0.2\n",
      "qstylizer                          0.2.0\n",
      "QtAwesome                          1.0.2\n",
      "qtconsole                          5.1.0\n",
      "QtPy                               1.9.0\n",
      "rdflib                             5.0.0\n",
      "regex                              2021.4.4\n",
      "requests                           2.25.1\n",
      "requests-oauthlib                  1.3.0\n",
      "retrying                           1.3.3\n",
      "rope                               0.19.0\n",
      "rsa                                4.7.2\n",
      "Rtree                              0.9.7\n",
      "ruamel-yaml-conda                  0.15.80\n",
      "ruamel.yaml                        0.17.17\n",
      "ruamel.yaml.clib                   0.2.6\n",
      "scikit-image                       0.18.1\n",
      "scikit-learn                       0.24.2\n",
      "scipy                              1.6.2\n",
      "seaborn                            0.11.1\n",
      "SecretStorage                      3.3.1\n",
      "Send2Trash                         1.5.0\n",
      "setuptools                         49.6.0.post20210108\n",
      "setuptools-scm                     6.3.2\n",
      "simplegeneric                      0.8.1\n",
      "simtool                            0.3.3\n",
      "singledispatch                     0.0.0\n",
      "sip                                4.19.13\n",
      "six                                1.16.0\n",
      "sniffio                            1.2.0\n",
      "snowballstemmer                    2.1.0\n",
      "sortedcollections                  2.1.0\n",
      "sortedcontainers                   2.4.0\n",
      "soupsieve                          2.0.1\n",
      "spglib                             1.16.2\n",
      "Sphinx                             4.0.2\n",
      "sphinxcontrib-applehelp            1.0.2\n",
      "sphinxcontrib-devhelp              1.0.2\n",
      "sphinxcontrib-htmlhelp             2.0.0\n",
      "sphinxcontrib-jsmath               1.0.1\n",
      "sphinxcontrib-qthelp               1.0.3\n",
      "sphinxcontrib-serializinghtml      1.1.5\n",
      "sphinxcontrib-websupport           1.2.4\n",
      "spyder                             5.0.3\n",
      "spyder-kernels                     2.0.3\n",
      "SQLAlchemy                         1.4.17\n",
      "statsmodels                        0.12.2\n",
      "symengine                          0.7.2\n",
      "sympy                              1.8\n",
      "tables                             3.6.1\n",
      "tabulate                           0.8.9\n",
      "tblib                              1.7.0\n",
      "tenacity                           7.0.0\n",
      "tensorboard                        2.4.1\n",
      "tensorboard-plugin-wit             1.8.0\n",
      "tensorflow                         2.2.0\n",
      "tensorflow-estimator               2.4.0\n",
      "termcolor                          1.1.0\n",
      "terminado                          0.10.0\n",
      "testpath                           0.5.0\n",
      "textdistance                       4.2.1\n",
      "textwrap3                          0.9.2\n",
      "threadpoolctl                      2.1.0\n",
      "three-merge                        0.1.1\n",
      "tifffile                           2021.4.8\n",
      "tinycss2                           1.1.0\n",
      "tinydb                             4.5.1\n",
      "toml                               0.10.2\n",
      "tomli                              1.2.1\n",
      "toolz                              0.11.1\n",
      "torch                              1.8.1\n",
      "torch-geometric                    1.7.0\n",
      "torch-scatter                      2.0.6\n",
      "torch-sparse                       0.6.9\n",
      "tornado                            6.1\n",
      "tqdm                               4.61.0\n",
      "traitlets                          5.0.5\n",
      "typed-ast                          1.4.3\n",
      "typing-extensions                  3.10.0.0\n",
      "ujson                              4.0.2\n",
      "uncertainties                      3.1.6\n",
      "unicodecsv                         0.14.1\n",
      "urllib3                            1.26.5\n",
      "watchdog                           1.0.2\n",
      "wcwidth                            0.2.5\n",
      "webencodings                       0.5.1\n",
      "websocket-client                   0.57.0\n",
      "Werkzeug                           2.0.1\n",
      "wheel                              0.36.2\n",
      "whichcraft                         0.6.1\n",
      "widgetsnbextension                 3.5.1\n",
      "wrapt                              1.12.1\n",
      "wurlitzer                          2.1.0\n",
      "xarray                             0.19.0\n",
      "xlrd                               2.0.1\n",
      "XlsxWriter                         1.4.3\n",
      "xlwt                               1.3.0\n",
      "xmltodict                          0.12.0\n",
      "yamlmagic                          0.2.0\n",
      "yapf                               0.31.0\n",
      "yarl                               1.6.3\n",
      "zict                               2.0.0\n",
      "zipp                               3.4.1\n",
      "zope.event                         4.5.0\n",
      "zope.interface                     5.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip list\n",
    "#!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import\n",
    "Some python packages are needed to run this notebook. We import all of those below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch import tensor, cuda, manual_seed, zeros, nn, optim\n",
    "from torch import float as torch_float\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch import device as torch_device\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# nndebugger functions\n",
    "from bin.nndebugger import constants, loss, dl_debug\n",
    "from bin.nndebugger import torch_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix random seeds to ensure reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(constants.RANDOM_SEED)\n",
    "manual_seed(constants.RANDOM_SEED)\n",
    "np.random.seed(constants.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>property</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>[*]C[*]</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.8972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>[*]CC([*])C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>[*]CC([*])CC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>[*]CC([*])CCC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>[*]CC([*])CC(C)C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               smiles property   value\n",
       "822           [*]C[*]      Egc  6.8972\n",
       "823       [*]CC([*])C      Egc  6.5196\n",
       "824      [*]CC([*])CC      Egc  6.5170\n",
       "825     [*]CC([*])CCC      Egc  6.7336\n",
       "826  [*]CC([*])CC(C)C      Egc  6.7394"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('data/export.csv',index_col=0)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3380"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = 512\n",
    "N_DATA = len(data_df)\n",
    "\n",
    "def featurize_smiles(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=N_FEATURES, useChirality=True)\n",
    "    return np.array(fp)\n",
    "\n",
    "feature_array = np.zeros((N_DATA, N_FEATURES))\n",
    "\n",
    "ind = 0\n",
    "for smiles in data_df.smiles.values:\n",
    "    feature_array[ind,:] = featurize_smiles(smiles)\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a logical architecture that will pass all test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity): # output_dim as only one property bandgap we looking\n",
    "\n",
    "        super(MyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):  # pytorch, how to forward path to occure\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):  # loop over all the hidden layers, inside each layer do matric multiplication\n",
    "            x = self.layers[i](x)          # matric multiplication\n",
    "            if i < (self.n_hidden - 1):    # checked to see if layers and hidden layer.\n",
    "                x = self.relu(x)           #  nd if it is a hidden layer,  then I will do regular non-linearity\n",
    "   \n",
    "        return x.view(data.num_graphs,)  #this is the return line of the view method The View method ensures that the output of the, of my net match, the shape of the labels.\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]  #   I create a list of functions called correct model class LS, the first function in the list Returns on my net of capacity one, the second returns that - of capacity 2 and the third returns a my net of capacity three.\n",
    "correct_model_class_ls = [lambda : MyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare inputs for DebugSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bug free processing pipeline! \n",
    "# just responsible for massaging. Some inputs. That will need to run that netdebugger\n",
    "model_type = 'mlp'\n",
    "# data_set\n",
    "n_test = int(np.floor(N_DATA*constants.TRAIN_FRAC))\n",
    "n_train = N_DATA - n_test\n",
    "(X_train, X_test, label_train, \n",
    "label_test) = train_test_split(\n",
    "                                    feature_array,\n",
    "                                    data_df.value.values.tolist(),\n",
    "                                    test_size=n_test,\n",
    "                                    shuffle=True,\n",
    "                                    random_state=constants.RANDOM_SEED\n",
    "                                )\n",
    "\n",
    "train_X = [Data(x=tensor(X_train[ind,:], dtype=torch_float).view(1,N_FEATURES),\n",
    "                y=tensor(label_train[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(n_train)]\n",
    "data_set = train_X\n",
    "zero_data_set = [Data(x=zeros((1,N_FEATURES)), y=x.y) for x in train_X]\n",
    "loss_fn = loss.st_loss()\n",
    "target_mean = np.mean(label_train)\n",
    "device = torch_device('cuda' if cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test output shape\n",
    "\n",
    "The shape of the model output should match the shape of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Verified that shape of model predictions is equal to shape of labels\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "source": [
    "# this cell should pass since it uses a bug-free model\n",
    "# In order to run this test. We need to instantiate what I call a net debugger, debug session\n",
    "# some of our inputs, like our mynet architecture and our polymer band Gap data.\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn,\n",
    "                 device, do_test_output_shape=True) # only do_test_output_shape is true and all other are false.\n",
    "ds.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buggy model. Can you spot the bug?\n",
    "# buggy net same as my net above only line 35 is change This is the return line of the forward method line\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x # Spoiler! The bug is here. The correct line is 'return x.view(data.num_graphs,)' \n",
    "# This is the return line of the forward method line\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data contains 676 points\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/share64/debian10/anaconda/anaconda-7/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e94700ea8c33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn,\n\u001b[1;32m      4\u001b[0m                  device, do_test_output_shape=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# pytorch, doesn't stop the training by raising an error. While not debugger, will stop the training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/23_35419_NetDebugger_notebook/netdebugger_notebook/bin/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_target_abs_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_abs_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_output_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;31m# self.grad_check(min_model, file_name='first_grad_check.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# print('\\nSet of gradients plotted to first_grad_check.png\\n', flush=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/23_35419_NetDebugger_notebook/netdebugger_notebook/bin/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_output_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"The model output shape {self.output.shape} and label shape {self.data.y.shape} are not the same\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nVerified that shape of model predictions is equal to shape of labels\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same"
     ]
    }
   ],
   "source": [
    "# this cell should NOT pass since it uses a buggy model \n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()\n",
    "# pytorch, doesn't stop the training by raising an error. While not debugger, will stop the training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test input independent baseline\n",
    "The loss of the model should be lower when real features are passed in than when zeroed features are passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer without bugs!\n",
    "#  input, independent Baseline, this test, make sure that the loss of the model is lower \n",
    "#when real features are passed in as opposed to when zeroed out features are passed in.\n",
    "\n",
    "# The trainer is a function that is responsible for updating the parameters of the model based on training data.\n",
    "\n",
    "\n",
    "def trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):  # Loop over a specified number epochs\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass  # within epochs line 14, performs the forwardpass\n",
    "            loss = loss_obj(output, data) # compute loss  # computes the loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            loss.backward() # perform backward pass performs the backward pass \n",
    "            optimizer.step() # update weights and line 18 updates the parameters\n",
    "        loss_history.append(per_epoch_loss) # we track with the loss has been over each epoch\n",
    "    \n",
    "    return loss_history   # return the loss of each at Bach\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 0.22568964213132858\n",
      "..last epoch zero_data_loss 14.373095512390137\n",
      "Input-independent baseline is verified\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "source": [
    "# this test should pass since we are using a trainer without bugs\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=trainer) # do_test_input_independent_baseline=True\n",
    "ds.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer with bugs! Can you spot the bug?\n",
    "# buggy trainer. It is exactly the same as trainer. Except for one difference.\n",
    "def buggy_trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history\n",
    "\n",
    "# not included a line to do the backward pass, and this will definitely negatively affect model training because it means that we're not computing gradients.\n",
    "# Spoiler! The bug is that there is no backward pass being performed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 134.40143203735352\n",
      "..last epoch zero_data_loss 134.75465393066406\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e1a846772a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn,\n\u001b[1;32m      4\u001b[0m                  device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/23_35419_NetDebugger_notebook/netdebugger_notebook/bin/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_input_independent_baseline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_input_independent_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/23_35419_NetDebugger_notebook/netdebugger_notebook/bin/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_input_independent_baseline\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..last epoch zero_data_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_loss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#loss for all points in 5th epoch gets printed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mzero_loss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDL_DBG_IIB_THRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             raise ValueError('''The loss of zeroed inputs is nearly the same as the loss of\n\u001b[0m\u001b[1;32m    155\u001b[0m                     \u001b[0mreal\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mindicate\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0manything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                     during training. Check your trainer function and your model architecture.'''\n",
      "\u001b[0;31mValueError\u001b[0m: The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture."
     ]
    }
   ],
   "source": [
    "# this test should NOT pass since we are using a buggy trainer\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n",
    "ds.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit small batch\n",
    "If you hope to learn a good map on your whole data set using model archicture ***A***, then ***A*** should have enough capacity to completely overfit a small batch of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "..Epoch 0\n",
      "....Outputs -0.1179 -0.0832 -0.0910 -0.1023 -0.1179\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "....Loss: 5.164145787847168\n",
      "....R2: -24.459165859291733\n",
      "..Epoch 1\n",
      "....Outputs -0.0351 -0.0503 -0.0165 -0.0337 -0.0503\n",
      "....Labels  2.9694 5.7012 5.3739 5.0497 5.6991\n",
      "....Loss: 5.1002019730011945\n",
      "....R2: -23.832585224647396\n",
      "..Epoch 2\n",
      "....Outputs 0.0199 0.0257 0.0519 0.0095 0.0095\n",
      "....Labels  5.0497 2.9694 5.3739 5.7012 5.6991\n",
      "....Loss: 5.040911194917921\n",
      "....R2: -23.258576547216546\n",
      "..Epoch 3\n",
      "....Outputs 0.0799 0.1340 0.0778 0.0778 0.0908\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "....Loss: 4.973188421236432\n",
      "....R2: -22.611143571313786\n",
      "..Epoch 4\n",
      "....Outputs 0.2393 0.1645 0.1709 0.1514 0.1645\n",
      "....Labels  5.3739 5.6991 2.9694 5.0497 5.7012\n",
      "....Loss: 4.888306674583275\n",
      "....R2: -21.81203734875709\n",
      "..Epoch 5\n",
      "....Outputs 0.2755 0.2755 0.2396 0.2719 0.3727\n",
      "....Labels  5.6991 5.7012 5.0497 2.9694 5.3739\n",
      "....Loss: 4.780760895888053\n",
      "....R2: -20.819321674296184\n",
      "..Epoch 6\n",
      "....Outputs 0.5403 0.4175 0.3498 0.4175 0.4008\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "....Loss: 4.6446308353101085\n",
      "....R2: -19.594419743915928\n",
      "..Epoch 7\n",
      "....Outputs 0.4866 0.7494 0.5966 0.5966 0.5624\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "....Loss: 4.474223592737906\n",
      "....R2: -18.1109610909653\n",
      "..Epoch 8\n",
      "....Outputs 1.0087 0.7640 0.6550 0.8208 0.8208\n",
      "....Labels  5.3739 2.9694 5.0497 5.7012 5.6991\n",
      "....Loss: 4.262562976718721\n",
      "....R2: -16.345578580264167\n",
      "..Epoch 9\n",
      "....Outputs 1.1004 0.8625 1.3298 1.0138 1.1004\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "....Loss: 4.000683249294005\n",
      "....R2: -14.279722807063035\n",
      "..Epoch 10\n",
      "....Outputs 1.4460 1.4460 1.3233 1.7261 1.1165\n",
      "....Labels  5.6991 5.7012 2.9694 5.3739 5.0497\n",
      "....Loss: 3.6791838931022047\n",
      "....R2: -11.922608164542643\n",
      "..Epoch 11\n",
      "....Outputs 1.7023 1.8692 2.2155 1.8692 1.4256\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 3.2884296161716975\n",
      "....R2: -9.323436240061477\n",
      "..Epoch 12\n",
      "....Outputs 2.3882 2.8105 2.3882 2.1640 1.8034\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "....Loss: 2.8176726352501023\n",
      "....R2: -6.579285171455151\n",
      "..Epoch 13\n",
      "....Outputs 2.2582 2.7181 3.0192 3.5265 3.0192\n",
      "....Labels  5.0497 2.9694 5.7012 5.3739 5.6991\n",
      "....Loss: 2.264633277358701\n",
      "....R2: -3.8960178687320104\n",
      "..Epoch 14\n",
      "....Outputs 4.3795 2.8022 3.3662 3.7759 3.7759\n",
      "....Labels  5.3739 5.0497 2.9694 5.6991 5.7012\n",
      "....Loss: 1.6494163029837876\n",
      "....R2: -1.597214199637254\n",
      "..Epoch 15\n",
      "....Outputs 4.6727 4.0771 3.4411 5.3692 4.6727\n",
      "....Labels  5.7012 2.9694 5.0497 5.3739 5.6991\n",
      "....Loss: 1.0886557608986942\n",
      "....R2: -0.13143178826869573\n",
      "..Epoch 16\n",
      "....Outputs 4.7595 5.6851 6.4372 5.6851 4.1538\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 1.0137416102653687\n",
      "....R2: 0.018926020206208927\n",
      "..Epoch 17\n",
      "....Outputs 6.6780 4.8544 7.3817 5.2752 6.6780\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "....Loss: 1.5032216284894655\n",
      "....R2: -1.1572140147627552\n",
      "..Epoch 18\n",
      "....Outputs 7.3862 5.4967 7.3862 7.9418 5.3938\n",
      "....Labels  5.7012 2.9694 5.6991 5.3739 5.0497\n",
      "....Loss: 1.9383026429379178\n",
      "....R2: -2.586661506547125\n",
      "..Epoch 19\n",
      "....Outputs 7.6840 5.4178 7.6840 8.0510 5.6893\n",
      "....Labels  5.6991 2.9694 5.7012 5.3739 5.0497\n",
      "....Loss: 2.0708368752933457\n",
      "....R2: -3.0939163101924985\n",
      "..Epoch 20\n",
      "....Outputs 5.7587 7.8103 7.6210 7.6210 5.1292\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "....Loss: 1.922636548018467\n",
      "....R2: -2.528918120037567\n",
      "..Epoch 21\n",
      "....Outputs 7.3226 7.3511 7.3226 4.7229 5.6589\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "....Loss: 1.5886828333765388\n",
      "....R2: -1.409470549849393\n",
      "..Epoch 22\n",
      "....Outputs 6.8872 4.2626 5.4514 6.8872 6.7891\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "....Loss: 1.1536622860713375\n",
      "....R2: -0.27058743268346963\n",
      "..Epoch 23\n",
      "....Outputs 5.1924 6.4081 6.4081 3.8045 6.2042\n",
      "....Labels  5.0497 5.6991 5.7012 2.9694 5.3739\n",
      "....Loss: 0.69420922004294\n",
      "....R2: 0.5399257036105634\n",
      "..Epoch 24\n",
      "....Outputs 4.9270 3.3841 5.6515 5.9386 5.9386\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "....Loss: 0.2748860139214505\n",
      "....R2: 0.9278639270869202\n",
      "..Epoch 25\n",
      "....Outputs 4.6885 5.5193 5.1660 5.5193 3.0193\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "....Loss: 0.21981446944876504\n",
      "....R2: 0.9538725177056382\n",
      "..Epoch 26\n",
      "....Outputs 4.7667 5.1738 2.7204 5.1738 4.4982\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "....Loss: 0.507708553532785\n",
      "....R2: 0.7539200954605649\n",
      "..Epoch 27\n",
      "....Outputs 2.4897 4.9118 4.9118 4.3658 4.4588\n",
      "....Labels  2.9694 5.7012 5.6991 5.0497 5.3739\n",
      "....Loss: 0.7453869572521141\n",
      "....R2: 0.4695911732633542\n",
      "..Epoch 28\n",
      "....Outputs 4.7337 4.2404 4.7337 2.3185 4.2931\n",
      "....Labels  5.7012 5.3739 5.6991 2.9694 5.0497\n",
      "....Loss: 0.9109141914434785\n",
      "....R2: 0.2078596824062664\n",
      "..Epoch 29\n",
      "....Outputs 2.1993 4.6357 4.2793 4.6357 4.1057\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "....Loss: 1.0060802032132161\n",
      "....R2: 0.03369909554808459\n",
      "..Epoch 30\n",
      "....Outputs 2.1268 4.6118 4.3216 4.0460 4.6118\n",
      "....Labels  2.9694 5.6991 5.0497 5.3739 5.7012\n",
      "....Loss: 1.0365585329832634\n",
      "....R2: -0.02573415753218633\n",
      "..Epoch 31\n",
      "....Outputs 4.0527 4.6545 4.4154 4.6545 2.0950\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "....Loss: 1.0098521566299308\n",
      "....R2: 0.026439889898899782\n",
      "..Epoch 32\n",
      "....Outputs 4.7562 4.5555 4.7562 2.0996 4.1178\n",
      "....Labels  5.7012 5.0497 5.6991 2.9694 5.3739\n",
      "....Loss: 0.9338846750944092\n",
      "....R2: 0.16740517599035165\n",
      "..Epoch 33\n",
      "....Outputs 4.9086 4.2337 4.7348 2.1358 4.9086\n",
      "....Labels  5.7012 5.3739 5.0497 2.9694 5.6991\n",
      "....Loss: 0.818192785965012\n",
      "....R2: 0.36091516894210385\n",
      "..Epoch 34\n",
      "....Outputs 2.1978 4.9443 5.1026 5.1026 4.3922\n",
      "....Labels  2.9694 5.0497 5.7012 5.6991 5.3739\n",
      "....Loss: 0.675936598369334\n",
      "....R2: 0.5638266441682538\n",
      "..Epoch 35\n",
      "....Outputs 2.2805 5.3268 5.1722 5.3268 4.5841\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "....Loss: 0.5276622799208848\n",
      "....R2: 0.7341973475973769\n",
      "..Epoch 36\n",
      "....Outputs 5.5701 4.7984 2.3781 5.5701 5.4033\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "....Loss: 0.4098121653001753\n",
      "....R2: 0.839669231781845\n",
      "..Epoch 37\n",
      "....Outputs 5.0203 5.8125 5.8125 5.6192 2.4830\n",
      "....Labels  5.3739 5.6991 5.7012 5.0497 2.9694\n",
      "....Loss: 0.3771534718395285\n",
      "....R2: 0.8642051094096417\n",
      "..Epoch 38\n",
      "....Outputs 6.0330 5.2339 6.0330 2.5872 5.7999\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "....Loss: 0.43588264420996164\n",
      "....R2: 0.8186212715101856\n",
      "..Epoch 39\n",
      "....Outputs 6.2105 5.4244 5.9271 6.2105 2.6834\n",
      "....Labels  5.7012 5.3739 5.0497 5.6991 2.9694\n",
      "....Loss: 0.5243915006021532\n",
      "....R2: 0.7374823475598442\n",
      "..Epoch 40\n",
      "....Outputs 5.9880 6.3295 6.3295 2.7627 5.5719\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "....Loss: 0.59235378537624\n",
      "....R2: 0.6650272049880903\n",
      "..Epoch 41\n",
      "....Outputs 2.8192 6.3797 6.3797 5.9781 5.6645\n",
      "....Labels  2.9694 5.7012 5.6991 5.0497 5.3739\n",
      "....Loss: 0.6152077805356975\n",
      "....R2: 0.6386809666472795\n",
      "..Epoch 42\n",
      "....Outputs 2.8514 6.3624 5.7012 6.3624 5.9023\n",
      "....Labels  2.9694 5.6991 5.3739 5.7012 5.0497\n",
      "....Loss: 0.5874100555395686\n",
      "....R2: 0.6705951718826839\n",
      "..Epoch 43\n",
      "....Outputs 6.2856 5.7726 6.2856 2.8608 5.6874\n",
      "....Labels  5.7012 5.0497 5.6991 2.9694 5.3739\n",
      "....Loss: 0.5134430582369054\n",
      "....R2: 0.7483298183706489\n",
      "..Epoch 44\n",
      "....Outputs 6.1669 6.1669 5.6370 5.6054 2.8520\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "....Loss: 0.40679599677618006\n",
      "....R2: 0.842020571239839\n",
      "..Epoch 45\n",
      "....Outputs 2.8314 5.4186 5.5598 6.0214 6.0214\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "....Loss: 0.2814564874086384\n",
      "....R2: 0.9243742429246306\n",
      "..Epoch 46\n",
      "....Outputs 5.8651 5.4695 2.8054 5.2289 5.8651\n",
      "....Labels  5.7012 5.3739 2.9694 5.0497 5.6991\n",
      "....Loss: 0.15655948268624972\n",
      "....R2: 0.97660053329863\n",
      "..Epoch 47\n",
      "....Outputs 2.7805 5.0505 5.7134 5.7134 5.3789\n",
      "....Labels  2.9694 5.0497 5.6991 5.7012 5.3739\n",
      "....Loss: 0.08490778641667263\n",
      "....R2: 0.9931175627963934\n",
      "Verified that a small batch can be overfit since the R2 was greater than 0.99\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "source": [
    "# this test should pass since we are using a good model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buggy model. Can you spot the \"bug\"?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.sigmoid(x) # Spoiler! The \"bug\" is here.  my net used a relu activation function. This model uses a sigmoid activation function.\n",
    "                                    # sigmoid. Activation usually results in back, gradients.\n",
    "   \n",
    "        return x.view(data.num_graphs,) \n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "..Epoch 0\n",
      "....Outputs 0.0960 0.1009 0.0976 0.0959 0.0960\n",
      "....Labels  5.7012 5.3739 5.0497 2.9694 5.6991\n",
      "....Loss: 4.967861455966612\n",
      "....R2: -22.560588584071393\n",
      "..Epoch 1\n",
      "....Outputs 0.4603 0.4596 0.4610 0.4603 0.4656\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "....Loss: 4.612118489684607\n",
      "....R2: -19.307108964786604\n",
      "..Epoch 2\n",
      "....Outputs 0.8247 0.8262 0.8318 0.8262 0.8257\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 4.25635726278205\n",
      "....R2: -16.29511017505854\n",
      "..Epoch 3\n",
      "....Outputs 1.1943 1.2021 1.1937 1.1962 1.1962\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "....Loss: 3.8985214071474976\n",
      "....R2: -13.509319622212006\n",
      "..Epoch 4\n",
      "....Outputs 1.5791 1.5731 1.5694 1.5731 1.5693\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "....Loss: 3.5366167455296513\n",
      "....R2: -10.940519449487216\n",
      "..Epoch 5\n",
      "....Outputs 1.9537 1.9596 1.9658 1.9542 1.9596\n",
      "....Labels  5.0497 5.6991 5.3739 2.9694 5.7012\n",
      "....Loss: 3.169038152289894\n",
      "....R2: -8.58742765551171\n",
      "..Epoch 6\n",
      "....Outputs 2.3578 2.3503 2.3492 2.3641 2.3578\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "....Loss: 2.7955982554755727\n",
      "....R2: -6.460993670045673\n",
      "..Epoch 7\n",
      "....Outputs 2.7686 2.7750 2.7584 2.7686 2.7567\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "....Loss: 2.4184601366102867\n",
      "....R2: -4.5837382086280725\n",
      "..Epoch 8\n",
      "....Outputs 3.1755 3.1914 3.1776 3.1914 3.1977\n",
      "....Labels  5.0497 5.7012 2.9694 5.6991 5.3739\n",
      "....Loss: 2.0434909248047863\n",
      "....R2: -2.9865078644795267\n",
      "..Epoch 9\n",
      "....Outputs 3.6300 3.6034 3.6238 3.6238 3.6051\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "....Loss: 1.6828300000540848\n",
      "....R2: -1.7035083304753607\n",
      "..Epoch 10\n",
      "....Outputs 4.0355 4.0611 4.0358 4.0611 4.0671\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "....Loss: 1.3597970066323193\n",
      "....R2: -0.7652058196471538\n",
      "..Epoch 11\n",
      "....Outputs 4.4607 4.4959 4.5014 4.4959 4.4650\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 1.116029152205711\n",
      "....R2: -0.18904495515346675\n",
      "..Epoch 12\n",
      "....Outputs 4.9171 4.9171 4.8803 4.9219 4.8699\n",
      "....Labels  5.6991 5.7012 5.0497 5.3739 2.9694\n",
      "....Loss: 1.0070957917808177\n",
      "....R2: 0.03174716575486525\n",
      "..Epoch 13\n",
      "....Outputs 5.2668 5.3101 5.3101 5.2488 5.3136\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "....Loss: 1.0536261976106052\n",
      "....R2: -0.05979121134955423\n",
      "..Epoch 14\n",
      "....Outputs 5.6567 5.6069 5.6587 5.5801 5.6567\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "....Loss: 1.2009258235350853\n",
      "....R2: -0.37682768174650505\n",
      "..Epoch 15\n",
      "....Outputs 5.8470 5.9392 5.9392 5.9392 5.8829\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 1.3717555000552257\n",
      "....R2: -0.7963899337821254\n",
      "..Epoch 16\n",
      "....Outputs 6.0376 6.1446 6.1425 6.1446 6.0825\n",
      "....Labels  2.9694 5.6991 5.3739 5.7012 5.0497\n",
      "....Loss: 1.5143528741534005\n",
      "....R2: -1.1892802789384618\n",
      "..Epoch 17\n",
      "....Outputs 6.2690 6.1486 6.2645 6.2690 6.2018\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "....Loss: 1.6046634334256402\n",
      "....R2: -1.4581881440461864\n",
      "..Epoch 18\n",
      "....Outputs 6.3164 6.1843 6.3094 6.2449 6.3164\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "....Loss: 1.6369869768047618\n",
      "....R2: -1.5582187792130267\n",
      "..Epoch 19\n",
      "....Outputs 6.2212 6.2962 6.2868 6.2962 6.1540\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "....Loss: 1.6160647415468383\n",
      "....R2: -1.493243302240589\n",
      "..Epoch 20\n",
      "....Outputs 6.0697 6.1425 6.2085 6.2202 6.2202\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "....Loss: 1.5519915645360292\n",
      "....R2: -1.2994603505412257\n",
      "..Epoch 21\n",
      "....Outputs 6.0872 6.1010 6.0212 6.1010 5.9435\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "....Loss: 1.4573485000940298\n",
      "....R2: -1.0275612883075902\n",
      "..Epoch 22\n",
      "....Outputs 5.9509 5.8694 5.7875 5.9509 5.9351\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "....Loss: 1.3456853192705813\n",
      "....R2: -0.7287581327531085\n",
      "..Epoch 23\n",
      "....Outputs 5.7812 5.7812 5.7634 5.6983 5.6127\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "....Loss: 1.2306599786300476\n",
      "....R2: -0.4458504058979653\n",
      "..Epoch 24\n",
      "....Outputs 5.6018 5.5177 5.4289 5.6018 5.5824\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "....Loss: 1.125260005151305\n",
      "....R2: -0.20879585636899312\n",
      "..Epoch 25\n",
      "....Outputs 5.4216 5.2445 5.3363 5.4216 5.4005\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "....Loss: 1.0406045012416847\n",
      "....R2: -0.03375727065471135\n",
      "..Epoch 26\n",
      "....Outputs 5.2476 5.0666 5.2250 5.2476 5.1611\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "....Loss: 0.9840910365745115\n",
      "....R2: 0.07547692997418587\n",
      "..Epoch 27\n",
      "....Outputs 4.9979 5.0615 4.9005 5.0856 5.0856\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "....Loss: 0.9575495348037005\n",
      "....R2: 0.12467434905064456\n",
      "..Epoch 28\n",
      "....Outputs 4.9399 4.7504 4.8507 4.9399 4.9144\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "....Loss: 0.9569264257205193\n",
      "....R2: 0.12581311267389705\n",
      "..Epoch 29\n",
      "....Outputs 4.7226 4.7867 4.6192 4.8135 4.8135\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "....Loss: 0.9742590632047686\n",
      "....R2: 0.09385840740952633\n",
      "..Epoch 30\n",
      "....Outputs 4.6801 4.6154 4.7083 4.7083 4.5086\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "....Loss: 1.0006583550044674\n",
      "....R2: 0.044086044165209004\n",
      "..Epoch 31\n",
      "....Outputs 4.6253 4.5957 4.4195 4.6253 4.5300\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "....Loss: 1.0285020687544235\n",
      "....R2: -0.00985149772370919\n",
      "..Epoch 32\n",
      "....Outputs 4.3517 4.5644 4.4666 4.5644 4.5336\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "....Loss: 1.05228676418647\n",
      "....R2: -0.057098489534107344\n",
      "..Epoch 33\n",
      "....Outputs 4.3048 4.5254 4.4932 4.5254 4.4246\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 1.068601814392423\n",
      "....R2: -0.09013190311826302\n",
      "..Epoch 34\n",
      "....Outputs 4.5073 4.4030 4.5073 4.2777 4.4736\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "....Loss: 1.0757636041793877\n",
      "....R2: -0.10479297437528201\n",
      "..Epoch 35\n",
      "....Outputs 4.5085 4.4735 4.2691 4.5085 4.4006\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "....Loss: 1.0734174479426015\n",
      "....R2: -0.09997930674526789\n",
      "..Epoch 36\n",
      "....Outputs 4.2771 4.5276 4.5276 4.4911 4.4155\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "....Loss: 1.0622124002202962\n",
      "....R2: -0.07713449807841832\n",
      "..Epoch 37\n",
      "....Outputs 4.4460 4.5245 4.5625 4.2999 4.5625\n",
      "....Labels  5.0497 5.3739 5.7012 2.9694 5.6991\n",
      "....Loss: 1.0435509360529964\n",
      "....R2: -0.03961963787455747\n",
      "..Epoch 38\n",
      "....Outputs 4.6110 4.3354 4.5715 4.4898 4.6110\n",
      "....Labels  5.7012 2.9694 5.3739 5.0497 5.6991\n",
      "....Loss: 1.0193875383252664\n",
      "....R2: 0.00796757593766273\n",
      "..Epoch 39\n",
      "....Outputs 4.6299 4.5448 4.6709 4.6709 4.3813\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "....Loss: 0.9920501313520865\n",
      "....R2: 0.06046177091965277\n",
      "..Epoch 40\n",
      "....Outputs 4.7396 4.7396 4.6971 4.6083 4.4352\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "....Loss: 0.964038281005141\n",
      "....R2: 0.11277094914786778\n",
      "..Epoch 41\n",
      "....Outputs 4.6779 4.4944 4.8144 4.8144 4.7704\n",
      "....Labels  5.0497 2.9694 5.6991 5.7012 5.3739\n",
      "....Loss: 0.9377807832494467\n",
      "....R2: 0.160443678084926\n",
      "..Epoch 42\n",
      "....Outputs 4.5565 4.7508 4.8927 4.8927 4.8472\n",
      "....Labels  2.9694 5.0497 5.7012 5.6991 5.3739\n",
      "....Loss: 0.9153560386609806\n",
      "....R2: 0.20011536988116674\n",
      "..Epoch 43\n",
      "....Outputs 4.9248 4.9718 4.9718 4.6187 4.8244\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "....Loss: 0.8982010239908943\n",
      "....R2: 0.22981637282077083\n",
      "..Epoch 44\n",
      "....Outputs 4.6786 5.0489 5.0489 5.0004 4.8961\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "....Loss: 0.8868918901035489\n",
      "....R2: 0.24908882053806525\n",
      "..Epoch 45\n",
      "....Outputs 5.1215 4.9632 4.7337 5.0715 5.1215\n",
      "....Labels  5.6991 5.0497 2.9694 5.3739 5.7012\n",
      "....Loss: 0.8810746972245804\n",
      "....R2: 0.2589070801770629\n",
      "..Epoch 46\n",
      "....Outputs 4.7819 5.1360 5.0237 5.1874 5.1874\n",
      "....Labels  2.9694 5.3739 5.0497 5.6991 5.7012\n",
      "....Loss: 0.8795891715295255\n",
      "....R2: 0.2614040474785745\n",
      "..Epoch 47\n",
      "....Outputs 5.1917 5.0756 4.8213 5.2446 5.2446\n",
      "....Labels  5.3739 5.0497 2.9694 5.7012 5.6991\n",
      "....Loss: 0.8807525231625454\n",
      "....R2: 0.2594489776370509\n",
      "..Epoch 48\n",
      "....Outputs 5.2916 5.2916 4.8506 5.1173 5.2373\n",
      "....Labels  5.7012 5.6991 2.9694 5.0497 5.3739\n",
      "....Loss: 0.8827194232317561\n",
      "....R2: 0.2561376420767162\n",
      "..Epoch 49\n",
      "....Outputs 5.3275 4.8688 5.1480 5.3275 5.2717\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "....Loss: 0.8838149936307153\n",
      "....R2: 0.25429006028794787\n",
      "..Epoch 50\n",
      "....Outputs 5.3518 5.2946 4.8756 5.3518 5.1671\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "....Loss: 0.8827776944211303\n",
      "....R2: 0.2560394935111028\n",
      "..Epoch 51\n",
      "....Outputs 4.8712 5.3646 5.1749 5.3646 5.3059\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "....Loss: 0.878887532352773\n",
      "....R2: 0.2625818772552768\n",
      "..Epoch 52\n",
      "....Outputs 5.1718 5.3666 5.3666 5.3063 4.8561\n",
      "....Labels  5.0497 5.6991 5.7012 5.3739 2.9694\n",
      "....Loss: 0.8719849069430926\n",
      "....R2: 0.2741195690381112\n",
      "..Epoch 53\n",
      "....Outputs 5.3588 4.8316 5.1590 5.3588 5.2970\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "....Loss: 0.8624044254810468\n",
      "....R2: 0.28998228139351345\n",
      "..Epoch 54\n",
      "....Outputs 5.2792 4.7990 5.1378 5.3426 5.3426\n",
      "....Labels  5.3739 2.9694 5.0497 5.6991 5.7012\n",
      "....Loss: 0.8508488175505379\n",
      "....R2: 0.3088823193917334\n",
      "..Epoch 55\n",
      "....Outputs 5.3199 5.2548 5.3199 5.1101 4.7599\n",
      "....Labels  5.7012 5.3739 5.6991 5.0497 2.9694\n",
      "....Loss: 0.8382256111672292\n",
      "....R2: 0.3292370913358521\n",
      "..Epoch 56\n",
      "....Outputs 5.2256 5.2923 5.2923 5.0775 4.7163\n",
      "....Labels  5.3739 5.6991 5.7012 5.0497 2.9694\n",
      "....Loss: 0.825473968587565\n",
      "....R2: 0.3494900612110987\n",
      "..Epoch 57\n",
      "....Outputs 5.2619 5.1936 4.6698 5.0421 5.2619\n",
      "....Labels  5.7012 5.3739 2.9694 5.0497 5.6991\n",
      "....Loss: 0.8134065485725146\n",
      "....R2: 0.36837032012272564\n",
      "..Epoch 58\n",
      "....Outputs 4.6223 5.0056 5.1604 5.2304 5.2304\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "....Loss: 0.802591649939198\n",
      "....R2: 0.38505476171659425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..Epoch 59\n",
      "....Outputs 5.1997 4.9698 5.1997 4.5753 5.1280\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "....Loss: 0.7932886690368016\n",
      "....R2: 0.39922802445867434\n",
      "..Epoch 60\n",
      "....Outputs 5.1713 5.1713 4.9361 5.0979 4.5303\n",
      "....Labels  5.6991 5.7012 5.0497 5.3739 2.9694\n",
      "....Loss: 0.7854485797891338\n",
      "....R2: 0.41104413611789836\n",
      "..Epoch 61\n",
      "....Outputs 5.1465 5.0714 5.1465 4.9059 4.4883\n",
      "....Labels  5.6991 5.3739 5.7012 5.0497 2.9694\n",
      "....Loss: 0.7787694078111971\n",
      "....R2: 0.42101816903215405\n",
      "..Epoch 62\n",
      "....Outputs 5.0494 4.4504 5.1263 5.1263 4.8802\n",
      "....Labels  5.3739 2.9694 5.7012 5.6991 5.0497\n",
      "....Loss: 0.7727887679303608\n",
      "....R2: 0.4298766989373952\n",
      "..Epoch 63\n",
      "....Outputs 5.1115 4.4171 4.8597 5.0329 5.1115\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "....Loss: 0.7669891738152412\n",
      "....R2: 0.4384018590522113\n",
      "..Epoch 64\n",
      "....Outputs 5.0221 5.1025 5.1025 4.8449 4.3888\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "....Loss: 0.7608916658544421\n",
      "....R2: 0.44729567458000363\n",
      "..Epoch 65\n",
      "....Outputs 4.8359 5.0995 4.3655 5.0995 5.0174\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "....Loss: 0.7541280232499633\n",
      "....R2: 0.45707809758040796\n",
      "..Epoch 66\n",
      "....Outputs 5.1025 4.3472 5.1025 4.8326 5.0185\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "....Loss: 0.7464797135092264\n",
      "....R2: 0.46803481691037807\n",
      "..Epoch 67\n",
      "....Outputs 4.3334 5.1110 4.8348 5.1110 5.0253\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "....Loss: 0.7378888979841614\n",
      "....R2: 0.48020847547989887\n",
      "..Epoch 68\n",
      "....Outputs 4.8418 5.1246 4.3237 5.1246 5.0371\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "....Loss: 0.7284436228661323\n",
      "....R2: 0.4934304414378259\n",
      "..Epoch 69\n",
      "....Outputs 4.8530 5.0533 5.1425 5.1425 4.3173\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "....Loss: 0.7183404253088107\n",
      "....R2: 0.5073848250863189\n",
      "..Epoch 70\n",
      "....Outputs 5.1640 5.1640 4.8676 4.3135 5.0729\n",
      "....Labels  5.7012 5.6991 5.0497 2.9694 5.3739\n",
      "....Loss: 0.7078410616537302\n",
      "....R2: 0.5216797933393302\n",
      "..Epoch 71\n",
      "....Outputs 4.8846 4.3114 5.1880 5.0951 5.1880\n",
      "....Labels  5.0497 2.9694 5.6991 5.3739 5.7012\n",
      "....Loss: 0.6972173679439498\n",
      "....R2: 0.5359298897661191\n",
      "..Epoch 72\n",
      "....Outputs 5.2135 4.9031 5.2135 4.3102 5.1189\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "....Loss: 0.6867072346386907\n",
      "....R2: 0.5498155952269068\n",
      "..Epoch 73\n",
      "....Outputs 5.1432 5.2396 5.2396 4.3090 4.9220\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "....Loss: 0.6764752787692049\n",
      "....R2: 0.563131189897597\n",
      "..Epoch 74\n",
      "....Outputs 5.2654 4.3070 5.2654 4.9405 5.1672\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "....Loss: 0.6665945808856484\n",
      "....R2: 0.5757999003477586\n",
      "..Epoch 75\n",
      "....Outputs 4.3034 5.2899 5.2899 5.1899 4.9578\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "....Loss: 0.6570436356292213\n",
      "....R2: 0.5878686455553515\n",
      "..Epoch 76\n",
      "....Outputs 4.9731 4.2978 5.2106 5.3125 5.3125\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "....Loss: 0.6477245409845549\n",
      "....R2: 0.599476588691821\n",
      "..Epoch 77\n",
      "....Outputs 4.2896 5.3326 4.9858 5.3326 5.2288\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "....Loss: 0.6384901202265685\n",
      "....R2: 0.6108154554124458\n",
      "..Epoch 78\n",
      "....Outputs 5.3496 4.9956 5.2440 5.3496 4.2785\n",
      "....Labels  5.6991 5.0497 5.3739 5.7012 2.9694\n",
      "....Loss: 0.6291808719378738\n",
      "....R2: 0.6220814465511564\n",
      "..Epoch 79\n",
      "....Outputs 5.0021 5.3635 4.2644 5.3635 5.2560\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "....Loss: 0.6196568500058607\n",
      "....R2: 0.6334361087548479\n",
      "..Epoch 80\n",
      "....Outputs 5.3742 5.0054 5.2647 4.2473 5.3742\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "....Loss: 0.6098222314046982\n",
      "....R2: 0.6449792907842654\n",
      "..Epoch 81\n",
      "....Outputs 5.2702 5.0056 5.3817 4.2273 5.3817\n",
      "....Labels  5.3739 5.0497 5.6991 2.9694 5.7012\n",
      "....Loss: 0.5996401403033378\n",
      "....R2: 0.6567357480635058\n",
      "..Epoch 82\n",
      "....Outputs 5.0030 5.3864 5.3864 4.2048 5.2729\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "....Loss: 0.5891298938518007\n",
      "....R2: 0.6686634715577767\n",
      "..Epoch 83\n",
      "....Outputs 5.2731 4.9979 5.3886 4.1800 5.3886\n",
      "....Labels  5.3739 5.0497 5.7012 2.9694 5.6991\n",
      "....Loss: 0.578359453241739\n",
      "....R2: 0.6806677020616605\n",
      "..Epoch 84\n",
      "....Outputs 4.9909 5.3890 4.1536 5.2714 5.3890\n",
      "....Labels  5.0497 5.7012 2.9694 5.3739 5.6991\n",
      "....Loss: 0.5674238863077801\n",
      "....R2: 0.6926293295275361\n",
      "..Epoch 85\n",
      "....Outputs 5.3879 4.1259 5.2683 5.3879 4.9825\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "....Loss: 0.5564250595568528\n",
      "....R2: 0.7044298690737179\n",
      "..Epoch 86\n",
      "....Outputs 5.3861 4.0975 5.2643 4.9734 5.3861\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "....Loss: 0.5454513001526032\n",
      "....R2: 0.7159732883576753\n",
      "..Epoch 87\n",
      "....Outputs 5.3841 5.3841 5.2602 4.0688 4.9641\n",
      "....Labels  5.7012 5.6991 5.3739 2.9694 5.0497\n",
      "....Loss: 0.5345630002732502\n",
      "....R2: 0.7271995961664381\n",
      "..Epoch 88\n",
      "....Outputs 5.3824 5.3824 5.2564 4.9551 4.0404\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "....Loss: 0.5237851482970265\n",
      "....R2: 0.738089119888788\n",
      "..Epoch 89\n",
      "....Outputs 5.2533 5.3815 4.0126 5.3815 4.9469\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "....Loss: 0.513106989201771\n",
      "....R2: 0.748659169103842\n",
      "..Epoch 90\n",
      "....Outputs 5.3818 5.3818 5.2515 4.9399 3.9856\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "....Loss: 0.502494104882197\n",
      "....R2: 0.7589489065920969\n",
      "..Epoch 91\n",
      "....Outputs 5.2512 5.3836 5.3836 4.9345 3.9599\n",
      "....Labels  5.3739 5.6991 5.7012 5.0497 2.9694\n",
      "....Loss: 0.4918949226406616\n",
      "....R2: 0.7690106631388811\n",
      "..Epoch 92\n",
      "....Outputs 4.9308 5.2525 5.3870 5.3870 3.9353\n",
      "....Labels  5.0497 5.3739 5.7012 5.6991 2.9694\n",
      "....Loss: 0.4812584876265695\n",
      "....R2: 0.778892248668048\n",
      "..Epoch 93\n",
      "....Outputs 5.2556 3.9121 5.3922 4.9288 5.3922\n",
      "....Labels  5.3739 2.9694 5.7012 5.0497 5.6991\n",
      "....Loss: 0.47054522386590875\n",
      "....R2: 0.7886267980855207\n",
      "..Epoch 94\n",
      "....Outputs 4.9286 5.3991 3.8902 5.3991 5.2604\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "....Loss: 0.45973389482837734\n",
      "....R2: 0.7982282993179477\n",
      "..Epoch 95\n",
      "....Outputs 3.8695 5.4075 4.9301 5.2669 5.4075\n",
      "....Labels  2.9694 5.6991 5.0497 5.3739 5.7012\n",
      "....Loss: 0.44882667680710897\n",
      "....R2: 0.8076888156074978\n",
      "..Epoch 96\n",
      "....Outputs 3.8498 5.4174 5.4174 4.9330 5.2747\n",
      "....Labels  2.9694 5.7012 5.6991 5.0497 5.3739\n",
      "....Loss: 0.4378452641645603\n",
      "....R2: 0.8169842287789562\n",
      "..Epoch 97\n",
      "....Outputs 3.8310 5.4283 4.9371 5.2837 5.4283\n",
      "....Labels  2.9694 5.7012 5.0497 5.3739 5.6991\n",
      "....Loss: 0.426827483960569\n",
      "....R2: 0.8260790251835115\n",
      "..Epoch 98\n",
      "....Outputs 5.4400 5.2934 3.8127 4.9421 5.4400\n",
      "....Labels  5.7012 5.3739 2.9694 5.0497 5.6991\n",
      "....Loss: 0.41581708800219985\n",
      "....R2: 0.8349361974433245\n",
      "..Epoch 99\n",
      "....Outputs 5.3036 4.9477 5.4522 5.4522 3.7948\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "....Loss: 0.4048569711037656\n",
      "....R2: 0.8435230318246769\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error: Your model was not able to overfit a small batch \n                               of data. The maximum R2 over 100 epochs was not greater than 0.99",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-cb8eea2359e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn,\n\u001b[1;32m      4\u001b[0m                  device, do_test_overfit_small_batch=True, trainer=trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/23_35419_NetDebugger_notebook/netdebugger_notebook/bin/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_overfit_small_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_overfit_small_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/23_35419_NetDebugger_notebook/netdebugger_notebook/bin/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_overfit_small_batch\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             raise ValueError(f'''Error: Your model was not able to overfit a small batch \n\u001b[0m\u001b[1;32m    189\u001b[0m                                of data. The maximum R2 over {k.DL_DBG_OVERFIT_EPOCHS} epochs was not greater than {k.DL_DBG_SUFFICIENT_R2_SMALL_BATCH}''')\n\u001b[1;32m    190\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Verified that a small batch can be overfit since the R2 was greater than {k.DL_DBG_SUFFICIENT_R2_SMALL_BATCH}\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Your model was not able to overfit a small batch \n                               of data. The maximum R2 over 100 epochs was not greater than 0.99"
     ]
    }
   ],
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart Dependencies\n",
    "The `forward` method should not mix information from separate instances.\n",
    "Black circles are four carbon atoms and gold circles are for chlorine atoms and hydrogen's are treated implicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./images/graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./images/graphnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./images/graph_batch2.png)\n",
    "\n",
    "Then we take the sum of all the added features to get a feature Vector, that represents the entire pollen repeat unit. Finally. We pass this polymer feature Vector through a number of hidden layers to get the band Gap out at the end. \n",
    "\n",
    "However, we must be careful when we program the normalization operation as mentioned. The features of the graph are 2D Matrix. So when we pass in a bunch of graphs together to the network, we will actually pass in a 3D Matrix. Therefore. We need to make sure that the mean is computed along this blue Dimension out of the blue might be hard to see, but along this Dimension and we need to make sure that information is not passed along. \n",
    "\n",
    "The red Dimension as that will mix up information of different polymers for instances. This is not desirable because the band gap of one polymer is independent of the band, gap of another polymer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to illustrate the point\n",
    "# graph. Neural networks\n",
    "# Black circles are four carbon atoms and gold circles are for chlorine atoms and hydrogen's are treated implicitly\n",
    "\n",
    "np.random.seed(constants.RANDOM_SEED)\n",
    "polymer_indices = data_df.sample(n=4).index\n",
    "polymer_smiles = data_df.loc[polymer_indices, 'smiles'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we can create a 2d Matrix of features for this entire polymer by counting the types of atoms in polymer\n",
    "#  where each atom contributes a row of features\n",
    "# featureize  the polymers\n",
    "feature_dict = {'C': np.array([1,0,0,0]),\n",
    "    'O': np.array([0,1,0,0]),\n",
    "    'N': np.array([0,0,1,0]),\n",
    "    'Cl': np.array([0,0,0,1])\n",
    "}\n",
    "N_FEATURES_ = len(feature_dict)\n",
    "N_DATA_ = len(polymer_smiles)\n",
    "MAX_N_ATOMS = max([Chem.MolFromSmiles(smile).GetNumAtoms() for smile in polymer_smiles])\n",
    "PROJECTOR_DIM = 100\n",
    "\n",
    "def featurize_smiles_by_atom(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    features = np.zeros((MAX_N_ATOMS, N_FEATURES_))\n",
    "    for ind,atom in enumerate(mol.GetAtoms()):\n",
    "        atom_feature = feature_dict[atom.GetSymbol()]\n",
    "        features[ind, :] = atom_feature\n",
    "\n",
    "    return features\n",
    "\n",
    "labels = data_df.loc[polymer_indices, 'value'].values\n",
    "\n",
    "train_X_ = [Data(x=tensor(featurize_smiles_by_atom(polymer_smiles[ind]), dtype=torch_float),\n",
    "                    y=tensor(labels[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(N_DATA_)\n",
    "]\n",
    "\n",
    "data_set_ = train_X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNet(nn.Module):  # architecture graph net\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(GraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):     #forward method\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)  #  set the dimensions of the features such that the zero with Dimension is the number of polymers. The first Dimension is related to the number of Atoms for polymer and the last Dimension is the number of features\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=2)      #  mean to be taken over the future Dimension.\n",
    "        x = x - x_mean[:, :, None] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_graphnet_class_ls = [lambda : GraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs -0.3991 -0.3810 -0.3220 -0.3801\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: -0.3991157114505768\n",
      "Finished charting dependencies. Data is not getting mixed between instances in the same batch.\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "source": [
    "# this test should pass since we are using a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn,\n",
    "                 device, do_chart_dependencies=True)\n",
    "ds.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a buggy model. Can you spot the bugs?\n",
    "\n",
    "class BuggyGraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyGraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)  #this mixes information between instances because the 2D feature, matrices of different polymers. Stacked along the zero dimension in line\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=0) # Spoiler! this is the bug.  # #  mean to be taken over the zeroth  Dimension.\n",
    "        x = x - x_mean[None, :, :] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_graphnet_class_ls = [lambda : BuggyGraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs -0.1933 -0.1732 -0.1948 -0.1743\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: -0.19329825043678284\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data is getting mixed between instances in the same batch.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-096c66c884c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn,\n\u001b[1;32m      4\u001b[0m                  device, do_chart_dependencies=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbest_model_capacity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/23_35419_NetDebugger_notebook/netdebugger_notebook/bin/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_chart_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_choose_model_size_by_overfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/23_35419_NetDebugger_notebook/netdebugger_notebook/bin/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mchart_dependencies\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_ind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;31m# print(data.x.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data is getting mixed between instances in the same batch.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished charting dependencies. Data is not getting mixed between instances in the same batch.\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data is getting mixed between instances in the same batch."
     ]
    }
   ],
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn,\n",
    "                 device, do_chart_dependencies=True)\n",
    "best_model_capacity = ds.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit training data\n",
    "The capacity of your architecture should be just large enough to overfit the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Beginning model size search\n",
      "\n",
      "..Training model 0 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.612588712779379 [r2] -7.962118969080686\n",
      "......Outputs -0.0134 0.0056 0.0020 0.0056 -0.0062 0.0249 0.0047 0.0048 0.0136 -0.0035\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.612588712779379 [best r2] -7.962118969080686\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.7001437196337448 [r2] -4.767112549579306\n",
      "......Outputs 0.4713 0.6923 0.5647 0.6923 0.6993 0.9451 0.5744 0.6923 0.5705 0.7246\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.7001437196337448 [best r2] -4.767112549579306\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.6620075555319445 [r2] -1.9849688850561997\n",
      "......Outputs 2.4214 3.8174 2.9446 3.8174 3.8828 5.0795 3.0282 3.7635 2.8393 4.1709\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.6620075555319445 [best r2] -1.9849688850561997\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 1.91752414735812 [r2] -0.5488270478833801\n",
      "......Outputs 3.7650 6.7821 4.1415 6.7821 7.0137 7.2677 5.5540 6.0398 4.1668 5.5404\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.91752414735812 [best r2] -0.5488270478833801\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.8156375853221953 [r2] -0.3886077020074197\n",
      "......Outputs 2.4765 4.9482 2.2764 4.9482 5.2034 4.1719 4.3791 3.9470 2.5410 2.7561\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.8156375853221953 [best r2] -0.3886077020074197\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.4945765542987297 [r2] 0.059069446248030344\n",
      "......Outputs 2.7501 6.0014 2.1245 6.0014 6.3554 4.0252 5.6162 4.4012 2.6117 2.4081\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.4945765542987297 [best r2] 0.059069446248030344\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.3907806853147182 [r2] 0.1852234102414716\n",
      "......Outputs 3.4749 7.5528 2.3780 7.5528 8.0138 4.4849 7.4588 5.5622 3.0809 2.6881\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.3907806853147182 [best r2] 0.1852234102414716\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.227739365574489 [r2] 0.36505866179471946\n",
      "......Outputs 3.2639 6.2107 2.0893 6.2107 6.5330 3.6048 6.7607 4.9667 2.6599 2.4311\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.227739365574489 [best r2] 0.36505866179471946\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.0642390812252958 [r2] 0.5229107321872967\n",
      "......Outputs 3.7273 5.8962 2.3307 5.8962 6.1911 3.5589 6.9827 5.3086 2.7576 2.8538\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0642390812252958 [best r2] 0.5229107321872967\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 0.9481586457530299 [r2] 0.6213105031737572\n",
      "......Outputs 4.5703 6.0944 2.8360 6.0944 6.3746 3.7868 7.6583 6.0006 3.0478 3.5160\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.9481586457530299 [best r2] 0.6213105031737572\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8315051390266102 [r2] 0.7087599370689766\n",
      "......Outputs 4.5917 5.4849 2.7860 5.4849 5.6544 3.3298 7.2083 5.3938 2.7189 3.3043\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8315051390266102 [best r2] 0.7087599370689766\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7367726080813999 [r2] 0.7713410613996057\n",
      "......Outputs 5.1017 5.6753 3.0626 5.6753 5.7732 3.2934 7.4988 5.4550 2.6958 3.3328\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7367726080813999 [best r2] 0.7713410613996057\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.6538761409787291 [r2] 0.8199006179912371\n",
      "......Outputs 5.2737 5.6375 3.1845 5.6375 5.6257 3.1459 7.3657 5.2477 2.5629 3.2022\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6538761409787291 [best r2] 0.8199006179912371\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.5871498093489826 [r2] 0.8547824553911068\n",
      "......Outputs 5.3274 5.5936 3.2803 5.5936 5.4837 3.0226 7.1419 5.0955 2.4621 3.1141\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5871498093489826 [best r2] 0.8547824553911068\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5279318331097949 [r2] 0.8825976093650496\n",
      "......Outputs 5.3966 5.7363 3.3657 5.7363 5.5975 2.9844 7.1068 5.1173 2.4323 3.1520\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5279318331097949 [best r2] 0.8825976093650496\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.47647403465031296 [r2] 0.9043687750915036\n",
      "......Outputs 5.2189 5.6615 3.2545 5.6615 5.6213 2.8749 6.9054 4.9599 2.3328 3.1335\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.47647403465031296 [best r2] 0.9043687750915036\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.432186105036164 [r2] 0.9213202736215496\n",
      "......Outputs 5.1971 5.6999 3.2333 5.6999 5.7828 2.8567 6.8549 5.0332 2.3050 3.2692\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.432186105036164 [best r2] 0.9213202736215496\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.39207352539619145 [r2] 0.9352475393938005\n",
      "......Outputs 5.0712 5.5828 3.1294 5.5828 5.7330 2.7908 6.6824 4.9830 2.2366 3.3043\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.39207352539619145 [best r2] 0.9352475393938005\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.3571251786611685 [r2] 0.9462767617940456\n",
      "......Outputs 5.0580 5.6479 3.0960 5.6479 5.7714 2.7681 6.6646 5.0495 2.2090 3.3406\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3571251786611685 [best r2] 0.9462767617940456\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.32585458422299923 [r2] 0.9552730883903486\n",
      "......Outputs 5.0063 5.6292 3.0295 5.6292 5.6864 2.7233 6.5954 5.0272 2.1666 3.3037\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.32585458422299923 [best r2] 0.9552730883903486\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.2989420691460821 [r2] 0.9623560391342536\n",
      "......Outputs 5.0395 5.6649 3.0153 5.6649 5.6638 2.7099 6.6005 5.0650 2.1575 3.3015\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2989420691460821 [best r2] 0.9623560391342536\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.2751484899346243 [r2] 0.9681099273808559\n",
      "......Outputs 5.0418 5.6745 2.9841 5.6745 5.6132 2.6881 6.5873 5.0519 2.1465 3.2876\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2751484899346243 [best r2] 0.9681099273808559\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.25482410968296715 [r2] 0.9726471696461525\n",
      "......Outputs 5.0499 5.6929 2.9560 5.6929 5.5842 2.6807 6.6034 5.0769 2.1375 3.2838\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.25482410968296715 [best r2] 0.9726471696461525\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.23721339648304798 [r2] 0.9762971993096669\n",
      "......Outputs 5.0496 5.7084 2.9233 5.7084 5.5523 2.6802 6.6280 5.1009 2.1245 3.2892\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.23721339648304798 [best r2] 0.9762971993096669\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.22176108116665333 [r2] 0.9792846678374488\n",
      "......Outputs 5.0426 5.6980 2.9022 5.6980 5.5268 2.6878 6.6374 5.0944 2.1111 3.2863\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.22176108116665333 [best r2] 0.9792846678374488\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.20858023847648682 [r2] 0.9816740051490696\n",
      "......Outputs 5.0407 5.7083 2.8968 5.7083 5.5121 2.7071 6.6590 5.1102 2.1079 3.2825\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.20858023847648682 [best r2] 0.9816740051490696\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.19670748041979452 [r2] 0.9837009237237416\n",
      "......Outputs 5.0298 5.7089 2.8853 5.7089 5.4857 2.7273 6.6730 5.1276 2.0985 3.2753\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.19670748041979452 [best r2] 0.9837009237237416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.18624584283360338 [r2] 0.9853885129906151\n",
      "......Outputs 5.0278 5.7117 2.8758 5.7117 5.4681 2.7502 6.6846 5.1434 2.0905 3.2711\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.18624584283360338 [best r2] 0.9853885129906151\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.1770083492139858 [r2] 0.9868019811415891\n",
      "......Outputs 5.0403 5.7204 2.8784 5.7204 5.4566 2.7812 6.6953 5.1520 2.0945 3.2734\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1770083492139858 [best r2] 0.9868019811415891\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.16851980051630988 [r2] 0.9880374679705571\n",
      "......Outputs 5.0411 5.7291 2.8729 5.7291 5.4359 2.8053 6.7010 5.1464 2.0989 3.2685\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16851980051630988 [best r2] 0.9880374679705571\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.16083067376418106 [r2] 0.9891042029938654\n",
      "......Outputs 5.0438 5.7275 2.8707 5.7275 5.4296 2.8220 6.7038 5.1489 2.1007 3.2628\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16083067376418106 [best r2] 0.9891042029938654\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.15364384119535818 [r2] 0.9900562189427362\n",
      "......Outputs 5.0445 5.7286 2.8759 5.7286 5.4238 2.8340 6.7036 5.1544 2.1038 3.2589\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15364384119535818 [best r2] 0.9900562189427362\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.14711937823174936 [r2] 0.9908828100639241\n",
      "......Outputs 5.0412 5.7319 2.8865 5.7319 5.4216 2.8408 6.7072 5.1571 2.1106 3.2539\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14711937823174936 [best r2] 0.9908828100639241\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.14089560196967932 [r2] 0.9916378854085628\n",
      "......Outputs 5.0413 5.7327 2.8905 5.7327 5.4159 2.8431 6.7100 5.1568 2.1145 3.2415\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14089560196967932 [best r2] 0.9916378854085628\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.13503639841919152 [r2] 0.9923189085914084\n",
      "......Outputs 5.0410 5.7291 2.8917 5.7291 5.4047 2.8444 6.7040 5.1510 2.1131 3.2362\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13503639841919152 [best r2] 0.9923189085914084\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.1295103122592373 [r2] 0.9929347108235376\n",
      "......Outputs 5.0391 5.7418 2.8989 5.7418 5.4014 2.8476 6.7043 5.1484 2.1128 3.2300\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1295103122592373 [best r2] 0.9929347108235376\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.12439509251702684 [r2] 0.9934817991921944\n",
      "......Outputs 5.0484 5.7439 2.9108 5.7439 5.4020 2.8704 6.7111 5.1674 2.1190 3.2245\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12439509251702684 [best r2] 0.9934817991921944\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.1194186869194012 [r2] 0.9939928866900736\n",
      "......Outputs 5.0497 5.7285 2.9169 5.7285 5.3948 2.8727 6.7082 5.1651 2.1228 3.2175\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1194186869194012 [best r2] 0.9939928866900736\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.11466526913473842 [r2] 0.9944615909234188\n",
      "......Outputs 5.0329 5.7238 2.9141 5.7238 5.3833 2.8629 6.6970 5.1466 2.1193 3.2064\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11466526913473842 [best r2] 0.9944615909234188\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.11023505883443682 [r2] 0.9948812877371516\n",
      "......Outputs 5.0321 5.7409 2.9146 5.7409 5.3755 2.8619 6.7012 5.1537 2.1164 3.2001\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11023505883443682 [best r2] 0.9948812877371516\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.10672811685416173 [r2] 0.9952017935058955\n",
      "......Outputs 5.0632 5.7614 2.9375 5.7614 5.4000 2.8736 6.7277 5.1882 2.1286 3.2066\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10672811685416173 [best r2] 0.9952017935058955\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.10430783723015198 [r2] 0.9954169444407861\n",
      "......Outputs 5.0720 5.7342 2.9556 5.7342 5.4162 2.8773 6.7202 5.1772 2.1383 3.1996\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10430783723015198 [best r2] 0.9954169444407861\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.1027024069942483 [r2] 0.995556936859341\n",
      "......Outputs 5.0137 5.6833 2.9221 5.6833 5.3541 2.8472 6.6741 5.1061 2.1092 3.1765\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1027024069942483 [best r2] 0.995556936859341\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.10654367827093562 [r2] 0.9952183628929433\n",
      "......Outputs 4.9811 5.7070 2.8899 5.7070 5.3159 2.8367 6.6674 5.0990 2.0824 3.1627\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1027024069942483 [best r2] 0.995556936859341\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.11236535804096026 [r2] 0.9946815372625741\n",
      "......Outputs 5.0874 5.8440 2.9648 5.8440 5.4049 2.8942 6.7731 5.2367 2.1323 3.1943\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1027024069942483 [best r2] 0.995556936859341\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.13994088866533405 [r2] 0.9917508254006485\n",
      "......Outputs 5.1836 5.8227 3.0568 5.8227 5.5254 2.9296 6.8087 5.2759 2.1980 3.2225\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1027024069942483 [best r2] 0.995556936859341\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.15000810761985942 [r2] 0.9905212579303283\n",
      "......Outputs 4.9558 5.5520 2.9230 5.5520 5.3346 2.8127 6.5904 5.0121 2.1120 3.1486\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1027024069942483 [best r2] 0.995556936859341\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.1983353208968358 [r2] 0.9834300435393762\n",
      "......Outputs 4.7703 5.5492 2.7741 5.5492 5.1502 2.7427 6.5230 4.9264 1.9871 3.0992\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1027024069942483 [best r2] 0.995556936859341\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.19963996712783796 [r2] 0.9832113327990932\n",
      "......Outputs 5.1136 5.9903 2.9929 5.9903 5.4579 2.9544 6.9295 5.3874 2.1107 3.2348\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1027024069942483 [best r2] 0.995556936859341\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.218231266029515 [r2] 0.9799388795646874\n",
      "......Outputs 5.3365 5.9336 3.1498 5.9336 5.6669 3.0121 6.9459 5.4232 2.2294 3.2456\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1027024069942483 [best r2] 0.995556936859341\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.19370394179678474 [r2] 0.9841948668852535\n",
      "......Outputs 4.8775 5.4194 2.8780 5.4194 5.2490 2.7668 6.4040 4.8683 2.0883 3.0504\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1027024069942483 [best r2] 0.995556936859341\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.16660002431971502 [r2] 0.9883084696384435\n",
      "......Outputs 4.8559 5.6967 2.8528 5.6967 5.2552 2.7964 6.6073 5.0000 2.0644 3.1505\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1027024069942483 [best r2] 0.995556936859341\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.15422305260576596 [r2] 0.989981104864936\n",
      "......Outputs 5.1985 6.0242 3.0578 6.0242 5.5381 3.0143 7.0231 5.4169 2.1801 3.3195\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1027024069942483 [best r2] 0.995556936859341\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.11053185194833161 [r2] 0.9948536877397115\n",
      "......Outputs 5.1017 5.5772 2.9983 5.5772 5.3788 2.8773 6.6364 5.1678 2.1226 3.1433\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1027024069942483 [best r2] 0.995556936859341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.10203932901514434 [r2] 0.9956141231895166\n",
      "......Outputs 4.9797 5.6025 2.9214 5.6025 5.3354 2.7861 6.5456 5.0430 2.0795 3.0744\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10203932901514434 [best r2] 0.9956141231895166\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.0878155713576997 [r2] 0.9967516388813022\n",
      "......Outputs 5.0818 5.9089 3.0058 5.9089 5.4507 2.8753 6.8126 5.2135 2.1481 3.2101\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0878155713576997 [best r2] 0.9967516388813022\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.07529761349123075 [r2] 0.9976117286745693\n",
      "......Outputs 5.0626 5.7334 2.9925 5.7334 5.3914 2.8787 6.7178 5.1694 2.1361 3.2003\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07529761349123075 [best r2] 0.9976117286745693\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.07170390378267145 [r2] 0.9978342573849696\n",
      "......Outputs 5.0040 5.6232 2.9337 5.6232 5.3536 2.8290 6.6085 5.1066 2.0914 3.1136\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07170390378267145 [best r2] 0.9978342573849696\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.06489828614144712 [r2] 0.9982258607799013\n",
      "......Outputs 5.0578 5.7793 2.9758 5.7793 5.3956 2.8584 6.7271 5.1932 2.1195 3.1481\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06489828614144712 [best r2] 0.9982258607799013\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.06333400201490294 [r2] 0.9983103564143065\n",
      "......Outputs 5.0751 5.7914 2.9904 5.7914 5.4022 2.8750 6.7505 5.1854 2.1339 3.1732\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06333400201490294 [best r2] 0.9983103564143065\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.059930710812402165 [r2] 0.9984870655978164\n",
      "......Outputs 5.0316 5.6896 2.9573 5.6896 5.3771 2.8518 6.6669 5.1204 2.1117 3.1432\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.059930710812402165 [best r2] 0.9984870655978164\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.057345849898277984 [r2] 0.9986147593479802\n",
      "......Outputs 5.0320 5.7161 2.9590 5.7161 5.3744 2.8549 6.6863 5.1491 2.1120 3.1369\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.057345849898277984 [best r2] 0.9986147593479802\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.05714660014767649 [r2] 0.9986243687392335\n",
      "......Outputs 5.0637 5.7768 2.9787 5.7768 5.3948 2.8743 6.7406 5.1863 2.1274 3.1472\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05714660014767649 [best r2] 0.9986243687392335\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.055175321548666924 [r2] 0.9987176369774045\n",
      "......Outputs 5.0583 5.7355 2.9787 5.7355 5.3877 2.8701 6.7048 5.1582 2.1258 3.1432\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.055175321548666924 [best r2] 0.9987176369774045\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.0538553791972432 [r2] 0.9987782582442631\n",
      "......Outputs 5.0305 5.7057 2.9613 5.7057 5.3668 2.8544 6.6728 5.1345 2.1136 3.1299\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0538553791972432 [best r2] 0.9987782582442631\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.052475780236887856 [r2] 0.9988400505852613\n",
      "......Outputs 5.0418 5.7482 2.9641 5.7482 5.3770 2.8622 6.7108 5.1612 2.1166 3.1355\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.052475780236887856 [best r2] 0.9988400505852613\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.05328365594765423 [r2] 0.9988040603306737\n",
      "......Outputs 5.0696 5.7604 2.9833 5.7604 5.3965 2.8771 6.7294 5.1809 2.1280 3.1446\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.052475780236887856 [best r2] 0.9988400505852613\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.051472909316178664 [r2] 0.9988839627956202\n",
      "......Outputs 5.0522 5.7206 2.9758 5.7206 5.3815 2.8654 6.6913 5.1525 2.1224 3.1297\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.051472909316178664 [best r2] 0.9988839627956202\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.05092143513188172 [r2] 0.9989077488494698\n",
      "......Outputs 5.0242 5.7156 2.9578 5.7156 5.3575 2.8521 6.6773 5.1333 2.1112 3.1210\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05092143513188172 [best r2] 0.9989077488494698\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.05041559512505494 [r2] 0.9989293413325728\n",
      "......Outputs 5.0468 5.7544 2.9685 5.7544 5.3748 2.8667 6.7169 5.1645 2.1179 3.1351\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.052538151174017896 [r2] 0.9988372915933464\n",
      "......Outputs 5.0775 5.7588 2.9883 5.7588 5.4028 2.8803 6.7326 5.1869 2.1306 3.1394\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.05064824201165814 [r2] 0.9989194372496216\n",
      "......Outputs 5.0484 5.7158 2.9754 5.7158 5.3785 2.8630 6.6867 5.1492 2.1226 3.1212\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.05204593826180223 [r2] 0.9988589756165229\n",
      "......Outputs 5.0102 5.7074 2.9504 5.7074 5.3415 2.8468 6.6656 5.1196 2.1056 3.1143\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.052040245913121466 [r2] 0.9988592251942487\n",
      "......Outputs 5.0478 5.7596 2.9657 5.7596 5.3700 2.8682 6.7222 5.1687 2.1145 3.1331\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.05907862324436255 [r2] 0.9985297811962296\n",
      "......Outputs 5.1024 5.7785 3.0009 5.7785 5.4237 2.8916 6.7564 5.2121 2.1389 3.1405\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.05717359850788028 [r2] 0.9986230686247436\n",
      "......Outputs 5.0528 5.7108 2.9827 5.7108 5.3863 2.8651 6.6812 5.1457 2.1288 3.1164\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.06611531262974445 [r2] 0.9981586966105779\n",
      "......Outputs 4.9700 5.6724 2.9287 5.6724 5.3063 2.8302 6.6239 5.0779 2.0913 3.0997\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.0673193917504925 [r2] 0.9980910189926512\n",
      "......Outputs 5.0299 5.7614 2.9467 5.7614 5.3448 2.8621 6.7207 5.1654 2.1000 3.1259\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.09062018059301642 [r2] 0.9965408365177072\n",
      "......Outputs 5.1658 5.8403 3.0316 5.8403 5.4755 2.9205 6.8270 5.2815 2.1595 3.1550\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.0934640050580765 [r2] 0.9963203203138475\n",
      "......Outputs 5.0937 5.7178 3.0198 5.7178 5.4364 2.8840 6.6962 5.1664 2.1582 3.1241\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.12330774355744807 [r2] 0.9935952535451198\n",
      "......Outputs 4.8721 5.5714 2.8835 5.5714 5.2313 2.7867 6.5100 4.9694 2.0634 3.0697\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.13786100175705793 [r2] 0.9919942117176782\n",
      "......Outputs 4.9311 5.7153 2.8726 5.7153 5.2398 2.8219 6.6626 5.1022 2.0454 3.1014\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.18659165369792868 [r2] 0.985334203033805\n",
      "......Outputs 5.2940 6.0038 3.0816 6.0038 5.5806 2.9867 7.0116 5.4437 2.1917 3.1974\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.21130637087627818 [r2] 0.9811918351220749\n",
      "......Outputs 5.2646 5.7970 3.1453 5.7970 5.6306 2.9626 6.8148 5.2921 2.2496 3.1620\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.2519654451628788 [r2] 0.9732574256877856\n",
      "......Outputs 4.7047 5.3680 2.8193 5.3680 5.1379 2.7078 6.2710 4.7660 2.0408 3.0035\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.28310877106718874 [r2] 0.9662380214198824\n",
      "......Outputs 4.6767 5.5617 2.7394 5.5617 5.0479 2.7347 6.4685 4.8886 1.9615 3.0623\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.3097276090379314 [r2] 0.9595907229360734\n",
      "......Outputs 5.3506 6.2392 3.1150 6.2392 5.6983 3.0972 7.2920 5.5888 2.1994 3.2962\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.29037699247782844 [r2] 0.9644822336045717\n",
      "......Outputs 5.3931 5.8392 3.2162 5.8392 5.7270 3.0394 6.9021 5.4177 2.2900 3.1508\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.28327696099291444 [r2] 0.9661978947125798\n",
      "......Outputs 4.7071 5.2492 2.8207 5.2492 5.0925 2.6349 6.1040 4.7445 2.0314 2.8979\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.22296898050638742 [r2] 0.9790583866383518\n",
      "......Outputs 4.7998 5.7586 2.8409 5.7586 5.2229 2.7642 6.5833 5.0120 2.0385 3.1418\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.21295391331726865 [r2] 0.9808973995822418\n",
      "......Outputs 5.2716 6.1576 3.0916 6.1576 5.6503 3.0680 7.2014 5.4892 2.1997 3.3467\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.13901818204852992 [r2] 0.9918592493852098\n",
      "......Outputs 5.1142 5.5122 3.0124 5.5122 5.3787 2.8869 6.6106 5.1713 2.1240 3.0648\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.12972118341221603 [r2] 0.9929116844164111\n",
      "......Outputs 4.9376 5.4979 2.9002 5.4979 5.2721 2.7358 6.4284 5.0014 2.0520 2.9699\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.09682125770192304 [r2] 0.9960512223450304\n",
      "......Outputs 5.0927 5.9734 2.9963 5.9734 5.4752 2.8691 6.8508 5.2321 2.1389 3.1847\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.07960383515420219 [r2] 0.997330750166876\n",
      "......Outputs 5.0912 5.8084 3.0092 5.8084 5.4196 2.9047 6.7938 5.2080 2.1527 3.2169\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.06863417196777682 [r2] 0.9980157241902503\n",
      "......Outputs 4.9867 5.5665 2.9278 5.5665 5.3365 2.8264 6.5721 5.0905 2.0899 3.0753\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.056755311327719105 [r2] 0.9986431424314033\n",
      "......Outputs 5.0434 5.7345 2.9539 5.7345 5.3994 2.8491 6.6915 5.1806 2.1050 3.0916\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.05668621660130188 [r2] 0.998646444135965\n",
      "......Outputs 5.0894 5.8249 2.9913 5.8249 5.4055 2.8845 6.7755 5.2108 2.1346 3.1530\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05041559512505494 [best r2] 0.9989293413325728\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.048391255624605944 [r2] 0.9990135955497217\n",
      "......Outputs 5.0374 5.7105 2.9577 5.7105 5.3687 2.8573 6.6790 5.1234 2.1143 3.1277\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.048391255624605944 [best r2] 0.9990135955497217\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.048184452159653904 [r2] 0.9990220084734491\n",
      "......Outputs 5.0278 5.6963 2.9508 5.6963 5.3825 2.8545 6.6686 5.1328 2.1074 3.1144\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.048184452159653904 [best r2] 0.9990220084734491\n",
      "..Plotting of gradients skipped\n",
      "\n",
      "..Training model 1 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.739300727750808 [r2] -8.461277389672464\n",
      "......Outputs -0.1192 -0.1244 -0.1256 -0.1244 -0.1316 -0.1254 -0.1191 -0.1210 -0.1191 -0.1324\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.739300727750808 [best r2] -8.461277389672464\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.8495211112355605 [r2] -5.242156491373782\n",
      "......Outputs 0.3381 0.5200 0.4152 0.5200 0.5181 0.7335 0.4214 0.5464 0.3716 0.6045\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.8495211112355605 [best r2] -5.242156491373782\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.7279033297803066 [r2] -2.1345788050865213\n",
      "......Outputs 2.2511 3.4515 2.7844 3.4515 3.5443 4.6990 2.7105 3.5559 2.4724 3.9315\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.7279033297803066 [best r2] -2.1345788050865213\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 2.016563403815883 [r2] -0.7129512518813226\n",
      "......Outputs 3.9259 6.7906 4.4715 6.7906 7.1059 7.5785 5.4600 6.3103 4.1583 5.9270\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.016563403815883 [best r2] -0.7129512518813226\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.8646104677049986 [r2] -0.4645273172444564\n",
      "......Outputs 2.5498 4.9539 2.4251 4.9539 5.2783 4.3128 4.2874 4.0839 2.5131 2.9817\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.8646104677049986 [best r2] -0.4645273172444564\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.5505711181549495 [r2] -0.012755525765442366\n",
      "......Outputs 2.6789 5.7434 2.1786 5.7434 6.2255 3.9553 5.2413 4.3046 2.4663 2.4339\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.5505711181549495 [best r2] -0.012755525765442366\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.455480280685176 [r2] 0.10765274385567192\n",
      "......Outputs 3.4288 7.4762 2.5249 7.4762 8.0779 4.4478 7.1392 5.5325 2.9731 2.6679\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.455480280685176 [best r2] 0.10765274385567192\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.2927082380873909 [r2] 0.29608167091105797\n",
      "......Outputs 3.2584 6.3617 2.2258 6.3617 6.8276 3.6440 6.6624 5.0467 2.6018 2.3962\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.2927082380873909 [best r2] 0.29608167091105797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.1358440886156123 [r2] 0.4565511361563186\n",
      "......Outputs 3.5278 5.8883 2.3460 5.8883 6.2576 3.4048 6.7548 5.1609 2.5861 2.5665\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.1358440886156123 [best r2] 0.4565511361563186\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 1.0209814300624607 [r2] 0.5609065820719996\n",
      "......Outputs 4.3393 6.2014 2.8961 6.2014 6.5363 3.7308 7.6351 5.9285 2.9399 3.1642\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0209814300624607 [best r2] 0.5609065820719996\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8998264030704474 [r2] 0.6589337820374406\n",
      "......Outputs 4.4791 5.5616 2.9345 5.5616 5.8141 3.3497 7.2552 5.5819 2.7109 3.1355\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8998264030704474 [best r2] 0.6589337820374406\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7903523235296026 [r2] 0.7368746386053098\n",
      "......Outputs 4.8839 5.5298 3.1533 5.5298 5.6945 3.2393 7.3631 5.5485 2.6407 3.1681\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7903523235296026 [best r2] 0.7368746386053098\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.7030753580330723 [r2] 0.7917787764003968\n",
      "......Outputs 5.1990 5.6447 3.3325 5.6447 5.6989 3.1456 7.4243 5.4796 2.5416 3.1302\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7030753580330723 [best r2] 0.7917787764003968\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.6265303390862801 [r2] 0.8346495226256319\n",
      "......Outputs 5.1959 5.5301 3.3261 5.5301 5.4582 2.9425 7.0530 5.2082 2.3578 2.9624\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6265303390862801 [best r2] 0.8346495226256319\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.564038765296215 [r2] 0.8659894013290912\n",
      "......Outputs 5.3683 5.7935 3.4793 5.7935 5.6071 2.9273 7.0506 5.2428 2.3138 2.9893\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.564038765296215 [best r2] 0.8659894013290912\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.5057649030771663 [r2] 0.892249667663433\n",
      "......Outputs 5.2028 5.6670 3.3506 5.6670 5.5590 2.8086 6.7415 5.0925 2.1928 2.9241\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5057649030771663 [best r2] 0.892249667663433\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.4562684340551844 [r2] 0.912307573171313\n",
      "......Outputs 5.1627 5.7674 3.2945 5.7674 5.7602 2.8146 6.7019 5.1174 2.1464 2.9958\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4562684340551844 [best r2] 0.912307573171313\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.41185930653443836 [r2] 0.928547240032234\n",
      "......Outputs 5.0164 5.6175 3.1715 5.6175 5.6940 2.7651 6.5059 5.0688 2.0711 3.0396\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.41185930653443836 [best r2] 0.928547240032234\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.37323421417300884 [r2] 0.9413208062879056\n",
      "......Outputs 5.0116 5.6868 3.1172 5.6868 5.7169 2.7644 6.5139 5.1353 2.0322 3.1149\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.37323421417300884 [best r2] 0.9413208062879056\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.33876555080792614 [r2] 0.9516585445857055\n",
      "......Outputs 4.9747 5.6514 3.0246 5.6514 5.6165 2.7103 6.4770 5.1350 1.9729 3.1068\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.33876555080792614 [best r2] 0.9516585445857055\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.30877222005168303 [r2] 0.959839632196979\n",
      "......Outputs 4.9972 5.7022 2.9973 5.7022 5.5853 2.7039 6.5174 5.1911 1.9418 3.1109\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.30877222005168303 [best r2] 0.959839632196979\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.282649091672549 [r2] 0.9663475700310288\n",
      "......Outputs 4.9995 5.7116 2.9649 5.7116 5.5469 2.6976 6.5284 5.2058 1.9142 3.0927\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.282649091672549 [best r2] 0.9663475700310288\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.26032634495646667 [r2] 0.9714531967036563\n",
      "......Outputs 5.0074 5.7115 2.9365 5.7115 5.5097 2.7012 6.5601 5.2136 1.8919 3.0831\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.26032634495646667 [best r2] 0.9714531967036563\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.24114340474073429 [r2] 0.9755053060309119\n",
      "......Outputs 5.0255 5.7156 2.9229 5.7156 5.4893 2.7235 6.6060 5.2282 1.8820 3.1011\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.24114340474073429 [best r2] 0.9755053060309119\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.2242715431760018 [r2] 0.9788129942905948\n",
      "......Outputs 5.0245 5.7019 2.9031 5.7019 5.4599 2.7412 6.6273 5.2196 1.8761 3.1189\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2242715431760018 [best r2] 0.9788129942905948\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.20972481511837443 [r2] 0.9814723268216302\n",
      "......Outputs 5.0174 5.7166 2.8857 5.7166 5.4423 2.7593 6.6504 5.2077 1.8753 3.1269\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.20972481511837443 [best r2] 0.9814723268216302\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.19689748379463387 [r2] 0.9836694213613598\n",
      "......Outputs 5.0195 5.7294 2.8767 5.7294 5.4286 2.7842 6.6796 5.2082 1.8804 3.1334\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.19689748379463387 [best r2] 0.9836694213613598\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.18554681603753634 [r2] 0.9854979882228613\n",
      "......Outputs 5.0215 5.7178 2.8725 5.7178 5.4143 2.8068 6.6842 5.1982 1.8858 3.1352\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.18554681603753634 [best r2] 0.9854979882228613\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.1753479574785318 [r2] 0.9870484226470103\n",
      "......Outputs 5.0233 5.7279 2.8736 5.7279 5.4057 2.8294 6.6944 5.1823 1.8930 3.1352\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1753479574785318 [best r2] 0.9870484226470103\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.16619642899215004 [r2] 0.9883650474389353\n",
      "......Outputs 5.0307 5.7438 2.8778 5.7438 5.4023 2.8476 6.7125 5.1729 1.9034 3.1380\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16619642899215004 [best r2] 0.9883650474389353\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.15776155206793957 [r2] 0.9895160828233885\n",
      "......Outputs 5.0302 5.7332 2.8789 5.7332 5.3940 2.8585 6.7108 5.1605 1.9136 3.1375\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15776155206793957 [best r2] 0.9895160828233885\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.15000585908537364 [r2] 0.9905215420898851\n",
      "......Outputs 5.0237 5.7245 2.8786 5.7245 5.3811 2.8637 6.7031 5.1477 1.9235 3.1361\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15000585908537364 [best r2] 0.9905215420898851\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.142792821279603 [r2] 0.9914111703377836\n",
      "......Outputs 5.0281 5.7286 2.8820 5.7286 5.3725 2.8672 6.7060 5.1408 1.9331 3.1353\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.142792821279603 [best r2] 0.9914111703377836\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.13605538567389133 [r2] 0.9922025478851462\n",
      "......Outputs 5.0348 5.7455 2.8858 5.7455 5.3736 2.8758 6.7119 5.1430 1.9465 3.1354\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13605538567389133 [best r2] 0.9922025478851462\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.12996383141411572 [r2] 0.9928851417316195\n",
      "......Outputs 5.0435 5.7487 2.8934 5.7487 5.3845 2.8840 6.7167 5.1492 1.9622 3.1385\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12996383141411572 [best r2] 0.9928851417316195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.12425842724798561 [r2] 0.9934961136206648\n",
      "......Outputs 5.0425 5.7292 2.8959 5.7292 5.3878 2.8809 6.7036 5.1413 1.9731 3.1354\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12425842724798561 [best r2] 0.9934961136206648\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.11925680312291644 [r2] 0.9940091621190668\n",
      "......Outputs 5.0153 5.7113 2.8837 5.7113 5.3555 2.8689 6.6773 5.1159 1.9778 3.1235\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11925680312291644 [best r2] 0.9940091621190668\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.11619940093482997 [r2] 0.9943124003477559\n",
      "......Outputs 4.9986 5.7106 2.8768 5.7106 5.3297 2.8609 6.6729 5.1147 1.9828 3.1162\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11619940093482997 [best r2] 0.9943124003477559\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.11400678607364434 [r2] 0.994525018620513\n",
      "......Outputs 5.0384 5.7724 2.9059 5.7724 5.3664 2.8810 6.7326 5.1659 2.0002 3.1285\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11400678607364434 [best r2] 0.994525018620513\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.12632868807914707 [r2] 0.9932775866511747\n",
      "......Outputs 5.1249 5.8375 3.0097 5.8375 5.4706 2.9161 6.8120 5.2215 2.0342 3.1537\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11400678607364434 [best r2] 0.994525018620513\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.13686179074853436 [r2] 0.9921098424146378\n",
      "......Outputs 5.0901 5.7085 2.9901 5.7085 5.4500 2.8762 6.7115 5.1590 2.0447 3.1372\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11400678607364434 [best r2] 0.994525018620513\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.16791142213801669 [r2] 0.9881236846526326\n",
      "......Outputs 4.8631 5.5126 2.7963 5.5126 5.2364 2.7817 6.4874 4.9878 2.0055 3.0710\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11400678607364434 [best r2] 0.994525018620513\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.17750964235981376 [r2] 0.9867271208890799\n",
      "......Outputs 4.9077 5.7241 2.8029 5.7241 5.2391 2.8610 6.6560 5.1130 2.0208 3.1244\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11400678607364434 [best r2] 0.994525018620513\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.20775475342208302 [r2] 0.9818187734139886\n",
      "......Outputs 5.2775 6.0817 3.0350 6.0817 5.5857 3.0291 7.0861 5.4253 2.1213 3.2364\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11400678607364434 [best r2] 0.994525018620513\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.17006837402788938 [r2] 0.9878166040112782\n",
      "......Outputs 5.1201 5.6401 3.0107 5.6401 5.4112 2.8309 6.6317 5.1395 2.1045 3.0754\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11400678607364434 [best r2] 0.994525018620513\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.17578311681660047 [r2] 0.9869840592713284\n",
      "......Outputs 4.7926 5.4802 2.8136 5.4802 5.2470 2.6826 6.3839 4.9140 2.0367 2.9946\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11400678607364434 [best r2] 0.994525018620513\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.13159647465353413 [r2] 0.9927052611131651\n",
      "......Outputs 5.0513 5.9618 2.9425 5.9618 5.4657 2.9455 6.8672 5.2427 2.1146 3.2355\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11400678607364434 [best r2] 0.994525018620513\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.11334979557691421 [r2] 0.994587938496708\n",
      "......Outputs 5.1533 5.7500 2.9782 5.7500 5.3852 2.9550 6.8288 5.2636 2.1282 3.1966\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11334979557691421 [best r2] 0.994587938496708\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.09651723201542453 [r2] 0.9960759822994243\n",
      "......Outputs 4.9829 5.5449 2.8852 5.5449 5.2790 2.7747 6.5664 5.0663 2.0754 3.0242\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09651723201542453 [best r2] 0.9960759822994243\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.08081736436877508 [r2] 0.9972487465068824\n",
      "......Outputs 5.0437 5.8427 2.9473 5.8427 5.4201 2.8325 6.7253 5.1510 2.0978 3.1105\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08081736436877508 [best r2] 0.9972487465068824\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.07672071857445653 [r2] 0.9975206001845897\n",
      "......Outputs 5.0844 5.8169 2.9733 5.8169 5.4343 2.8893 6.7662 5.1891 2.1097 3.1731\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07672071857445653 [best r2] 0.9975206001845897\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.06901398198399336 [r2] 0.9979937021249654\n",
      "......Outputs 5.0074 5.6413 2.9172 5.6413 5.3621 2.8466 6.6511 5.1200 2.0871 3.1104\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06901398198399336 [best r2] 0.9979937021249654\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.06514386656298554 [r2] 0.9982124083997512\n",
      "......Outputs 5.0308 5.7354 2.9338 5.7354 5.3688 2.8584 6.6875 5.1546 2.0933 3.1124\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06514386656298554 [best r2] 0.9982124083997512\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.0630940008655727 [r2] 0.9983231377970461\n",
      "......Outputs 5.0745 5.7875 2.9624 5.7875 5.3830 2.8768 6.7350 5.1798 2.1044 3.1343\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0630940008655727 [best r2] 0.9983231377970461\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.05936778097205977 [r2] 0.9985153541333344\n",
      "......Outputs 5.0413 5.7094 2.9462 5.7094 5.3768 2.8582 6.6940 5.1376 2.0975 3.1169\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05936778097205977 [best r2] 0.9985153541333344\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.05732901459944168 [r2] 0.9986155725722032\n",
      "......Outputs 5.0229 5.7172 2.9432 5.7172 5.3758 2.8600 6.6857 5.1387 2.0969 3.1161\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05732901459944168 [best r2] 0.9986155725722032\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.05534622934819201 [r2] 0.9987096803303466\n",
      "......Outputs 5.0492 5.7673 2.9563 5.7673 5.3863 2.8757 6.7232 5.1692 2.1045 3.1246\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05534622934819201 [best r2] 0.9987096803303466\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.05358326023608205 [r2] 0.9987905734180595\n",
      "......Outputs 5.0598 5.7417 2.9612 5.7417 5.3819 2.8719 6.7161 5.1632 2.1065 3.1191\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05358326023608205 [best r2] 0.9987905734180595\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.05147893816404362 [r2] 0.9988837013449796\n",
      "......Outputs 5.0368 5.7124 2.9531 5.7124 5.3667 2.8583 6.6843 5.1406 2.1029 3.1106\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05147893816404362 [best r2] 0.9988837013449796\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.04978237900340627 [r2] 0.9989560672206341\n",
      "......Outputs 5.0335 5.7392 2.9506 5.7392 5.3726 2.8624 6.6992 5.1477 2.1041 3.1147\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04978237900340627 [best r2] 0.9989560672206341\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.04926237930316212 [r2] 0.9989777620296205\n",
      "......Outputs 5.0589 5.7564 2.9637 5.7564 5.3891 2.8771 6.7247 5.1677 2.1105 3.1232\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04926237930316212 [best r2] 0.9989777620296205\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.04805889783690515 [r2] 0.9990270985421253\n",
      "......Outputs 5.0568 5.7320 2.9670 5.7320 5.3833 2.8704 6.7055 5.1594 2.1109 3.1152\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04805889783690515 [best r2] 0.9990270985421253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.04696316101055017 [r2] 0.9990709568563683\n",
      "......Outputs 5.0308 5.7186 2.9562 5.7186 5.3661 2.8561 6.6816 5.1415 2.1062 3.1067\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04696316101055017 [best r2] 0.9990709568563683\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.04627569783207959 [r2] 0.9990979570968368\n",
      "......Outputs 5.0368 5.7410 2.9566 5.7410 5.3719 2.8653 6.7034 5.1522 2.1078 3.1151\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.04749614184493341 [r2] 0.9990497499377753\n",
      "......Outputs 5.0669 5.7569 2.9720 5.7569 5.3926 2.8807 6.7305 5.1741 2.1151 3.1229\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.047006401041857264 [r2] 0.9990692452872442\n",
      "......Outputs 5.0595 5.7323 2.9738 5.7323 5.3860 2.8689 6.7048 5.1625 2.1153 3.1121\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.046944741112796166 [r2] 0.9990716854923177\n",
      "......Outputs 5.0208 5.7097 2.9554 5.7097 5.3611 2.8502 6.6707 5.1357 2.1080 3.1023\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.04695004067824133 [r2] 0.999071475886681\n",
      "......Outputs 5.0298 5.7388 2.9531 5.7388 5.3655 2.8644 6.7006 5.1498 2.1092 3.1138\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.05077527818018486 [r2] 0.9989140099061429\n",
      "......Outputs 5.0821 5.7746 2.9783 5.7746 5.4010 2.8892 6.7497 5.1877 2.1216 3.1251\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.05167874853245395 [r2] 0.9988750189236685\n",
      "......Outputs 5.0769 5.7374 2.9866 5.7374 5.3994 2.8731 6.7135 5.1727 2.1244 3.1119\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.053856885835173895 [r2] 0.9987781898853269\n",
      "......Outputs 5.0015 5.6813 2.9506 5.6813 5.3487 2.8367 6.6407 5.1173 2.1090 3.0941\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.05708655590271573 [r2] 0.9986272579871227\n",
      "......Outputs 4.9989 5.7237 2.9328 5.7237 5.3382 2.8534 6.6788 5.1300 2.1038 3.1065\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.06651427112898553 [r2] 0.9981364076729428\n",
      "......Outputs 5.1109 5.8170 2.9845 5.8170 5.4131 2.9095 6.7958 5.2149 2.1278 3.1344\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.07785877905296289 [r2] 0.997446496726647\n",
      "......Outputs 5.1355 5.7725 3.0243 5.7725 5.4476 2.8969 6.7616 5.2128 2.1423 3.1230\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.08434228935258573 [r2] 0.9970035156681794\n",
      "......Outputs 4.9651 5.6204 2.9518 5.6204 5.3372 2.8100 6.5772 5.0818 2.1088 3.0782\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.10909166164180896 [r2] 0.9949869232407896\n",
      "......Outputs 4.8911 5.6439 2.8740 5.6439 5.2572 2.8077 6.5805 5.0518 2.0774 3.0803\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.121185662478691 [r2] 0.9938138033205992\n",
      "......Outputs 5.1209 5.8977 2.9662 5.8977 5.4031 2.9419 6.8705 5.2492 2.1238 3.1500\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.17332528297698163 [r2] 0.9873454974778372\n",
      "......Outputs 5.3119 5.9112 3.1206 5.9112 5.5860 2.9844 6.9440 5.3502 2.1880 3.1686\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.1741345079297619 [r2] 0.9872270583881455\n",
      "......Outputs 4.9578 5.5163 2.9961 5.5163 5.3663 2.7738 6.4820 5.0440 2.1260 3.0528\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.246571881985895 [r2] 0.9743900729823682\n",
      "......Outputs 4.6413 5.4186 2.7628 5.4186 5.0867 2.6834 6.2982 4.8519 2.0248 3.0096\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.23339377350374718 [r2] 0.9770543813039794\n",
      "......Outputs 5.0374 5.9697 2.8936 5.9697 5.3631 2.9814 6.9450 5.2699 2.1101 3.1912\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.3059855534717249 [r2] 0.9605612550512063\n",
      "......Outputs 5.5327 6.1441 3.1929 6.1441 5.7519 3.1426 7.2919 5.5809 2.2611 3.2509\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.24886017740645397 [r2] 0.9739125245335922\n",
      "......Outputs 4.9704 5.4165 3.0216 5.4165 5.3156 2.7306 6.3670 5.0047 2.1478 2.9654\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.282037704423725 [r2] 0.9664929970997883\n",
      "......Outputs 4.5722 5.3610 2.7678 5.3610 5.0802 2.5940 6.1544 4.7581 2.0256 2.9432\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.21113019236910407 [r2] 0.9812231849865224\n",
      "......Outputs 5.0771 6.1019 2.9600 6.1019 5.5428 3.0171 7.0435 5.3431 2.1588 3.2981\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.19298663665752094 [r2] 0.9843117061389044\n",
      "......Outputs 5.3259 5.8744 3.0743 5.8744 5.5287 3.0636 7.0384 5.4190 2.2064 3.2250\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.144282549314755 [r2] 0.9912310245275814\n",
      "......Outputs 4.9374 5.4035 2.9079 5.4035 5.2099 2.7189 6.4102 4.9819 2.0916 2.9272\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.11080807035810221 [r2] 0.9948279343965586\n",
      "......Outputs 4.9393 5.7656 2.9271 5.7656 5.3711 2.7533 6.6037 5.0708 2.1055 3.0449\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.09866600738148738 [r2] 0.9958993155660043\n",
      "......Outputs 5.1380 5.9689 3.0291 5.9689 5.5250 2.9400 6.9103 5.2846 2.1678 3.2370\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.06733733621037234 [r2] 0.9980900011522416\n",
      "......Outputs 5.0519 5.6262 2.9650 5.6262 5.3559 2.8803 6.6719 5.1513 2.1259 3.1243\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.06346360983383428 [r2] 0.9983034339054966\n",
      "......Outputs 4.9871 5.6280 2.9195 5.6280 5.3222 2.8196 6.5987 5.1061 2.0993 3.0420\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.05468207082057125 [r2] 0.9987404623703117\n",
      "......Outputs 5.0773 5.8316 2.9765 5.8316 5.4029 2.8702 6.7564 5.2076 2.1287 3.1186\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.05025693958774749 [r2] 0.9989360693557428\n",
      "......Outputs 5.0817 5.7600 2.9845 5.7600 5.3886 2.8747 6.7376 5.1763 2.1298 3.1402\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.046889293473102896 [r2] 0.9990738771094904\n",
      "......Outputs 5.0192 5.6693 2.9485 5.6693 5.3649 2.8490 6.6597 5.1160 2.1116 3.1003\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04627569783207959 [best r2] 0.9990979570968368\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.04487903236740051 [r2] 0.9991515852505278\n",
      "......Outputs 5.0337 5.7412 2.9587 5.7412 5.3831 2.8689 6.6998 5.1627 2.1184 3.1078\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04487903236740051 [best r2] 0.9991515852505278\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.04499972767931702 [r2] 0.999147015750181\n",
      "......Outputs 5.0653 5.7753 2.9755 5.7753 5.3870 2.8810 6.7341 5.1900 2.1261 3.1185\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04487903236740051 [best r2] 0.9991515852505278\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.042978323993654415 [r2] 0.9992219272744715\n",
      "......Outputs 5.0504 5.7246 2.9655 5.7246 5.3731 2.8628 6.6993 5.1521 2.1186 3.1043\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.042978323993654415 [best r2] 0.9992219272744715\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.04277195652794585 [r2] 0.9992293814226412\n",
      "......Outputs 5.0371 5.7159 2.9599 5.7159 5.3714 2.8590 6.6857 5.1417 2.1152 3.1052\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04277195652794585 [best r2] 0.9992293814226412\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.04256416645026896 [r2] 0.9992368507063245\n",
      "......Outputs 5.0486 5.7481 2.9661 5.7481 5.3833 2.8717 6.7129 5.1654 2.1204 3.1151\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04256416645026896 [best r2] 0.9992368507063245\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.04269159508308463 [r2] 0.9992322744325043\n",
      "......Outputs 5.0563 5.7460 2.9704 5.7460 5.3853 2.8742 6.7169 5.1687 2.1223 3.1124\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04256416645026896 [best r2] 0.9992368507063245\n",
      "..Plotting of gradients skipped\n",
      "Finished model size search. The optimal capacity is 1\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this cell should return the integer 1\n",
    "# find the minimum capacity, my net over fit the entire training data. \n",
    "# for this data set training procedure and architecture. That the optimal capacity is one. Therefore we need only one hidden layer to completely over fit the training data, but this may not be the case on different architectures in different data sets.\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn,\n",
    "                 device, do_choose_model_size_by_overfit=True, trainer=trainer)\n",
    "ds.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Verified that shape of model predictions is equal to shape of labels\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 0.23777171038091183\n",
      "..last epoch zero_data_loss 14.244109153747559\n",
      "Input-independent baseline is verified\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "..Epoch 0\n",
      "....Outputs -0.1052 -0.1486 -0.1483 -0.1343 -0.1052\n",
      "....Labels  5.7012 2.9694 5.3739 5.0497 5.6991\n",
      "....Loss: 5.186427729446679\n",
      "....R2: -24.679339335586373\n",
      "..Epoch 1\n",
      "....Outputs -0.0979 -0.0846 -0.0802 -0.0420 -0.0420\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "....Loss: 5.127613424741939\n",
      "....R2: -24.10023388046654\n",
      "..Epoch 2\n",
      "....Outputs -0.0176 -0.0377 0.0243 -0.0491 0.0243\n",
      "....Labels  5.3739 5.0497 5.6991 2.9694 5.7012\n",
      "....Loss: 5.069319288314261\n",
      "....R2: -23.53276465049816\n",
      "..Epoch 3\n",
      "....Outputs 0.0135 0.0490 0.1007 0.0027 0.1007\n",
      "....Labels  5.0497 5.3739 5.7012 2.9694 5.6991\n",
      "....Loss: 5.004514944559789\n",
      "....R2: -22.909536406766325\n",
      "..Epoch 4\n",
      "....Outputs 0.1922 0.0648 0.0738 0.1922 0.1261\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "....Loss: 4.927711925236218\n",
      "....R2: -22.181301918218292\n",
      "..Epoch 5\n",
      "....Outputs 0.2238 0.3047 0.3047 0.1391 0.1472\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "....Loss: 4.832992322445626\n",
      "....R2: -21.29869536615369\n",
      "..Epoch 6\n",
      "....Outputs 0.2357 0.3461 0.4436 0.4436 0.2313\n",
      "....Labels  5.0497 5.3739 5.7012 5.6991 2.9694\n",
      "....Loss: 4.716200588279892\n",
      "....R2: -20.23399606004504\n",
      "..Epoch 7\n",
      "....Outputs 0.3452 0.6153 0.3462 0.6153 0.4984\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "....Loss: 4.571562245662504\n",
      "....R2: -18.951540655391707\n",
      "..Epoch 8\n",
      "....Outputs 0.4893 0.8269 0.6888 0.8269 0.4806\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 4.392630448290525\n",
      "....R2: -17.42029308821763\n",
      "..Epoch 9\n",
      "....Outputs 0.9262 0.6483 0.6641 1.0866 1.0866\n",
      "....Labels  5.3739 5.0497 2.9694 5.7012 5.6991\n",
      "....Loss: 4.172274109683817\n",
      "....R2: -15.618539840097444\n",
      "..Epoch 10\n",
      "....Outputs 1.4034 0.8787 0.8545 1.2216 1.4034\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "....Loss: 3.9021968718448514\n",
      "....R2: -13.536690334443403\n",
      "..Epoch 11\n",
      "....Outputs 1.1415 1.7889 1.5883 1.1058 1.7889\n",
      "....Labels  2.9694 5.7012 5.3739 5.0497 5.6991\n",
      "....Loss: 3.5727570760394065\n",
      "....R2: -11.185803855957978\n",
      "..Epoch 12\n",
      "....Outputs 1.4126 2.2531 1.4633 2.0390 2.2531\n",
      "....Labels  5.0497 5.6991 2.9694 5.3739 5.7012\n",
      "....Loss: 3.1743533219304956\n",
      "....R2: -8.61961539258932\n",
      "..Epoch 13\n",
      "....Outputs 1.8530 1.7834 2.5899 2.8139 2.8139\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "....Loss: 2.6953919329893967\n",
      "....R2: -5.935711571632596\n",
      "..Epoch 14\n",
      "....Outputs 3.2551 2.2293 2.3177 3.4827 3.4827\n",
      "....Labels  5.3739 5.0497 2.9694 5.7012 5.6991\n",
      "....Loss: 2.13085870366912\n",
      "....R2: -3.3346744700775774\n",
      "..Epoch 15\n",
      "....Outputs 4.0529 2.8661 2.7601 4.2762 4.2762\n",
      "....Labels  5.3739 2.9694 5.0497 5.7012 5.6991\n",
      "....Loss: 1.486848236394713\n",
      "....R2: -1.110476155488592\n",
      "..Epoch 16\n",
      "....Outputs 3.3799 4.9821 5.2038 5.2038 3.4908\n",
      "....Labels  5.0497 5.3739 5.7012 5.6991 2.9694\n",
      "....Loss: 0.8609761667309089\n",
      "....R2: 0.29233216116129723\n",
      "..Epoch 17\n",
      "....Outputs 6.2205 6.0028 4.1397 4.0748 6.2205\n",
      "....Labels  5.6991 5.3739 2.9694 5.0497 5.7012\n",
      "....Loss: 0.8070846702421502\n",
      "....R2: 0.37815039425539565\n",
      "..Epoch 18\n",
      "....Outputs 4.6872 7.1724 6.9592 7.1724 4.7735\n",
      "....Labels  2.9694 5.6991 5.3739 5.7012 5.0497\n",
      "....Loss: 1.4053902435005337\n",
      "....R2: -0.8855628815229042\n",
      "..Epoch 19\n",
      "....Outputs 7.5976 4.9880 7.8112 5.3345 7.8112\n",
      "....Labels  5.3739 2.9694 5.6991 5.0497 5.7012\n",
      "....Loss: 1.8981095646948787\n",
      "....R2: -2.439455712689301\n",
      "..Epoch 20\n",
      "....Outputs 5.0153 8.0273 8.0273 5.6613 7.8060\n",
      "....Labels  2.9694 5.6991 5.7012 5.0497 5.3739\n",
      "....Loss: 2.0642832358624017\n",
      "....R2: -3.0680449591529877\n",
      "..Epoch 21\n",
      "....Outputs 4.8362 7.6594 7.8960 7.8960 5.7628\n",
      "....Labels  2.9694 5.3739 5.6991 5.7012 5.0497\n",
      "....Loss: 1.942174195468842\n",
      "....R2: -2.6010036062106825\n",
      "..Epoch 22\n",
      "....Outputs 7.5346 5.6920 7.2789 7.5346 4.5261\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "....Loss: 1.6245314582838515\n",
      "....R2: -1.5194367184135738\n",
      "..Epoch 23\n",
      "....Outputs 7.0570 4.1526 6.7754 5.5078 7.0570\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "....Loss: 1.2046686153688135\n",
      "....R2: -0.38542304351034984\n",
      "..Epoch 24\n",
      "....Outputs 6.5394 5.2658 6.2269 3.7674 6.5394\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "....Loss: 0.7509638632703896\n",
      "....R2: 0.46162451088525147\n",
      "..Epoch 25\n",
      "....Outputs 5.6986 6.0374 3.4027 5.0100 6.0374\n",
      "....Labels  5.3739 5.6991 2.9694 5.0497 5.7012\n",
      "....Loss: 0.3232054214082446\n",
      "....R2: 0.9002749098716902\n",
      "..Epoch 26\n",
      "....Outputs 5.5897 5.5897 4.7738 3.0786 5.2347\n",
      "....Labels  5.6991 5.7012 5.0497 2.9694 5.3739\n",
      "....Loss: 0.16237610968070376\n",
      "....R2: 0.9748295233353597\n",
      "..Epoch 27\n",
      "....Outputs 5.2160 2.8086 4.5773 4.8511 5.2160\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "....Loss: 0.4452406507385457\n",
      "....R2: 0.8107496049089989\n",
      "..Epoch 28\n",
      "....Outputs 4.9235 4.5527 2.5944 4.9235 4.4320\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "....Loss: 0.6932337902401176\n",
      "....R2: 0.5412177311100448\n",
      "..Epoch 29\n",
      "....Outputs 2.4329 4.3390 4.7149 4.7149 4.3426\n",
      "....Labels  2.9694 5.3739 5.7012 5.6991 5.0497\n",
      "....Loss: 0.8718218647068765\n",
      "....R2: 0.27439089336555844\n",
      "..Epoch 30\n",
      "....Outputs 4.2052 4.5848 4.5848 4.3092 2.3203\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "....Loss: 0.982205437239424\n",
      "....R2: 0.07901649031309554\n",
      "..Epoch 31\n",
      "....Outputs 4.1433 2.2507 4.3294 4.5258 4.5258\n",
      "....Labels  5.3739 2.9694 5.0497 5.6991 5.7012\n",
      "....Loss: 1.0303116777744779\n",
      "....R2: -0.013408234989019219\n",
      "..Epoch 32\n",
      "....Outputs 4.3995 4.5308 4.1453 4.5308 2.2193\n",
      "....Labels  5.0497 5.7012 5.3739 5.6991 2.9694\n",
      "....Loss: 1.0226952576683697\n",
      "....R2: 0.0015192399087265995\n",
      "..Epoch 33\n",
      "....Outputs 4.2044 4.5928 2.2232 4.5150 4.5928\n",
      "....Labels  5.3739 5.6991 2.9694 5.0497 5.7012\n",
      "....Loss: 0.9657033573690726\n",
      "....R2: 0.10970342087499341\n",
      "..Epoch 34\n",
      "....Outputs 4.7052 4.7052 2.2580 4.6698 4.3139\n",
      "....Labels  5.7012 5.6991 2.9694 5.0497 5.3739\n",
      "....Loss: 0.8664780885377048\n",
      "....R2: 0.2832588344742023\n",
      "..Epoch 35\n",
      "....Outputs 2.3184 4.8568 4.4664 4.8605 4.8605\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "....Loss: 0.734123673957291\n",
      "....R2: 0.48549968818716505\n",
      "..Epoch 36\n",
      "....Outputs 2.3997 5.0501 5.0664 5.0501 4.6533\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "....Loss: 0.5812336109270901\n",
      "....R2: 0.677485961450596\n",
      "..Epoch 37\n",
      "....Outputs 5.2637 5.2865 2.4971 4.8644 5.2637\n",
      "....Labels  5.7012 5.0497 2.9694 5.3739 5.6991\n",
      "....Loss: 0.4288778132338331\n",
      "....R2: 0.8244041020197346\n",
      "..Epoch 38\n",
      "....Outputs 5.0872 5.4886 5.5022 5.4886 2.6035\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "....Loss: 0.31951137704770927\n",
      "....R2: 0.902541476108728\n",
      "..Epoch 39\n",
      "....Outputs 5.7098 5.7098 2.7111 5.3067 5.6965\n",
      "....Labels  5.6991 5.7012 2.9694 5.3739 5.0497\n",
      "....Loss: 0.31297209366046885\n",
      "....R2: 0.9064899237495049\n",
      "..Epoch 40\n",
      "....Outputs 5.5069 5.9112 5.9112 5.8520 2.8116\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "....Loss: 0.393811636596421\n",
      "....R2: 0.8519445937243905\n",
      "..Epoch 41\n",
      "....Outputs 6.0771 6.0771 2.8974 5.9540 5.6722\n",
      "....Labels  5.6991 5.7012 2.9694 5.0497 5.3739\n",
      "....Loss: 0.48911566582441723\n",
      "....R2: 0.7716135475000772\n",
      "..Epoch 42\n",
      "....Outputs 2.9615 6.1946 5.9930 5.7897 6.1947\n",
      "....Labels  2.9694 5.7012 5.0497 5.3739 5.6991\n",
      "....Loss: 0.5571170574937583\n",
      "....R2: 0.7036942384867952\n",
      "..Epoch 43\n",
      "....Outputs 5.9667 5.8522 6.2563 2.9996 6.2563\n",
      "....Labels  5.0497 5.3739 5.6991 2.9694 5.7012\n",
      "....Loss: 0.5812526846382737\n",
      "....R2: 0.6774647761191975\n",
      "..Epoch 44\n",
      "....Outputs 6.2612 6.2612 3.0111 5.8590 5.8803\n",
      "....Labels  5.6991 5.7012 2.9694 5.3739 5.0497\n",
      "....Loss: 0.557944126787612\n",
      "....R2: 0.7028138104609709\n",
      "..Epoch 45\n",
      "....Outputs 2.9989 6.2151 6.2151 5.7448 5.8159\n",
      "....Labels  2.9694 5.7012 5.6991 5.0497 5.3739\n",
      "....Loss: 0.49185178297184445\n",
      "....R2: 0.7690512177828328\n",
      "..Epoch 46\n",
      "....Outputs 5.7336 6.1284 6.1284 2.9682 5.5753\n",
      "....Labels  5.3739 5.6991 5.7012 2.9694 5.0497\n",
      "....Loss: 0.3930587104375698\n",
      "....R2: 0.8525101829701734\n",
      "..Epoch 47\n",
      "....Outputs 5.3881 5.6256 6.0149 6.0149 2.9259\n",
      "....Labels  5.0497 5.3739 5.7012 5.6991 2.9694\n",
      "....Loss: 0.27491513590074784\n",
      "....R2: 0.927848642551097\n",
      "..Epoch 48\n",
      "....Outputs 5.5060 5.8885 5.8885 2.8791 5.1988\n",
      "....Labels  5.3739 5.6991 5.7012 2.9694 5.0497\n",
      "....Loss: 0.15414953440010626\n",
      "....R2: 0.9773153722921032\n",
      "..Epoch 49\n",
      "....Outputs 5.0205 2.8342 5.7624 5.7624 5.3876\n",
      "....Labels  5.0497 2.9694 5.6991 5.7012 5.3739\n",
      "....Loss: 0.07357395055614144\n",
      "....R2: 0.9948323232072149\n",
      "Verified that a small batch can be overfit since the R2 was greater than 0.99\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs -0.0734 -0.0912 -0.0731 -0.0912 -0.0706\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "....Loss: -0.07340135425329208\n",
      "Finished charting dependencies. Data is not getting mixed between instances in the same batch.\n",
      "\n",
      "\n",
      "Beginning model size search\n",
      "\n",
      "..Training model 0 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.697626745953917 [r2] -8.295617695251963\n",
      "......Outputs -0.1117 -0.1060 -0.1165 -0.1060 -0.1006 -0.0928 -0.0947 -0.1026 -0.0865 -0.0975\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.697626745953917 [best r2] -8.295617695251963\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.79299482316163 [r2] -5.060183027811168\n",
      "......Outputs 0.3317 0.6008 0.4492 0.6008 0.6427 0.7694 0.4853 0.5329 0.4608 0.6642\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.79299482316163 [best r2] -5.060183027811168\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.6929199222363396 [r2] -2.0546968661917835\n",
      "......Outputs 2.1174 3.6396 2.7610 3.6396 3.7770 4.7347 2.8289 3.4942 2.6360 3.9811\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.6929199222363396 [best r2] -2.0546968661917835\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 1.9878718278733132 [r2] -0.6645544216840304\n",
      "......Outputs 3.6848 6.9816 4.3623 6.9816 7.3275 7.5507 5.5981 6.1675 4.2941 5.9096\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.9878718278733132 [best r2] -0.6645544216840304\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.8505523344899761 [r2] -0.4425271113653799\n",
      "......Outputs 2.3987 5.0649 2.3987 5.0649 5.3919 4.3151 4.4135 3.9584 2.6107 3.0225\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.8505523344899761 [best r2] -0.4425271113653799\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.5403476260428484 [r2] 0.0005553949109297074\n",
      "......Outputs 2.5190 5.8594 2.1533 5.8594 6.2755 3.9288 5.4288 4.1736 2.5477 2.4866\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.5403476260428484 [best r2] 0.0005553949109297074\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.43882637074159 [r2] 0.12795676211978035\n",
      "......Outputs 3.2171 7.6031 2.4966 7.6031 8.0687 4.4073 7.4178 5.3491 3.0552 2.7485\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.43882637074159 [best r2] 0.12795676211978035\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.268554373713264 [r2] 0.32214092462157295\n",
      "......Outputs 3.0968 6.4079 2.2522 6.4079 6.7954 3.6861 6.9024 4.8943 2.7063 2.4952\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.268554373713264 [best r2] 0.32214092462157295\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.1085503710806774 [r2] 0.4823549037408792\n",
      "......Outputs 3.3854 5.9062 2.4071 5.9062 6.2207 3.5348 6.9683 5.0195 2.7113 2.7171\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.1085503710806774 [best r2] 0.4823549037408792\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 0.9916940402042991 [r2] 0.5857365216557757\n",
      "......Outputs 4.1659 6.2125 2.9949 6.2125 6.4838 3.8614 7.8324 5.7578 3.0615 3.4075\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.9916940402042991 [best r2] 0.5857365216557757\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8687257453056475 [r2] 0.6821028564976261\n",
      "......Outputs 4.2991 5.5958 3.0057 5.5958 5.8628 3.4754 7.4516 5.3757 2.7987 3.2988\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8687257453056475 [best r2] 0.6821028564976261\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7605979847680534 [r2] 0.7563134375369798\n",
      "......Outputs 4.6813 5.5790 3.1905 5.5790 5.8084 3.3174 7.5436 5.2951 2.6850 3.2555\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7605979847680534 [best r2] 0.7563134375369798\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.6755931249602666 [r2] 0.807738784480458\n",
      "......Outputs 4.9905 5.7127 3.3415 5.7127 5.8464 3.1911 7.5968 5.2304 2.5591 3.1626\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6755931249602666 [best r2] 0.807738784480458\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.6022695183683394 [r2] 0.8472071583767091\n",
      "......Outputs 4.9873 5.5720 3.2711 5.5720 5.6097 2.9869 7.2208 4.9893 2.3495 2.9912\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6022695183683394 [best r2] 0.8472071583767091\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5396828600636978 [r2] 0.8773130156309291\n",
      "......Outputs 5.1674 5.7685 3.3597 5.7685 5.7291 2.9609 7.1960 5.0874 2.3173 3.0772\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5396828600636978 [best r2] 0.8773130156309291\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.4819304404649729 [r2] 0.9021659666258619\n",
      "......Outputs 5.0329 5.6778 3.1520 5.6778 5.6710 2.8419 6.9170 4.9916 2.2209 3.0469\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4819304404649729 [best r2] 0.9021659666258619\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.43345704664500184 [r2] 0.9208568419224482\n",
      "......Outputs 5.0423 5.7549 3.0967 5.7549 5.8170 2.8199 6.8538 5.1015 2.1892 3.1563\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.43345704664500184 [best r2] 0.9208568419224482\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.3902864096565096 [r2] 0.9358364922369173\n",
      "......Outputs 4.9520 5.6459 2.9904 5.6459 5.7547 2.7482 6.6711 5.0772 2.1171 3.1833\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3902864096565096 [best r2] 0.9358364922369173\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.35252396688252285 [r2] 0.9476521875352443\n",
      "......Outputs 4.9652 5.6959 2.9635 5.6959 5.7776 2.7291 6.6695 5.1497 2.0808 3.2153\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.35252396688252285 [best r2] 0.9476521875352443\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.31835264085721293 [r2] 0.9573088204371304\n",
      "......Outputs 4.9406 5.6624 2.9273 5.6624 5.6728 2.6987 6.6172 5.1271 2.0283 3.1929\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.31835264085721293 [best r2] 0.9573088204371304\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.28875131575252494 [r2] 0.9648788130699619\n",
      "......Outputs 4.9751 5.7090 2.9395 5.7090 5.6354 2.7107 6.6517 5.1621 2.0002 3.1985\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.28875131575252494 [best r2] 0.9648788130699619\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.26281141079227355 [r2] 0.9709055818688267\n",
      "......Outputs 4.9828 5.6995 2.9299 5.6995 5.5689 2.7086 6.6221 5.1392 1.9718 3.1824\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.26281141079227355 [best r2] 0.9709055818688267\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.24094067937009578 [r2] 0.9755464733075262\n",
      "......Outputs 5.0044 5.7130 2.9310 5.7130 5.5510 2.7266 6.6215 5.1416 1.9540 3.1901\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.24094067937009578 [best r2] 0.9755464733075262\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.22201847264415617 [r2] 0.9792365525934041\n",
      "......Outputs 5.0277 5.7272 2.9251 5.7272 5.5348 2.7512 6.6341 5.1409 1.9437 3.1965\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.22201847264415617 [best r2] 0.9792365525934041\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.2058966256232557 [r2] 0.9821425394574933\n",
      "......Outputs 5.0333 5.7178 2.9150 5.7178 5.5055 2.7731 6.6392 5.1281 1.9380 3.1938\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2058966256232557 [best r2] 0.9821425394574933\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.1920536685105207 [r2] 0.9844630254133524\n",
      "......Outputs 5.0473 5.7324 2.9150 5.7324 5.4965 2.7912 6.6653 5.1393 1.9370 3.1882\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1920536685105207 [best r2] 0.9844630254133524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.17978612734091132 [r2] 0.986384499947238\n",
      "......Outputs 5.0575 5.7334 2.9197 5.7334 5.4801 2.8086 6.6849 5.1511 1.9417 3.1805\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17978612734091132 [best r2] 0.986384499947238\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.16877867505441155 [r2] 0.9880006868499874\n",
      "......Outputs 5.0523 5.7444 2.9190 5.7444 5.4616 2.8246 6.6918 5.1436 1.9713 3.1676\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16877867505441155 [best r2] 0.9880006868499874\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.15896543909911795 [r2] 0.9893554656228891\n",
      "......Outputs 5.0457 5.7302 2.9209 5.7302 5.4496 2.8381 6.7141 5.1486 2.0046 3.1647\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15896543909911795 [best r2] 0.9893554656228891\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.15002232654169556 [r2] 0.9905194609090558\n",
      "......Outputs 5.0408 5.7285 2.9185 5.7285 5.4435 2.8438 6.7138 5.1464 2.0353 3.1577\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15002232654169556 [best r2] 0.9905194609090558\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.14191981011739166 [r2] 0.9915158706008043\n",
      "......Outputs 5.0411 5.7362 2.9182 5.7362 5.4355 2.8497 6.6980 5.1484 2.0603 3.1517\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14191981011739166 [best r2] 0.9915158706008043\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.13462738976039612 [r2] 0.9923653682885244\n",
      "......Outputs 5.0433 5.7361 2.9189 5.7361 5.4290 2.8536 6.6971 5.1491 2.0831 3.1438\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13462738976039612 [best r2] 0.9923653682885244\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.12794842770821835 [r2] 0.9931040969394219\n",
      "......Outputs 5.0433 5.7344 2.9180 5.7344 5.4252 2.8550 6.7048 5.1522 2.0970 3.1347\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12794842770821835 [best r2] 0.9931040969394219\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.12177097814254245 [r2] 0.9937539014807909\n",
      "......Outputs 5.0409 5.7323 2.9174 5.7323 5.4161 2.8556 6.7043 5.1507 2.1077 3.1290\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12177097814254245 [best r2] 0.9937539014807909\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.11606231864822457 [r2] 0.9943258119362027\n",
      "......Outputs 5.0509 5.7391 2.9192 5.7391 5.4079 2.8576 6.7034 5.1540 2.1166 3.1259\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11606231864822457 [best r2] 0.9943258119362027\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.11077171709461847 [r2] 0.9948313274819275\n",
      "......Outputs 5.0536 5.7436 2.9218 5.7436 5.4033 2.8593 6.7082 5.1613 2.1224 3.1217\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11077171709461847 [best r2] 0.9948313274819275\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.10579483900313792 [r2] 0.9952853418886314\n",
      "......Outputs 5.0475 5.7280 2.9259 5.7280 5.3996 2.8605 6.6994 5.1590 2.1232 3.1177\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10579483900313792 [best r2] 0.9952853418886314\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.10108122779280401 [r2] 0.995696099149934\n",
      "......Outputs 5.0390 5.7298 2.9231 5.7298 5.3883 2.8566 6.6952 5.1504 2.1192 3.1123\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10108122779280401 [best r2] 0.995696099149934\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.09671055223417867 [r2] 0.9960602472506803\n",
      "......Outputs 5.0489 5.7430 2.9250 5.7430 5.3845 2.8581 6.7136 5.1597 2.1192 3.1120\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09671055223417867 [best r2] 0.9960602472506803\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.09322057043093619 [r2] 0.9963394634034239\n",
      "......Outputs 5.0679 5.7589 2.9412 5.7589 5.3988 2.8681 6.7229 5.1759 2.1289 3.1142\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09322057043093619 [best r2] 0.9963394634034239\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.09038293544962138 [r2] 0.9965589251038808\n",
      "......Outputs 5.0543 5.7184 2.9509 5.7184 5.4020 2.8680 6.6990 5.1619 2.1292 3.1095\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09038293544962138 [best r2] 0.9965589251038808\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.08795076050275073 [r2] 0.9967416296954872\n",
      "......Outputs 5.0112 5.7008 2.9310 5.7008 5.3621 2.8499 6.6589 5.1198 2.1088 3.0972\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.08921583758628175 [r2] 0.9966472192437614\n",
      "......Outputs 5.0183 5.7442 2.9197 5.7442 5.3372 2.8513 6.6816 5.1367 2.1002 3.0958\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.0943367563350172 [r2] 0.9962512789944188\n",
      "......Outputs 5.1079 5.8298 2.9660 5.8298 5.4181 2.8918 6.7886 5.2329 2.1366 3.1165\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.11333221525334018 [r2] 0.9945896171660604\n",
      "......Outputs 5.1411 5.7517 3.0151 5.7517 5.4901 2.9054 6.7745 5.2187 2.1731 3.1286\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.12630717095359564 [r2] 0.9932798764666652\n",
      "......Outputs 4.9446 5.5383 2.9262 5.5383 5.3101 2.8156 6.5569 4.9935 2.0913 3.0809\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.16765789603281608 [r2] 0.988159521200792\n",
      "......Outputs 4.8376 5.6112 2.8359 5.6112 5.1786 2.7869 6.5137 4.9854 2.0120 3.0580\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.18726095013971622 [r2] 0.9852288031415728\n",
      "......Outputs 5.1573 6.0647 3.0129 6.0647 5.4671 2.9751 6.9534 5.4270 2.1176 3.1647\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.20807149143522635 [r2] 0.981763293813462\n",
      "......Outputs 5.3201 5.8545 3.1535 5.8545 5.6268 3.0116 6.9483 5.3397 2.2374 3.1311\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.20103115444706038 [r2] 0.9829765345289343\n",
      "......Outputs 4.8276 5.3253 2.8943 5.3253 5.2233 2.7196 6.3364 4.7273 2.0950 2.9567\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.17084552621435384 [r2] 0.9877050019995159\n",
      "......Outputs 4.8283 5.7241 2.8744 5.7241 5.3017 2.7929 6.5591 5.0288 2.0524 3.1404\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.1639481018409821 [r2] 0.9886777164594552\n",
      "......Outputs 5.2401 6.0232 3.0810 6.0232 5.5876 3.0330 7.0133 5.5220 2.1570 3.2688\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.11478622786998481 [r2] 0.9944498999849829\n",
      "......Outputs 5.1164 5.5157 3.0129 5.5157 5.3489 2.8708 6.6287 5.0985 2.1338 2.9814\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.10398236531171703 [r2] 0.9954455008506692\n",
      "......Outputs 4.9570 5.7174 2.9241 5.7174 5.3295 2.7719 6.5874 4.9980 2.1040 2.9676\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.08808692825410958 [r2] 0.9967315324906999\n",
      "......Outputs 5.0766 5.9677 3.0114 5.9677 5.4807 2.8979 6.8377 5.2659 2.1485 3.2064\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08795076050275073 [best r2] 0.9967416296954872\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.0714049693617013 [r2] 0.9978522777559701\n",
      "......Outputs 5.0549 5.6105 2.9938 5.6105 5.3945 2.8845 6.6984 5.1662 2.1266 3.1303\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0714049693617013 [best r2] 0.9978522777559701\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.06634434034065682 [r2] 0.9981459177278036\n",
      "......Outputs 5.0028 5.6336 2.9423 5.6336 5.3568 2.8296 6.6339 5.1025 2.0995 3.0105\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06634434034065682 [best r2] 0.9981459177278036\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.061418355967869015 [r2] 0.9984110229844367\n",
      "......Outputs 5.0737 5.8521 2.9881 5.8521 5.3992 2.8704 6.7372 5.2138 2.1250 3.0977\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.061418355967869015 [best r2] 0.9984110229844367\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.056735882086542316 [r2] 0.9986440712678384\n",
      "......Outputs 5.0733 5.7470 2.9846 5.7470 5.3912 2.8709 6.7187 5.1656 2.1279 3.1176\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.056735882086542316 [best r2] 0.9986440712678384\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.054494287835775555 [r2] 0.9987490982400713\n",
      "......Outputs 5.0279 5.6691 2.9536 5.6691 5.3775 2.8498 6.6804 5.1157 2.1139 3.0733\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054494287835775555 [best r2] 0.9987490982400713\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.052422368648623964 [r2] 0.998842410653486\n",
      "......Outputs 5.0436 5.7573 2.9693 5.7573 5.3819 2.8678 6.7151 5.1771 2.1185 3.0908\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.052422368648623964 [best r2] 0.998842410653486\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.05179981342485702 [r2] 0.9988697418902323\n",
      "......Outputs 5.0626 5.7695 2.9782 5.7695 5.3842 2.8739 6.7234 5.1818 2.1226 3.0996\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05179981342485702 [best r2] 0.9988697418902323\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.05024666058236993 [r2] 0.9989365045207378\n",
      "......Outputs 5.0492 5.7107 2.9695 5.7107 5.3721 2.8616 6.6903 5.1408 2.1184 3.0891\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05024666058236993 [best r2] 0.9989365045207378\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.049089339619960086 [r2] 0.9989849308699387\n",
      "......Outputs 5.0396 5.7271 2.9646 5.7271 5.3698 2.8606 6.6940 5.1491 2.1164 3.0918\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.049089339619960086 [best r2] 0.9989849308699387\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.048959315653989366 [r2] 0.9989903010183887\n",
      "......Outputs 5.0538 5.7590 2.9722 5.7590 5.3822 2.8718 6.7196 5.1749 2.1212 3.1007\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.048959315653989366 [best r2] 0.9989903010183887\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.048329055768435936 [r2] 0.9990161296763205\n",
      "......Outputs 5.0583 5.7327 2.9764 5.7327 5.3787 2.8716 6.7083 5.1630 2.1226 3.0976\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.048329055768435936 [best r2] 0.9990161296763205\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.04738860665968101 [r2] 0.9990540479558676\n",
      "......Outputs 5.0411 5.7203 2.9661 5.7203 5.3652 2.8595 6.6895 5.1436 2.1166 3.0894\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04738860665968101 [best r2] 0.9990540479558676\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.046857554466725 [r2] 0.9990751304562477\n",
      "......Outputs 5.0421 5.7438 2.9660 5.7438 5.3694 2.8631 6.7028 5.1587 2.1160 3.0985\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046857554466725 [best r2] 0.9990751304562477\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.04724982903384217 [r2] 0.999059580288031\n",
      "......Outputs 5.0602 5.7482 2.9767 5.7482 5.3828 2.8742 6.7186 5.1741 2.1224 3.1049\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046857554466725 [best r2] 0.9990751304562477\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.04656536966725283 [r2] 0.9990866287213124\n",
      "......Outputs 5.0555 5.7285 2.9752 5.7285 5.3760 2.8682 6.7036 5.1571 2.1226 3.0949\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04656536966725283 [best r2] 0.9990866287213124\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.04597707590544897 [r2] 0.9991095614881906\n",
      "......Outputs 5.0360 5.7250 2.9644 5.7250 5.3612 2.8575 6.6871 5.1422 2.1156 3.0922\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04597707590544897 [best r2] 0.9991095614881906\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.04583921155596092 [r2] 0.9991148935226217\n",
      "......Outputs 5.0442 5.7440 2.9672 5.7440 5.3700 2.8656 6.7049 5.1622 2.1155 3.1037\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.04697528626451899 [r2] 0.9990704770616508\n",
      "......Outputs 5.0674 5.7495 2.9799 5.7495 5.3876 2.8776 6.7241 5.1798 2.1240 3.1062\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.046217764153983085 [r2] 0.9991002142622034\n",
      "......Outputs 5.0551 5.7279 2.9760 5.7279 5.3766 2.8670 6.7014 5.1536 2.1238 3.0940\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.04634608723061427 [r2] 0.9990952108368968\n",
      "......Outputs 5.0253 5.7176 2.9596 5.7176 5.3553 2.8524 6.6768 5.1311 2.1124 3.0925\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.046467707822387266 [r2] 0.9990904559424124\n",
      "......Outputs 5.0418 5.7451 2.9643 5.7451 5.3688 2.8660 6.7058 5.1657 2.1124 3.1068\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.050065801065642186 [r2] 0.998944146704826\n",
      "......Outputs 5.0837 5.7640 2.9871 5.7640 5.4002 2.8864 6.7412 5.1976 2.1287 3.1099\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.049548656412393435 [r2] 0.9989658465009625\n",
      "......Outputs 5.0621 5.7264 2.9822 5.7264 5.3840 2.8694 6.7025 5.1509 2.1294 3.0934\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.05275989204140446 [r2] 0.9988274563007729\n",
      "......Outputs 4.9997 5.6928 2.9479 5.6928 5.3378 2.8392 6.6483 5.1027 2.1049 3.0874\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.05432007034508529 [r2] 0.9987570836864877\n",
      "......Outputs 5.0248 5.7434 2.9505 5.7434 5.3547 2.8601 6.6974 5.1651 2.1011 3.1074\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.06672539731476879 [r2] 0.9981245582585972\n",
      "......Outputs 5.1240 5.8061 3.0038 5.8061 5.4303 2.9085 6.7884 5.2464 2.1389 3.1207\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.07101952810699096 [r2] 0.9978754018183911\n",
      "......Outputs 5.0981 5.7357 3.0082 5.7357 5.4180 2.8845 6.7259 5.1611 2.1507 3.0961\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.08562312864069073 [r2] 0.9969118141664036\n",
      "......Outputs 4.9375 5.6237 2.9234 5.6237 5.2959 2.8051 6.5740 5.0219 2.0914 3.0701\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.10017967138776594 [r2] 0.9957725308558085\n",
      "......Outputs 4.9487 5.7062 2.9005 5.7062 5.2899 2.8265 6.6356 5.1246 2.0618 3.0965\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.1305538481786291 [r2] 0.9928203942629923\n",
      "......Outputs 5.2109 5.9176 3.0300 5.9176 5.4915 2.9594 6.9021 5.3760 2.1530 3.1478\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.16280915608907184 [r2] 0.9888344816096426\n",
      "......Outputs 5.2482 5.8118 3.0982 5.8118 5.5532 2.9540 6.8519 5.2550 2.2237 3.1136\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.19053044076609962 [r2] 0.984708503658437\n",
      "......Outputs 4.8166 5.4652 2.8890 5.4652 5.2291 2.7281 6.4095 4.8271 2.0836 3.0226\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.24919379785791332 [r2] 0.9738425322251458\n",
      "......Outputs 4.6862 5.5335 2.7599 5.5335 5.1002 2.7007 6.3991 4.9277 1.9591 3.0538\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.2777271914235225 [r2] 0.9675093765121955\n",
      "......Outputs 5.2646 6.1382 3.0287 6.1382 5.5628 3.0355 7.0983 5.6112 2.1360 3.2166\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.3285041764166213 [r2] 0.9545427633394944\n",
      "......Outputs 5.5633 5.9711 3.2542 5.9711 5.7900 3.1190 7.1339 5.5058 2.3413 3.1485\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.31417309956736666 [r2] 0.9584224174685698\n",
      "......Outputs 4.7504 5.2383 2.8596 5.2383 5.1550 2.6327 6.1883 4.5832 2.1085 2.8945\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.3195067573101794 [r2] 0.9569987246743543\n",
      "......Outputs 4.5446 5.5391 2.7069 5.5391 5.0749 2.6170 6.2702 4.7612 1.9465 3.0502\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.28712341203515535 [r2] 0.9652737047427694\n",
      "......Outputs 5.2483 6.2903 3.0448 6.2903 5.7212 3.1013 7.2391 5.6860 2.1407 3.3769\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.2199331920765911 [r2] 0.9796247570449219\n",
      "......Outputs 5.3480 5.6050 3.1041 5.6050 5.5331 3.0185 6.8673 5.3489 2.2029 3.0706\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.1914690577606694 [r2] 0.9845574704504828\n",
      "......Outputs 4.8824 5.3641 2.8639 5.3641 5.1576 2.6675 6.3129 4.7926 2.0795 2.8358\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.129565450884282 [r2] 0.9929286934917416\n",
      "......Outputs 4.9751 6.0366 2.9343 6.0366 5.4357 2.8043 6.7546 5.1555 2.1274 3.1294\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.11281605224454422 [r2] 0.9946387872990777\n",
      "......Outputs 5.1380 5.8985 3.0284 5.8985 5.5180 2.9533 6.9100 5.3323 2.1658 3.2737\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.07958711154666125 [r2] 0.9973318715901718\n",
      "......Outputs 5.0120 5.4439 2.9365 5.4439 5.3189 2.8347 6.5820 5.0803 2.0903 3.0456\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.06638418142858274 [r2] 0.9981436902330619\n",
      "......Outputs 5.0216 5.7178 2.9307 5.7178 5.3621 2.8170 6.6465 5.1400 2.0907 3.0161\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.06256216502173258 [r2] 0.9983512880724996\n",
      "......Outputs 5.0998 5.9078 2.9867 5.9078 5.4162 2.8748 6.7753 5.2395 2.1350 3.1227\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04583921155596092 [best r2] 0.9991148935226217\n",
      "..Plotting of gradients skipped\n",
      "\n",
      "..Training model 1 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.59029739320752 [r2] -7.875705574440387\n",
      "......Outputs -0.0176 -0.0250 -0.0257 -0.0250 -0.0275 -0.0358 -0.0153 -0.0124 -0.0128 -0.0144\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.59029739320752 [best r2] -7.875705574440387\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.471931178578515 [r2] -4.077658123720971\n",
      "......Outputs 0.5988 0.8731 0.6854 0.8731 0.9163 1.0334 0.7708 0.8611 0.6119 0.9366\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.471931178578515 [best r2] -4.077658123720971\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.5953608407543527 [r2] -1.8373749888166597\n",
      "......Outputs 2.9491 4.6967 3.5321 4.6967 4.8790 5.7530 3.7993 4.5529 3.1494 5.0284\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.5953608407543527 [best r2] -1.8373749888166597\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 1.8379189085018708 [r2] -0.4228985358495627\n",
      "......Outputs 3.5727 6.5410 3.8432 6.5410 6.9072 6.2597 5.4938 5.6133 3.6466 4.9880\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.8379189085018708 [best r2] -0.4228985358495627\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.787158946712616 [r2] -0.34538815592104344\n",
      "......Outputs 2.3901 4.7951 2.1812 4.7951 5.2149 3.5298 4.3935 3.7158 2.2882 2.5088\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.787158946712616 [best r2] -0.34538815592104344\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.4768276391810096 [r2] 0.08128487939012208\n",
      "......Outputs 2.8566 6.2475 2.2711 6.2475 6.7639 3.7130 5.9259 4.4689 2.5982 2.4304\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.4768276391810096 [best r2] 0.08128487939012208\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.351007926115308 [r2] 0.23115811505646722\n",
      "......Outputs 3.4726 7.4931 2.5376 7.4931 8.0282 4.0102 7.4776 5.4408 3.0261 2.6799\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.351007926115308 [best r2] 0.23115811505646722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.2129663636464343 [r2] 0.3802468306217779\n",
      "......Outputs 3.1858 5.9666 2.2142 5.9666 6.4575 3.1544 6.6191 4.7816 2.5952 2.4258\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.2129663636464343 [best r2] 0.3802468306217779\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.0471001272744604 [r2] 0.5381534905716436\n",
      "......Outputs 3.6408 5.8703 2.5117 5.8703 6.3322 3.2693 7.0701 5.2588 2.8016 2.9213\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0471001272744604 [best r2] 0.5381534905716436\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 0.9251318971703217 [r2] 0.6394806735957446\n",
      "......Outputs 4.3066 6.0685 2.9563 6.0685 6.4540 3.5286 7.7239 5.8273 3.0526 3.4884\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.9251318971703217 [best r2] 0.6394806735957446\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8102487751575443 [r2] 0.7234599689633869\n",
      "......Outputs 4.3155 5.4227 2.8883 5.4227 5.7513 3.0610 7.2285 5.3316 2.7115 3.2818\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8102487751575443 [best r2] 0.7234599689633869\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7118792701495031 [r2] 0.7865314333987087\n",
      "......Outputs 4.8149 5.6922 3.1555 5.6922 5.9129 3.0561 7.5801 5.4522 2.6941 3.3141\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7118792701495031 [best r2] 0.7865314333987087\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.6280047195404003 [r2] 0.8338703860216212\n",
      "......Outputs 4.9620 5.6696 3.2230 5.6696 5.7079 2.8613 7.3654 5.2692 2.5070 3.1209\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6280047195404003 [best r2] 0.8338703860216212\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.5636183199266551 [r2] 0.8661891150337018\n",
      "......Outputs 5.0503 5.6924 3.2818 5.6924 5.5879 2.7475 7.1379 5.1553 2.3717 3.0026\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5636183199266551 [best r2] 0.8661891150337018\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5041323908026056 [r2] 0.8929441399261837\n",
      "......Outputs 5.1440 5.8216 3.3303 5.8216 5.6918 2.7275 7.0734 5.1710 2.3055 3.0019\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5041323908026056 [best r2] 0.8929441399261837\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.4535588252173791 [r2] 0.913346026028516\n",
      "......Outputs 5.0481 5.6993 3.2062 5.6993 5.7132 2.6369 6.8735 5.0863 2.2062 2.9566\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4535588252173791 [best r2] 0.913346026028516\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.40915426486209644 [r2] 0.9294827437913429\n",
      "......Outputs 5.0569 5.7271 3.1794 5.7271 5.8480 2.6838 6.8456 5.1498 2.1667 3.0228\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.40915426486209644 [best r2] 0.9294827437913429\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.3705257040749813 [r2] 0.9421693700282965\n",
      "......Outputs 4.9804 5.6313 3.0854 5.6313 5.8050 2.6806 6.7108 5.1287 2.0825 3.0561\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3705257040749813 [best r2] 0.9421693700282965\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.33658258061287105 [r2] 0.9522795519489535\n",
      "......Outputs 5.0109 5.6911 3.0573 5.6911 5.7998 2.7246 6.6959 5.1821 2.0478 3.1083\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.33658258061287105 [best r2] 0.9522795519489535\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.3066511010997725 [r2] 0.9603895024361131\n",
      "......Outputs 5.0011 5.6761 3.0008 5.6761 5.6826 2.7168 6.6422 5.1920 2.0071 3.0977\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3066511010997725 [best r2] 0.9603895024361131\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.28098604386960485 [r2] 0.9667424126571625\n",
      "......Outputs 5.0278 5.7071 2.9828 5.7071 5.6261 2.7567 6.6525 5.2387 2.0022 3.1074\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.28098604386960485 [best r2] 0.9667424126571625\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.2589282862481249 [r2] 0.9717589893438124\n",
      "......Outputs 5.0201 5.6965 2.9443 5.6965 5.5648 2.7742 6.6322 5.2484 2.0026 3.0980\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2589282862481249 [best r2] 0.9717589893438124\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.24018497030274574 [r2] 0.9756996294377426\n",
      "......Outputs 5.0271 5.7133 2.9251 5.7133 5.5402 2.7990 6.6406 5.2587 2.0138 3.1032\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.24018497030274574 [best r2] 0.9756996294377426\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.22374584448782286 [r2] 0.9789122037318171\n",
      "......Outputs 5.0317 5.7161 2.9117 5.7161 5.5165 2.8176 6.6543 5.2600 2.0212 3.1189\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.22374584448782286 [best r2] 0.9789122037318171\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.20946028790910776 [r2] 0.9815190354841221\n",
      "......Outputs 5.0197 5.7135 2.8922 5.7135 5.4911 2.8215 6.6651 5.2604 2.0276 3.1268\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.20946028790910776 [best r2] 0.9815190354841221\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.19705323194584554 [r2] 0.9836435757961038\n",
      "......Outputs 5.0204 5.7326 2.8925 5.7326 5.4811 2.8289 6.6881 5.2681 2.0393 3.1367\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.19705323194584554 [best r2] 0.9836435757961038\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.18585831578581177 [r2] 0.9854492548083709\n",
      "......Outputs 5.0168 5.7167 2.8934 5.7167 5.4597 2.8355 6.6913 5.2589 2.0438 3.1409\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.18585831578581177 [best r2] 0.9854492548083709\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.17592160106411017 [r2] 0.9869635429349729\n",
      "......Outputs 5.0193 5.7218 2.8910 5.7218 5.4552 2.8452 6.7043 5.2494 2.0446 3.1423\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17592160106411017 [best r2] 0.9869635429349729\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.16679781381417075 [r2] 0.9882806925168021\n",
      "......Outputs 5.0239 5.7388 2.8980 5.7388 5.4347 2.8640 6.7070 5.2396 2.0539 3.1449\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16679781381417075 [best r2] 0.9882806925168021\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.15843130436797043 [r2] 0.9894268781703168\n",
      "......Outputs 5.0208 5.7334 2.9004 5.7334 5.4217 2.8669 6.7055 5.2207 2.0619 3.1450\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15843130436797043 [best r2] 0.9894268781703168\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.1507221392770364 [r2] 0.9904308064208827\n",
      "......Outputs 5.0229 5.7274 2.9069 5.7274 5.4171 2.8690 6.7136 5.2034 2.0686 3.1496\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1507221392770364 [best r2] 0.9904308064208827\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.14368199455662142 [r2] 0.9913038717287971\n",
      "......Outputs 5.0302 5.7309 2.9163 5.7309 5.4151 2.8741 6.7195 5.1905 2.0771 3.1552\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14368199455662142 [best r2] 0.9913038717287971\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.13701980880418038 [r2] 0.9920916122407931\n",
      "......Outputs 5.0299 5.7302 2.9183 5.7302 5.4045 2.8662 6.7122 5.1764 2.0833 3.1515\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13701980880418038 [best r2] 0.9920916122407931\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.13077545945457525 [r2] 0.9927959992410571\n",
      "......Outputs 5.0287 5.7348 2.9220 5.7348 5.3964 2.8610 6.7118 5.1674 2.0878 3.1515\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13077545945457525 [best r2] 0.9927959992410571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.12492317200126112 [r2] 0.993426339860534\n",
      "......Outputs 5.0295 5.7349 2.9282 5.7349 5.3966 2.8679 6.7137 5.1607 2.0943 3.1547\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12492317200126112 [best r2] 0.993426339860534\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.11935246036327472 [r2] 0.9939995476260416\n",
      "......Outputs 5.0265 5.7356 2.9319 5.7356 5.3960 2.8738 6.7063 5.1554 2.1000 3.1536\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11935246036327472 [best r2] 0.9939995476260416\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.11396668879361449 [r2] 0.9945288691501765\n",
      "......Outputs 5.0247 5.7284 2.9347 5.7284 5.3875 2.8690 6.7016 5.1530 2.1057 3.1498\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11396668879361449 [best r2] 0.9945288691501765\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.10899873405341555 [r2] 0.9949954601850939\n",
      "......Outputs 5.0318 5.7372 2.9369 5.7372 5.3826 2.8688 6.7086 5.1514 2.1083 3.1502\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10899873405341555 [best r2] 0.9949954601850939\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.1043926063652239 [r2] 0.9954094922774535\n",
      "......Outputs 5.0395 5.7435 2.9405 5.7435 5.3813 2.8716 6.7132 5.1531 2.1125 3.1515\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1043926063652239 [best r2] 0.9954094922774535\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.09991273458035088 [r2] 0.9957950297052337\n",
      "......Outputs 5.0363 5.7328 2.9421 5.7328 5.3803 2.8669 6.7092 5.1476 2.1127 3.1461\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09991273458035088 [best r2] 0.9957950297052337\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.09552960946964348 [r2] 0.9961558772634188\n",
      "......Outputs 5.0265 5.7243 2.9391 5.7243 5.3640 2.8637 6.7001 5.1437 2.1105 3.1415\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09552960946964348 [best r2] 0.9961558772634188\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.09139415294159242 [r2] 0.9964814958709027\n",
      "......Outputs 5.0350 5.7424 2.9457 5.7424 5.3635 2.8748 6.7106 5.1485 2.1132 3.1432\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09139415294159242 [best r2] 0.9964814958709027\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.08824978306874373 [r2] 0.9967194358555942\n",
      "......Outputs 5.0535 5.7552 2.9607 5.7552 5.3875 2.8845 6.7237 5.1604 2.1253 3.1472\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08824978306874373 [best r2] 0.9967194358555942\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.08465307390083736 [r2] 0.9969813920859921\n",
      "......Outputs 5.0453 5.7232 2.9628 5.7232 5.3826 2.8582 6.6991 5.1535 2.1235 3.1405\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08465307390083736 [best r2] 0.9969813920859921\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.08282185366802321 [r2] 0.9971105769456202\n",
      "......Outputs 5.0004 5.7008 2.9463 5.7008 5.3371 2.8353 6.6688 5.1305 2.1074 3.1295\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08282185366802321 [best r2] 0.9971105769456202\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.08140574394906769 [r2] 0.9972085404436293\n",
      "......Outputs 5.0204 5.7497 2.9494 5.7497 5.3411 2.8763 6.7026 5.1395 2.1089 3.1342\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.08878682915814413 [r2] 0.9966793864670602\n",
      "......Outputs 5.1053 5.8216 2.9960 5.8216 5.4340 2.9346 6.7730 5.1936 2.1458 3.1558\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.09409966580538039 [r2] 0.996270098157137\n",
      "......Outputs 5.0849 5.7163 3.0015 5.7163 5.4422 2.8583 6.7118 5.1827 2.1540 3.1425\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.1113129219048006 [r2] 0.9947806982417893\n",
      "......Outputs 4.9206 5.5772 2.9184 5.5772 5.2793 2.7318 6.5711 5.0806 2.0822 3.0936\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.1272784806648107 [r2] 0.9931761227534796\n",
      "......Outputs 4.9345 5.7449 2.8993 5.7449 5.2453 2.8554 6.6552 5.0957 2.0620 3.1108\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.16401368909161876 [r2] 0.9886686557142847\n",
      "......Outputs 5.2348 6.0247 3.0526 6.0247 5.5185 3.1056 6.9435 5.2868 2.1731 3.2034\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.16932538486238294 [r2] 0.9879228243234617\n",
      "......Outputs 5.1935 5.6866 3.0853 5.6866 5.5420 2.8866 6.7416 5.2236 2.1987 3.1397\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.20046901118691454 [r2] 0.9830716068239304\n",
      "......Outputs 4.7470 5.3444 2.8545 5.3444 5.2086 2.5395 6.3683 4.9415 2.0565 3.0014\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.1747733736287286 [r2] 0.9871331635771503\n",
      "......Outputs 4.8780 5.8382 2.8844 5.8382 5.2886 2.8141 6.6864 5.1010 2.0518 3.1576\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.19850728645584792 [r2] 0.9834012973017636\n",
      "......Outputs 5.3000 6.1135 3.0829 6.1135 5.6012 3.1801 7.0868 5.3806 2.1826 3.2683\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.14592525521673624 [r2] 0.9910302123035463\n",
      "......Outputs 5.0725 5.4724 2.9976 5.4724 5.3805 2.8098 6.5725 5.1309 2.1382 3.0213\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.14717321394392055 [r2] 0.9908761362970223\n",
      "......Outputs 4.8408 5.5579 2.8729 5.5579 5.2644 2.6403 6.4330 4.9899 2.0724 2.9856\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.11782726387087862 [r2] 0.9941519264433549\n",
      "......Outputs 5.0881 6.0554 2.9866 6.0554 5.5029 2.9446 6.8788 5.2246 2.1402 3.2422\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.09816425746108912 [r2] 0.9959409162460386\n",
      "......Outputs 5.1336 5.7049 3.0004 5.7049 5.4298 2.9698 6.7915 5.2119 2.1332 3.1997\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.0867151504097308 [r2] 0.9968325395245513\n",
      "......Outputs 4.9622 5.5061 2.9147 5.5061 5.2945 2.7754 6.5479 5.0925 2.0744 3.0163\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08140574394906769 [best r2] 0.9972085404436293\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.06790388537142995 [r2] 0.9980577260277141\n",
      "......Outputs 5.0278 5.8397 2.9632 5.8397 5.3863 2.8503 6.6992 5.1701 2.1105 3.0980\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06790388537142995 [best r2] 0.9980577260277141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.06945237300948703 [r2] 0.9979681323352911\n",
      "......Outputs 5.1129 5.8481 2.9995 5.8481 5.4457 2.9368 6.7978 5.1972 2.1405 3.1886\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06790388537142995 [best r2] 0.9980577260277141\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.056471738216607184 [r2] 0.9986566674066664\n",
      "......Outputs 5.0305 5.6278 2.9479 5.6278 5.3705 2.8470 6.6703 5.1289 2.1066 3.1118\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.056471738216607184 [best r2] 0.9986566674066664\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.05485249378654779 [r2] 0.9987325991481423\n",
      "......Outputs 5.0049 5.7034 2.9475 5.7034 5.3479 2.8276 6.6555 5.1449 2.1002 3.0903\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05485249378654779 [best r2] 0.9987325991481423\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.0535341345515509 [r2] 0.9987927900309652\n",
      "......Outputs 5.0728 5.8224 2.9854 5.8224 5.3938 2.8955 6.7457 5.1882 2.1251 3.1358\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0535341345515509 [best r2] 0.9987927900309652\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.051569121524269894 [r2] 0.99887978674425\n",
      "......Outputs 5.0734 5.7301 2.9785 5.7301 5.3974 2.8869 6.7276 5.1635 2.1246 3.1305\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.051569121524269894 [best r2] 0.99887978674425\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.049674454177942286 [r2] 0.9989605886653239\n",
      "......Outputs 5.0201 5.6757 2.9541 5.6757 5.3606 2.8384 6.6672 5.1362 2.1084 3.1030\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.049674454177942286 [best r2] 0.9989605886653239\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.04807578413343409 [r2] 0.9990264147316553\n",
      "......Outputs 5.0297 5.7568 2.9620 5.7568 5.3690 2.8581 6.6988 5.1571 2.1124 3.1162\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04807578413343409 [best r2] 0.9990264147316553\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.05018232681924224 [r2] 0.998939226089305\n",
      "......Outputs 5.0733 5.7788 2.9816 5.7788 5.3992 2.8965 6.7406 5.1771 2.1257 3.1341\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04807578413343409 [best r2] 0.9990264147316553\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.04795785225782042 [r2] 0.9990311853625974\n",
      "......Outputs 5.0576 5.7118 2.9735 5.7118 5.3829 2.8710 6.7012 5.1574 2.1206 3.1150\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04795785225782042 [best r2] 0.9990311853625974\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.04776442433547961 [r2] 0.999038984623108\n",
      "......Outputs 5.0212 5.7049 2.9565 5.7049 5.3564 2.8391 6.6692 5.1390 2.1086 3.1008\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04776442433547961 [best r2] 0.999038984623108\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.0470499863146051 [r2] 0.9990675184583666\n",
      "......Outputs 5.0431 5.7606 2.9664 5.7606 5.3765 2.8693 6.7112 5.1594 2.1148 3.1220\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.049700508878790996 [r2] 0.998959498018065\n",
      "......Outputs 5.0769 5.7639 2.9829 5.7639 5.4025 2.8954 6.7399 5.1782 2.1266 3.1313\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.047515101695912276 [r2] 0.9990489911310652\n",
      "......Outputs 5.0508 5.7114 2.9727 5.7114 5.3783 2.8612 6.6942 5.1565 2.1202 3.1081\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.04835231987339044 [r2] 0.9990151822391691\n",
      "......Outputs 5.0163 5.7065 2.9551 5.7065 5.3502 2.8368 6.6659 5.1351 2.1070 3.1005\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.048044008127634236 [r2] 0.9990277013015074\n",
      "......Outputs 5.0458 5.7628 2.9659 5.7628 5.3773 2.8751 6.7170 5.1606 2.1138 3.1237\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.05262152158989473 [r2] 0.9988335985667551\n",
      "......Outputs 5.0873 5.7727 2.9881 5.7727 5.4107 2.9021 6.7496 5.1882 2.1303 3.1308\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.05012377977608483 [r2] 0.9989416998266236\n",
      "......Outputs 5.0529 5.7065 2.9759 5.7065 5.3795 2.8572 6.6885 5.1576 2.1233 3.1042\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.05386958284950203 [r2] 0.9987776137224477\n",
      "......Outputs 4.9994 5.6864 2.9467 5.6864 5.3354 2.8233 6.6459 5.1214 2.1019 3.0943\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.053477018080045 [r2] 0.9987953646421578\n",
      "......Outputs 5.0387 5.7678 2.9588 5.7678 5.3680 2.8774 6.7173 5.1578 2.1093 3.1238\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.06470315563002058 [r2] 0.9982365133955222\n",
      "......Outputs 5.1151 5.8071 2.9998 5.8071 5.4328 2.9267 6.7822 5.2106 2.1394 3.1383\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.062349885694554824 [r2] 0.9983624575609062\n",
      "......Outputs 5.0714 5.7021 2.9884 5.7021 5.3968 2.8607 6.6933 5.1660 2.1336 3.1033\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.07516902410778238 [r2] 0.9976198788427574\n",
      "......Outputs 4.9596 5.6321 2.9280 5.6321 5.3016 2.7851 6.5946 5.0865 2.0888 3.0793\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.07694163111683526 [r2] 0.9975063010720426\n",
      "......Outputs 4.9999 5.7624 2.9331 5.7624 5.3309 2.8633 6.6959 5.1357 2.0891 3.1184\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.10341030856343926 [r2] 0.9954954759613834\n",
      "......Outputs 5.1704 5.8961 3.0222 5.8961 5.4812 2.9848 6.8626 5.2621 2.1543 3.1600\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.10936844392630564 [r2] 0.9949614530809959\n",
      "......Outputs 5.1418 5.7265 3.0332 5.7265 5.4663 2.8948 6.7405 5.2128 2.1681 3.1135\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.13738757403425672 [r2] 0.9920491025741185\n",
      "......Outputs 4.8815 5.4962 2.9006 5.4962 5.2439 2.6992 6.4769 5.0166 2.0732 3.0444\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.15876700876240976 [r2] 0.9893820233484567\n",
      "......Outputs 4.8699 5.6796 2.8557 5.6796 5.2087 2.7879 6.5843 5.0422 2.0344 3.0903\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.19646339947174335 [r2] 0.9837413474614054\n",
      "......Outputs 5.2399 6.0870 3.0383 6.0870 5.5424 3.0950 7.0171 5.3427 2.1653 3.2087\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.23125619843196377 [r2] 0.9774727590079835\n",
      "......Outputs 5.3376 5.8591 3.1420 5.8591 5.6503 3.0346 6.9148 5.3500 2.2539 3.1560\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.2531895323019975 [r2] 0.9729969553864217\n",
      "......Outputs 4.7817 5.2728 2.8842 5.2728 5.2024 2.5673 6.2821 4.9188 2.0786 2.9722\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.2978939957641482 [r2] 0.9626195314719013\n",
      "......Outputs 4.6332 5.4620 2.7441 5.4620 5.0369 2.6072 6.3338 4.8634 1.9612 3.0378\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.3005548978395219 [r2] 0.9619487559566706\n",
      "......Outputs 5.2479 6.3042 3.0382 6.3042 5.6002 3.1806 7.1935 5.4212 2.1326 3.3157\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.30337938854131563 [r2] 0.9612302157472713\n",
      "......Outputs 5.4520 5.9642 3.1894 5.9642 5.7293 3.1757 7.0883 5.4502 2.2632 3.1962\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.27570582243613045 [r2] 0.9679806053950729\n",
      "......Outputs 4.7835 5.1757 2.8748 5.1757 5.1644 2.5396 6.1637 4.8645 2.0776 2.8573\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.24132046622821265 [r2] 0.9754693219722367\n",
      "......Outputs 4.7303 5.6094 2.8082 5.6094 5.2241 2.6054 6.3956 4.9479 2.0380 3.0322\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.21700992634465321 [r2] 0.9801627968916742\n",
      "......Outputs 5.2364 6.2332 3.0430 6.2332 5.6684 3.1048 7.1811 5.4171 2.1680 3.3756\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.14936565511176278 [r2] 0.9906022749019985\n",
      "......Outputs 5.1664 5.6143 3.0125 5.6143 5.4073 2.9799 6.7693 5.2302 2.1357 3.1528\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.1401384065720108 [r2] 0.9917275225677963\n",
      "......Outputs 4.8952 5.4311 2.8763 5.4311 5.2147 2.6836 6.3868 5.0145 2.0549 2.9204\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.0942206077843658 [r2] 0.9962605042527041\n",
      "......Outputs 5.0433 5.9269 2.9607 5.9269 5.4499 2.8223 6.7306 5.2043 2.1177 3.1035\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0470499863146051 [best r2] 0.9990675184583666\n",
      "..Plotting of gradients skipped\n",
      "Finished model size search. The optimal capacity is 1\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn,\n",
    "                 device, do_all_tests=True, trainer=trainer) # do_all_tests=True test all together\n",
    "ds.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d41d4a34214ac41d40982b9575bce1bf3fd1035657656ceea5e87f8f277f244b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
