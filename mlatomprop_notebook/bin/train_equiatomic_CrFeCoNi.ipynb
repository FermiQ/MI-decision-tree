{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network on Equiatomic CrFeCoNi\n",
    "<b> nanoHUB tools by: </b>  <i>Mackinzie S. Farnell, Zachary D. McClure</i> and <i>Alejandro Strachan</i>, Materials Engineering, Purdue University <br>\n",
    "\n",
    "Neural networks are trained to predict relaxed vacancy formation energy (vfe), cohesive energy, pressure, and volume, with unrelaxed bispectrum coefficients and Pymatgen descriptors as inputs. A separate model is trained for each output property, and all models are trained on a structure with 25% Cr, 25% Fe, 25% Co, and 25% Ni. The models are built and trained using the Keras library and the results indicate that the models have good predictivie abilities.\n",
    "\n",
    "Overview\n",
    "1. Load Bispectrum Coefficients and Output Properties\n",
    "2. Add Pymatgen Descriptors\n",
    "3. Split Data into Testing/Training Data\n",
    "4. Normalize Data\n",
    "5. Train Neural Network\n",
    "6. Evaluate Neural Network\n",
    "7. Analyze Effects of Central Atom Descriptors as Inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries we will need\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as ke\n",
    "from keras.models import load_model\n",
    "\n",
    "import json as js\n",
    "import numpy as np\n",
    "import pymatgen as pymat\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as p\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Bispectrum Coefficients and Output Properties\n",
    "\n",
    "The unrelaxed bispectrum coefficients and relaxed vacancy formation energy, cohesive energy, and local atomic pressures and volumes are obtained from a JSON file and stored in a Python dictionary. The bispectrum coefficients are local geometric descriptors based on each atom's 12 nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify properties and filename here\n",
    "properties = [\"Relaxed_VFE\", \"Cohesive_Energy\", \"Pressure\", \"Volume\"]\n",
    "filename = '../data/25_25_25_25.json'\n",
    "input_prop_key = 'Unrelaxed_Bispectrum_Coefficients'\n",
    "\n",
    "properties_dictionaries = {}\n",
    "\n",
    "for output_prop_key in properties:\n",
    "    # open file and load data into data variable\n",
    "    with open(filename, 'r') as f:\n",
    "        data = js.load(f)\n",
    "\n",
    "    # get relevant information from data variable\n",
    "    elements = data['element']\n",
    "    output_properties = data[output_prop_key]\n",
    "    input_properties = data[input_prop_key]\n",
    "\n",
    "    # store input and output properties for specific element being searched for\n",
    "    elements_array = np.array([]) \n",
    "    output_properties_array = np.array([])\n",
    "    input_properties_array = np.array([])\n",
    "\n",
    "    # create counters to track number of each element\n",
    "    num_Cr = 0\n",
    "    num_Fe = 0\n",
    "    num_Co = 0\n",
    "    num_Ni = 0\n",
    "    num_Cu = 0\n",
    "    \n",
    "    # iterate through elements and get input and output properties for desired element\n",
    "    for i, val in enumerate(elements):\n",
    "        output_properties_array = np.append(output_properties_array, output_properties[i])\n",
    "        input_properties_array = np.append(input_properties_array, np.asarray(input_properties[i])) \n",
    "        if (val == 'Cr'):\n",
    "            elements_array = np.append(elements_array, 24)\n",
    "            num_Cr = num_Cr + 1\n",
    "        elif (val == 'Fe'):\n",
    "            elements_array = np.append(elements_array, 26)\n",
    "            num_Fe = num_Fe + 1\n",
    "        elif (val == 'Co'):\n",
    "            elements_array = np.append(elements_array, 27)\n",
    "            num_Co = num_Co + 1\n",
    "        elif (val == 'Ni'):\n",
    "            elements_array = np.append(elements_array, 28)\n",
    "            num_Ni = num_Ni + 1\n",
    "        elif (val == 'Cu'):\n",
    "            elements_array = np.append(elements_array, 29)\n",
    "            num_Cu = num_Cu + 1\n",
    " \n",
    "    # reshape input_properties_element (this should only happen if input property is Bispectrum coefficients)\n",
    "    num_rows = int (input_properties_array.shape[0]/55)\n",
    "    input_properties_array = np.reshape(input_properties_array, (num_rows, 55))\n",
    "    \n",
    "    # element number is included as input to model\n",
    "    elements_array = elements_array[np.newaxis].T\n",
    "    input_properties_array = np.append(input_properties_array, elements_array, 1)\n",
    "\n",
    "    input_properties_array = input_properties_array.astype(np.float)\n",
    "\n",
    "    num_elements = np.array([num_Co, num_Cr, num_Fe, num_Ni])\n",
    "\n",
    "    # if/elif stmts are used to set proper values in dictionary for each property\n",
    "    if (output_prop_key == 'Relaxed_VFE'):\n",
    "        min_val = 0.9\n",
    "        max_val = 2.3\n",
    "        display = 'Relaxed VFE (eV/atom)'\n",
    "        units = '(eV/atom)'\n",
    "    elif (output_prop_key == 'Cohesive_Energy'):\n",
    "        min_val = -5\n",
    "        max_val = -4\n",
    "        display = 'Cohesive Energy (eV/atom)'\n",
    "        units = '(eV/atom)'\n",
    "    elif (output_prop_key == 'Pressure'):\n",
    "        min_val = -7\n",
    "        max_val = 7\n",
    "        display = 'Pressure (GPa)'\n",
    "        units = '(GPa)'\n",
    "    elif (output_prop_key == 'Volume'):\n",
    "        min_val = 10.9\n",
    "        max_val = 11.3\n",
    "        display = 'Volume (\\u212B\\u00b3)'\n",
    "        units = '(\\u212B\\u00b3)'\n",
    "    \n",
    "    # create properties_dictionary\n",
    "    properties_dictionaries[output_prop_key] = {\n",
    "        'inputs': input_properties_array,\n",
    "        'outputs': output_properties_array,\n",
    "        'length': output_properties_array.shape[0],\n",
    "        'elements': elements_array,\n",
    "        'num_element': num_elements,\n",
    "        'min': min_val,\n",
    "        'max': max_val,\n",
    "        'display': display,\n",
    "        'units': units\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we plot histograms to show the distribution of values for each output property, establishing that the values vary across the structure. In the histograms, the elements are separated by color with Cr shown in red, Fe in orange, Co in blue, and Ni in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# iterate through each of the properties in dictionary to make the histograms\n",
    "for key in properties_dictionaries:\n",
    "    Cr_outs = []\n",
    "    Fe_outs = []\n",
    "    Co_outs = []\n",
    "    Ni_outs = []\n",
    "\n",
    "    colors = []\n",
    "\n",
    "    # separate out properties by element\n",
    "    for i, val in enumerate(properties_dictionaries[key]['elements']):\n",
    "        if (val == 24):\n",
    "            colors.append('red')\n",
    "            Cr_outs.append(properties_dictionaries[key]['outputs'][i])\n",
    "        elif (val == 26):\n",
    "            colors.append('orange')\n",
    "            Fe_outs.append(properties_dictionaries[key]['outputs'][i])\n",
    "        elif (val == 27):\n",
    "            colors.append('blue')\n",
    "            Co_outs.append(properties_dictionaries[key]['outputs'][i])\n",
    "        elif (val == 28):\n",
    "            colors.append('green')\n",
    "            Ni_outs.append(properties_dictionaries[key]['outputs'][i])\n",
    "\n",
    "    # plot the figures\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x=Cr_outs, name = 'Cr', nbinsx=70, marker_color = 'red'))\n",
    "    fig.add_trace(go.Histogram(x=Fe_outs, name = 'Fe', nbinsx=70, marker_color = 'orange'))\n",
    "    fig.add_trace(go.Histogram(x=Co_outs, name = 'Co', nbinsx=70, marker_color = 'blue'))\n",
    "    fig.add_trace(go.Histogram(x=Ni_outs, name = 'Ni', nbinsx=70, marker_color = 'green'))\n",
    "    fig.update_layout(barmode='overlay')\n",
    "    fig.update_traces(opacity=0.5)\n",
    "    fig.update_layout(\n",
    "        xaxis_title=properties_dictionaries[key]['display'],\n",
    "        yaxis_title=\"Frequency\",\n",
    "        font=dict(\n",
    "            family=\"Times New Roman, monospace\",\n",
    "            size=24,\n",
    "            color=\"black\"\n",
    "        )\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add Pymatgen Descriptors\n",
    "\n",
    "Properties are queried from Pymatgen to use as model inputs along with the bispectrum coefficients. We found that querying additional descriptors helped minimize the error in model predictions, which is further discussed in Section 7: Analyze Effects of Central Atom Descriptors as Inputs.\n",
    "\n",
    "For all properties, the properties are queried for just the central atom (i.e. the atom we are predicting on). We add central atom descriptors because the output property values vary based on which atom we are predicting on and the central atom descriptors give the neural network a way to distinguish between different atoms. The queried properties are:\n",
    "    - atomic_radius_calculated\n",
    "    - atomic_radius \n",
    "    - atomic_mass\n",
    "    - poissons_ratio\n",
    "    - electrical_resistivity\n",
    "    - thermal_conductivity\n",
    "    - brinell_hardness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare function to query property from pymatgen for a given element\n",
    "def get_property(element, property):\n",
    "    element_object = pymat.Element(element)\n",
    "    element_prop = getattr(element_object, property)\n",
    "    return element_prop\n",
    "\n",
    "# list of properties to add central atom descriptors for\n",
    "properties = ['atomic_radius_calculated', 'atomic_radius', 'atomic_mass', \n",
    "              'poissons_ratio', 'electrical_resistivity', 'thermal_conductivity', \n",
    "              'brinell_hardness']\n",
    "\n",
    "# iterate through all output properties\n",
    "for key in properties_dictionaries:    \n",
    "    # iterate through all properties to add\n",
    "    for add_property in properties:\n",
    "        atom_properties = []\n",
    "        elements = properties_dictionaries[key]['elements']\n",
    "        # determine which element to get property for\n",
    "        for i in elements:\n",
    "            if (i == 24):\n",
    "                ele = 'Cr'\n",
    "            elif (i == 26):\n",
    "                ele = 'Fe'\n",
    "            elif (i == 27):\n",
    "                ele = 'Co'\n",
    "            elif (i == 28):\n",
    "                ele = 'Ni'\n",
    "            elif (i == 29):\n",
    "                ele = 'Cu'\n",
    "            prop = get_property(ele, add_property)\n",
    "            atom_properties.append(float (prop))\n",
    "\n",
    "        # add property to array of inputs\n",
    "        atom_properties = np.asarray(atom_properties) \n",
    "        atom_properties = atom_properties[np.newaxis].T\n",
    "        properties_dictionaries[key]['inputs'] = np.append(properties_dictionaries[key]['inputs'], \n",
    "                                                           atom_properties, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tested the effects of adding nearest neighbor descriptors to describe the local environment of each atom. We found that these additional descriptors did not improve the model performance.\n",
    "\n",
    "The central atom and nearest neighbor descriptors were incorporated into one feature using a rule of mixtures equation where $ X_{i} $ is the property value for a nearest neighbor and $ X_{center} $ is the property value for the central atom. This emphasizes the central atom, while still including information about the 12 neighboring atoms.\n",
    "\n",
    "$$ \\ Input Feature = \\sum \\limits _{i=1} ^{12} ({X_{i}} + X_{center})\\ $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable determines if nearest neighbor descriptors are included\n",
    "add_nearest_neighbors = False\n",
    "\n",
    "# function to obtain identity of 12 nearest neighbor atoms\n",
    "def get_composition(c):\n",
    "    num_Cr = 0\n",
    "    num_Fe = 0\n",
    "    num_Co = 0\n",
    "    num_Ni = 0\n",
    "    num_Cu = 0\n",
    "    for i in c[3:15]:\n",
    "        if (i == '2.0'):\n",
    "            num_Cr = num_Cr + 1\n",
    "        elif (i == '4.0'):\n",
    "            num_Fe = num_Fe + 1\n",
    "        elif (i == '1.0'):\n",
    "            num_Co = num_Co + 1\n",
    "        elif (i == '5.0'):\n",
    "            num_Ni = num_Ni + 1\n",
    "        elif (i == '3.0'):\n",
    "            num_Cu = num_Cu + 1\n",
    "    composition_dictionary = {'Cr': num_Cr, 'Fe': num_Fe, 'Co': num_Co, 'Ni': num_Ni, 'Cu': num_Cu}\n",
    "    return composition_dictionary\n",
    "\n",
    "# list of properties to query\n",
    "properties_add = [\"atomic_radius_calculated\", \"atomic_radius\", \"atomic_mass\", \n",
    "              \"poissons_ratio\", \"electrical_resistivity\", \"thermal_conductivity\", \n",
    "              \"brinell_hardness\", ]\n",
    "\n",
    "# iterate through all properties\n",
    "for key in properties_dictionaries:\n",
    "    \n",
    "    if not add_nearest_neighbors:\n",
    "        continue\n",
    "        \n",
    "    # iterate through properties to add\n",
    "    for add_property in properties_add:\n",
    "        elements_dictionary = {\n",
    "            'Cr': '2.0',\n",
    "            'Fe': '4.0',\n",
    "            'Co': '1.0',\n",
    "            'Ni': '5.0',\n",
    "            'Cu': '3.0'\n",
    "        }\n",
    "\n",
    "        properties = []\n",
    "        elements =  ['Cr', 'Fe', 'Co', 'Ni', 'Cu']\n",
    "        \n",
    "        # iterate through all elements\n",
    "        for ele in elements:\n",
    "            with open('../data/5000Atom_ID_Type_Neighbors_25_25_25_25.csv') as csvfile:\n",
    "                readCSV = csv.reader(csvfile, delimiter=',')\n",
    "                i = 0\n",
    "                for row in readCSV:\n",
    "                    if (i != 0):\n",
    "                        if (elements_dictionary[ele] != row[2]):\n",
    "                            continue\n",
    "                            \n",
    "                        # gets dictionary that says how many of each atom are in 12 nearest neighbors\n",
    "                        comp = get_composition(row)\n",
    "                        elements_2 = ['Cr', 'Fe', 'Co', 'Ni', 'Cu']\n",
    "                        \n",
    "                        # query properties for 12 nearest neighbors + central atom\n",
    "                        sum_prop = 0\n",
    "                        for element in elements_2:\n",
    "                            prop = get_property(element, add_property) + get_property(ele, add_property)\n",
    "                            sum_prop = sum_prop + prop * comp[element]\n",
    "                        properties.append(float (sum_prop))\n",
    "                    i = i + 1\n",
    "\n",
    "        # add feature to list of inputs\n",
    "        properties = np.asarray(properties)\n",
    "        properties = properties[np.newaxis].T\n",
    "        properties_dictionaries[key]['inputs'] = np.append(properties_dictionaries[key]['inputs'], properties, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Data into Testing/Training Data\n",
    "The data is split into two groups: training and testing. Training data is used to train the neural network, so that it can determine how to predict the output properties when given bispectrum coefficients and Pymatgen descriptors as inputs. The testing data is used to evaluate the results of the trained model. We use 80% of the data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable determines what percent of data is used for training, must be less than 1\n",
    "train_percent = 0.8\n",
    "\n",
    "# iterate through properties\n",
    "for key in properties_dictionaries:\n",
    "    index_split_at = int (train_percent * properties_dictionaries[key]['length'])\n",
    "  \n",
    "    # shuffle data\n",
    "    properties_dictionaries[key]['inputs'], properties_dictionaries[key]['outputs'], \\\n",
    "            properties_dictionaries[key]['elements'] = shuffle(properties_dictionaries[key]['inputs'], \\\n",
    "                                                         properties_dictionaries[key]['outputs'], \\\n",
    "                                                         properties_dictionaries[key]['elements'], \\\n",
    "                                                         random_state=0)\n",
    "\n",
    "    # split data\n",
    "    train_inputs, test_inputs = np.split(properties_dictionaries[key]['inputs'], [index_split_at])\n",
    "    train_outputs, test_outputs = np.split(properties_dictionaries[key]['outputs'], [index_split_at])\n",
    "    train_elements, test_elements = np.split(properties_dictionaries[key]['elements'], [index_split_at])\n",
    "\n",
    "    # update properties dictionary\n",
    "    properties_dictionaries[key]['inputs_train'] = train_inputs\n",
    "    properties_dictionaries[key]['inputs_test'] = test_inputs\n",
    "    properties_dictionaries[key]['outputs_train'] = train_outputs\n",
    "    properties_dictionaries[key]['outputs_test'] = test_outputs\n",
    "    properties_dictionaries[key]['elements_train'] = train_elements\n",
    "    properties_dictionaries[key]['elements_test'] = test_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalize Data\n",
    "We normalize the inputs and outputs to the model using mean and standard deviation. We normalize the inputs because each bispectrum coefficient has a different range of values and this difference could lead the model to over-emphasize certain bispectrum coefficients. We normalize the outputs because it improves the model's predictions. Each data point (x) is normalized using the mean ($ \\mu $) and standard deviation ($ \\sigma $) of the set of values. The testing data is normalized with the mean and standard deviation of the training data so that the model predicts on points from the distribution it was trained on. \n",
    "\n",
    "$$ x_{new} = \\frac{x - µ}{σ}\\ $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to normalize data\n",
    "def normalize(test_train_properties, key, means, stdevs):\n",
    "    dims = test_train_properties[key].shape\n",
    "\n",
    "    for j in range(0, dims[0]):\n",
    "        test_train_properties[key][j] = (test_train_properties[key][j] - means)/stdevs\n",
    "  \n",
    "    test_train_properties[key] = np.nan_to_num(test_train_properties[key])\n",
    "    \n",
    "    return test_train_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through properties\n",
    "for key in properties_dictionaries:\n",
    "    # normalize inputs\n",
    "    means_ins = np.mean(properties_dictionaries[key][\"inputs_train\"], axis=0)\n",
    "    stdevs_ins = np.std(properties_dictionaries[key][\"inputs_train\"], axis=0)\n",
    "    test_train_properties = normalize(properties_dictionaries[key], \"inputs_train\", means_ins, stdevs_ins)\n",
    "    test_train_properties = normalize(properties_dictionaries[key], \"inputs_test\", means_ins, stdevs_ins)\n",
    "\n",
    "    # normalize outputs\n",
    "    means_outs = np.mean(properties_dictionaries[key][\"outputs_train\"], axis=0)\n",
    "    stdevs_outs = np.std(properties_dictionaries[key][\"outputs_train\"], axis=0)\n",
    "    test_train_properties = normalize(properties_dictionaries[key], \"outputs_train\", means_outs, stdevs_outs)\n",
    "    test_train_properties = normalize(properties_dictionaries[key], \"outputs_test\", means_outs, stdevs_outs)\n",
    "\n",
    "    # create dictionary to store stats data\n",
    "    stats_dict = {\"means_ins\": means_ins, \"stdevs_ins\": stdevs_ins, \n",
    "                  \"means_outs\": means_outs, \"stdevs_outs\": stdevs_outs}\n",
    "\n",
    "    # add stats data for property to dictionary\n",
    "    properties_dictionaries[key]['stats'] = stats_dict\n",
    "\n",
    "    stats_dict_list = stats_dict \n",
    "\n",
    "    # save states data for each property\n",
    "    for x in stats_dict_list:\n",
    "        stats_dict_list[x] = stats_dict_list[x].tolist()\n",
    "    json = js.dumps(stats_dict)\n",
    "    f = open(\"{}_stats.json\".format(key), \"w\")\n",
    "    f.write(json)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Neural Network\n",
    "A neural network is a machine learning model that was inspired by how we think the human brain works. Neural networks consist of a collection of neurons that have different weights and biases and are used to predict desired outputs. Here, we set up the architecture for the neural network. The network has a dense layer with 512 neurons followed by a dropout layer with dropout of 0.2. The activation function used is elu and the adagrad optimizer is used. A simple schematic of the network is shown below.\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=neural_net.png width=\"700px\" height='300px' align=\"center\" /> \n",
    "</p>\n",
    "\n",
    "We tested a few neural network architectures and activation functions to determine how the architecture/activation function affected the model performance and minimize the error of the predictions. The errors are plotted for each property below for a range of different architectures. The architectures tested all had a dropout layer and included:\n",
    "\n",
    "    - 1 layer with 64 neurons\n",
    "    - 1 layer with 256 neurons\n",
    "    - 1 layer with 512 neurons\n",
    "    - 3 layers with 256 neurons, 512 neurons, and 256 neurons\n",
    "    - 5 layers with 128 neurons, 256 neurons, 512 neurons, 256 neurons, and 128 neurons\n",
    "    \n",
    "For relaxed vacancy formation energy, cohesive energy, and volume, there is no affect of architecture on error. For pressure, increasing the size and number of layers gives small improvements in the error. We ultimately chose to use 1 layer with 512 neurons because this was improved on the smaller one layer networks for the pressure predictions while keeping the network architecture simple.\n",
    "\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=model-architectures-test.jpg width=\"700px\" height='300px' align=\"center\" /> \n",
    "</p>\n",
    "\n",
    "For the one layer network, we tested a few different activation functions. Say what an activation function is. The activation functions we tested were:\n",
    "\n",
    "    - elu\n",
    "    - relu\n",
    "    - tanh\n",
    "    - sigmoid\n",
    "    \n",
    "The results are shown below. Again, there is not a big difference between the results for most of the properties. We went with elu.\n",
    "\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=model-activation-fxn-test.jpg width=\"700px\" height='300px' align=\"center\" /> \n",
    "</p>\n",
    "\n",
    "Lastly, we looked at the effect of the dropout layer and found that whether or not there was a dropout layer had minimal effect on the results, so we decided to include the dropout layer in our architecture.\n",
    "\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=model-dropout-layer-test.jpg width=\"500px\" height='200px' align=\"center\" /> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds to use when initializing layers\n",
    "seeds = [0,1]\n",
    "\n",
    "# initializers for each layer\n",
    "initializer1 = ke.initializers.glorot_normal(seed=seeds[0])\n",
    "bias_initial1 = ke.initializers.Zeros()\n",
    "    \n",
    "initializer2 = ke.initializers.glorot_normal(seed=seeds[1])\n",
    "bias_initial2 = ke.initializers.Zeros()  \n",
    "\n",
    "# build model - 1 input layer, 3 hidden layers, 1 output layer\n",
    "input_layer = ke.layers.Input(shape=(63, ))\n",
    "\n",
    "layer1 = ke.layers.Dense(512, activation = 'elu', kernel_initializer = initializer1, \n",
    "                         bias_initializer = bias_initial1)(input_layer)\n",
    "layer2 = ke.layers.Dropout(0.2)(layer1)\n",
    "output = ke.layers.Dense(1, activation = 'linear', kernel_initializer = initializer2, \n",
    "                         bias_initializer = bias_initial2)(layer2)\n",
    "\n",
    "model = ke.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = ke.optimizers.Adagrad(0.002)\n",
    "  \n",
    "# compile model for training\n",
    "model.compile(loss = 'mse', optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "# print summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the neural network is trained to predict each of the output properties when bispectrum coefficients and central atom descriptors are input. We use 10% of the training data for validation. The validation data differs from the testing data in that it is used to evaluate and improve the model during training, while the test data will be used after training to evaluate model performance. \n",
    "\n",
    "During training, we also plot a loss graph, showing the mean absolute error for each training epoch. This graphs shows how the error on the training set and the validation set changes throughout training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through properties \n",
    "for key in properties_dictionaries:\n",
    "    saveid = key\n",
    "\n",
    "    # split data from dictionary into training and testing data\n",
    "    train_inputs = properties_dictionaries[key]['inputs_train']\n",
    "    train_outputs = properties_dictionaries[key]['outputs_train']\n",
    "    test_inputs = properties_dictionaries[key]['inputs_test']\n",
    "    test_outputs = properties_dictionaries[key]['outputs_test']\n",
    "    train_elements = properties_dictionaries[key]['elements_train']\n",
    "    test_elements = properties_dictionaries[key]['elements_test']\n",
    "    num_elements = properties_dictionaries[key]['num_element']\n",
    "  \n",
    "    # trains model\n",
    "    history = model.fit(train_inputs, train_outputs, batch_size=train_inputs.shape[0],\n",
    "                      epochs=5000, verbose = False, validation_split = 0.1, shuffle = False)\n",
    "    model.save('{}_trained_model.h5'.format(saveid))\n",
    "    \n",
    "    # saves model in file 'Property_trained_model.h5'\n",
    "    saved_model = ke.models.load_model('{}_trained_model.h5'.format(saveid))\n",
    "\n",
    "    # print loss and MAE of model \n",
    "    [loss, mae] = saved_model.evaluate(test_inputs, test_outputs, verbose = 0)\n",
    "    print(\"Loss: \", loss)\n",
    "    print(\"Mean Absolute Error: \", mae)\n",
    "\n",
    "    # plot mean absolute error as a function of training epoch\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error')\n",
    "    plt.title('Mean Abs Error versus Epoch - {}'.format(key))\n",
    "    plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),label='Loss on training set')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 6. Evaluate Neural Network\n",
    "Model is evaluated by plotting the predicted versus actual data and calculating the (mean absolute error) MAE and (mean squared error) MSE of the test predictions (equations are shown below). We divide the MAE and MSE by the range for each property so that we can compare the error for different output properties. In the predicted versus actual data plots for a well-trained model, all the points on the plot should fall along the x=y line, meaning the model predictions matched the data perfectly. For our models we see that the points are all clustered around the x=y line, which means the neural network has some predictive power.\n",
    "\n",
    "$$ MSE = \\frac{\\frac{1}{n}\\sum\\limits _{i=1} ^{n}(Y_{i}-\\hat{Y}_{i})^2}{max-min} $$\n",
    "\n",
    "\n",
    "$$ MAE = \\frac{\\frac{1}{n}\\sum\\limits _{i=1} ^{n}|Y_{i}-\\hat{Y}_{i}|}{max-min} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in properties_dictionaries:\n",
    "    # put model and stats_dict into variables\n",
    "    model = '{}_trained_model.h5'.format(key)\n",
    "    stats_dict = properties_dictionaries[key]['stats']\n",
    "\n",
    "    # separate out train/test inputs/outputs\n",
    "    train_inputs = properties_dictionaries[key]['inputs_train']\n",
    "    train_outputs = properties_dictionaries[key]['outputs_train']\n",
    "    test_inputs = properties_dictionaries[key]['inputs_test']\n",
    "    test_outputs = properties_dictionaries[key]['outputs_test']\n",
    "    train_elements = properties_dictionaries[key]['elements_train']\n",
    "    test_elements = properties_dictionaries[key]['elements_test']\n",
    "    num_elements = properties_dictionaries[key]['num_element']\n",
    "\n",
    "    saved_model = ke.models.load_model(model)\n",
    "\n",
    "    # undo normalization on output data for plotting\n",
    "    train_data = np.zeros((len(train_outputs),2))\n",
    "    train_data[:,0] = train_outputs * stats_dict[\"stdevs_outs\"] + stats_dict[\"means_outs\"]\n",
    "    test_data = np.zeros((len(test_outputs),2))\n",
    "    test_data[:,0] = test_outputs * stats_dict[\"stdevs_outs\"] + stats_dict[\"means_outs\"]\n",
    "    \n",
    "    # predict outputs of train/test data\n",
    "    predict_train_data = saved_model.predict(train_inputs)\n",
    "    predict_train_data = predict_train_data * stats_dict[\"stdevs_outs\"] + stats_dict[\"means_outs\"]\n",
    "    predict_test_data = saved_model.predict(test_inputs)\n",
    "    predict_test_data = predict_test_data * stats_dict[\"stdevs_outs\"] + stats_dict[\"means_outs\"]\n",
    "\n",
    "    # store predicted values in train/test_data arrays\n",
    "    for i in range(predict_train_data.shape[0]):\n",
    "        train_data[i,1] = predict_train_data[i]\n",
    "    for i in range(predict_test_data.shape[0]):\n",
    "        test_data[i,1] = predict_test_data[i]\n",
    "\n",
    "    element_ids = [24, 26, 27, 28]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=train_data[:,0], y=train_data[:,1],\n",
    "                mode='markers',\n",
    "                name='Training Dataset'))\n",
    "    fig.add_trace(go.Scatter(x=test_data[:,0], y=test_data[:,1],\n",
    "                mode='markers',\n",
    "                name='Test Dataset'))\n",
    "    x_lin = [-24, 24]\n",
    "    fig.add_trace(go.Scatter(x=x_lin, y=x_lin,\n",
    "                mode='lines',\n",
    "                name='lines'))\n",
    "    fig.update_xaxes(range=[properties_dictionaries[key]['min'], properties_dictionaries[key]['max']])\n",
    "    fig.update_yaxes(range=[properties_dictionaries[key]['min'], properties_dictionaries[key]['max']])\n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        xaxis_title=\"Molecular Mechanics {}\".format(properties_dictionaries[key]['units']),\n",
    "        yaxis_title=\"Neural Network {}\".format(properties_dictionaries[key]['units']),\n",
    "        title = \"{}\".format(key),\n",
    "        font=dict(\n",
    "            family=\"Times New Roman, monospace\",\n",
    "            size=24,\n",
    "            color= \"black\"\n",
    "        )    \n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    # calculate errors\n",
    "    test_mse = np.mean((test_data[:,1]-test_data[:,0])**2)\n",
    "    test_mae = np.mean(np.abs(test_data[:,1]-test_data[:,0]))\n",
    "    test_error = (test_data[:,1]-test_data[:,0])\n",
    "  \n",
    "    # find a normalized mse and mae\n",
    "    max = np.amax(test_data[:,0])\n",
    "    min = np.amin(test_data[:,0])\n",
    "    test_range = np.abs(max - min)\n",
    "    mse_norm = test_mse/test_range\n",
    "    mae_norm = test_mae/test_range\n",
    "\n",
    "    # print MSE/MAE\n",
    "    print(f'Test_MAE/range: {mae_norm:.5f}')\n",
    "    print(f'Test_MSE/range: {mse_norm:.5f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Effects of Central Atom Descriptors as Inputs\n",
    "\n",
    "We compare the results of using the following inputs for the model:\n",
    "- only bispectrum coefficients\n",
    "- bispectrum coefficients and central atom descriptors\n",
    "- bispectrum coefficients, central atom descriptors, and nearest neighbor descriptors.\n",
    "\n",
    "A plot that shows the error for each property with these three sets of inputs is shown below. For relaxed vacancy formation energy, pressure, and volume, using the central atom descriptors leads to small improvements in the MAE. For the cohesive energy, the central atom descriptors provide a larger improvement for the Cr and Co atoms. Adding nearest neighbor descriptors does not offer additional improvements beyond the central atom descriptors.\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=MAE-compare-bs-centr-nn.jpg width=\"700px\" height='300px' align=\"center\" /> \n",
    "</p>\n",
    "\n",
    "The scatter plots below show the neural network predictions versus the molecular mechanics predictions for each property. Using central atom descriptors results in the neural network predictions more closely matching the molecular mechanics predictions.\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=bs-coeffs/relaxed-vfe-CrFeCoNi-bs-centr-compare.png width=\"700px\" height='300px' align=\"center\" /> \n",
    "</p>\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=bs-coeffs/cohesive-energy-CrFeCoNi-bs-centr-compare.png width=\"700px\" height='300px' align=\"center\" /> \n",
    "</p>\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=bs-coeffs/pressure-CrFeCoNi-bs-centr-compare.png width=\"700px\" height='300px' align=\"center\" /> \n",
    "</p>\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=bs-coeffs/volume-CrFeCoNi-bs-centr-compare.png width=\"700px\" height='300px' align=\"center\" /> \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
